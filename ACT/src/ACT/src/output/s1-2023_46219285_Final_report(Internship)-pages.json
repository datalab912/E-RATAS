{
    "id": "1",
    "name": "Reflective journal Entities",
    "nodeType": "section",
    "text": "week 1\nduring week one of the internship, i was introduced to the teams, got an overview of the company's best\npractices, discussed the project with the team leader, and researched information relevant to the project. i\nobserved a very welcoming and encouraging atmosphere during induction and the first week.\ni started on training materials related to data governance and the alation data catalog during the first\nweek of my internship at domain. i also acquired an office laptop and accessed alation, snowflake,\ntableau, aws s3, and salesforce. i also earned a completion badge for completing the alation beginner\ntraining.\nthe alation data catalogue is an enterprise data cataloguing platform that improves the discovery,\nunderstanding, and usage of data. using this resource, data assets from multiple sources can be searched\nand organized, providing a central repository for metadata. a database, a data lake, or a cloud storage\nservice are some of these sources.\nthis experience provided me with a basic understanding of data governance principles, data safety, and\ndata cataloguing. as a result, i developed a greater understanding of the various stakeholders within the\ndomain and gained a better understanding of the critical role data governance plays in an established\norganization. there were challenges associated with adjusting to the hybrid environment, but there were\nalso opportunities for growth.\nmy experience with the company's data governance framework and internal software tools was enhanced\nby the onboarding and training process. support and guidance from my supervisor were crucial to\novercoming the challenges i faced during this learning process.\nwith these skills and knowledge gained, i plan to apply them to my future work at domain. to be a\nvaluable contributor to the organization and the data management industry, i intend to continue learning\nand expanding my understanding of data governance. furthermore, i will improve my ability to adapt to a\nvariety of environments and situations, which will serve me well professionally.\ninternship final report\nweek 2\nin the second week of my internship, i focused on familiarizing myself with the alation data catalogue\nand understanding the different features of alation and how they fall in domain use cases, completing\nworkshop recordings, and starting the alation university data steward training. in addition, i gained an\nunderstanding of a variety of data privacy policies and began onboarding data to the alation data\ncatalogue. besides that, i also did training on advanced sql to increase my abilities to query in alation\ncompose and did a course on hands-on snowflake.\nthe different policies i got to read about are the following.\n●\naustralian privacy principles (apps): australian privacy principles (app) are a set of 13\nprinciples that guide businesses and government agencies in protecting individuals' privacy.\n●\ncalifornia consumer privacy act (ccpa): as of january 1, 2020, california residents are entitled\nto certain privacy rights related to their personal information, and companies operating within the\nstate must comply with it.\n●\ngeneral data protection regulation (gdpr): the general data protection regulation (gdpr)\nregulates data protection and privacy in the european union, giving individuals more control over\ntheir personal data and harmonizing data protection laws across the member states since may 25,\n2018.\ni got to understand different roles in domain, like data owner, data steward and\ndata smes for different data sources. i also understood the basics of what a data steward is and how a\ndata steward manages data resources in the domain.\ndata onboarding is a major part of the domain right start program. during data onboarding, new data\nsources are imported, integrated, and prepared for use within an organization's existing data ecosystem.\nthis ensures that incoming data is accurate, consistent, and compatible with existing data structures and\nsystems. one of my tasks for me as a data governance intern was to collect collate and onboard metadata\ninto alation to strengthen data governance at domain. so in week 2 in learned to onboard metadata about\ndifferent data sources\nweek 3\nduring my third week at domain, i focused on importing data into alation from the data source registry,\nan excel file containing information about various third-party vendors that collaborate with domain. in\naddition to attending regular meetings for updates and discussing any issues with my supervisor, i found it\nrewarding to seek guidance and learn from their expertise.\na core responsibility as a data governance intern was to onboard data about different third-party vendors\ninto alation, which is essential for the successful implementation of the data catalogue. this task allowed\ninternship final report\nme to gain a deeper understanding of domain's data sources and the projects they work on, as well as how\nthe data catalogue supports teams in accessing crucial information about projects, data sources, data\nlineage, policies, and data owners.\nto accomplish this, i conducted interviews with various data smes at domain to gather insights about the\ndata they manage. this experience not only broadened my knowledge about the diverse data sources\nutilized for different tasks within domain but also taught me how to interpret contracts and privacy\npolicies to comprehend the permitted use and retention period for each data source.\nfurthermore, my communication skills were significantly enhanced through these interviews with data\nsmes and data stewards, contributing to my overall professional development in the field of data\ngovernance.\nweek 4\nthe goals for this week 4 continued from last week's task which was to keep onboarding metadata like a\ndescription, permitted uses, data owners, data stewards, data smes, and data located on different\nlocations like aws, snowflake and tableau. besides that, i also had to write sample queries in alation\ncompose for data users and flag depreciated data sources with warnings.\nother than that i also got access to the data onboarding requests and pii(personal identifiable information\n) assessments. i was tasked with understanding the process steps of data onboarding and what are the\nsteps that are required to do if the data had piis.\ndata onboarding requests are formal requests made within the domain or to a third-party vendor to\nimport, integrate, or ingest specific data sets into a new system, platform, or tool. these requests usually\ninvolve specifying the data sources, data types, and any required transformations or mapping to ensure\nseamless integration with the existing data infrastructure.\nif domain imports data from any third-party vendors or vice versa, the first step is to send a datao\nnboarding request which is then analysed by the data governance team and legal team to see it the data\nmeets all the requirements and the use cases. after that a pii assessment form is required if the data\ncontains piis, which is any information that can identify, contact, or locate an individual, either on its own\nor when combined with other relevant data. examples of pii include names, addresses, phone numbers,\nemail addresses, medicare numbers etc. only when the data request passes the pii can it be onboarded.\noverall this week was challenging yet enjoyable. i learned about the processes and requirements involved\nin data onboarding requests and pii assessments. this experience helped me gain a deeper understanding\nof data governance and the importance of protecting sensitive information in an organization.\nweek 5\ninternship final report\nthe goals for this week 5 continued from last week's task which was to keep onboarding metadata from\ndifferent documents and meetings with internal stakeholders to gather information regarding the software\nand data they use and the reason onto alation. besides following conversations in the data onboarding\nrequests between coey( data governance manager, legal team and third-party vendors ) to better\nunderstand how to evaluate them. i also did regular standups with my supervisor and other interns in\ndomain’s data team regarding our updates. i also had a training session on advanced tableau which was\ntaken by our head of the business intelligence team.\none of the best ways to learn skills is to follow someone's experience and that was my goal for this week\nwhich was to follow coey and how she asses the data onboarding requests and assesses piis which was a\nmajor task for the data governance team. the data governance team is responsible for ensuring that the\ndata entering domain is complete, accurate, consistent, and compliant with relevant policies and\nregulations.\nanother activity we engaged in was advanced tableau training in-house. tableau is one of the primary\nsoftware applications used for business intelligence (bi) at domain. it is a powerful data visualization\nand analytics tool that allows users to create interactive and shareable dashboards, transforming raw data\ninto easily understandable and actionable insights. by participating in this training, we gained a deeper\nunderstanding of tableau's advanced features, enhancing our ability to analyze, interpret, and present data\neffectively within the organization.\nweek 6\nthe goals for this week 5 continued from last week's task which was to keep onboarding metadata from\ndifferent documents and meetings with internal stakeholders to gather information regarding the software\nand data they use and the reason for alation and also viewing data onboarding requests and pii\nassessments asking questions and queries if i didn't understand something to my supervisor. the new\nthing in week 6 was to research 3 new tasks that were really important for domain right start a program\nto be successful.\n1)\nimplementing rules to data sources to monitor data health meaning implementing rules to data\nsources to monitor data health refers to the process of establishing and applying specific criteria\nor guidelines to assess the quality, accuracy, and consistency of data within an organization's data\nsources.\n2)\nresearching a way to automate the process of flagging pii data via the use of metadata and\nphysical data. which will drastically decrease the time it needs to flagg thousands of data tables.\n3)\nautomating importing glossary terms into alation from different data locations like aws ,\nconfluence articles.\nthis week i did research on the second task which is automatically flag piis. the software i looked into\nwas bigid.using bigid, domain can find and secure sensitive information by discovering and managing\ndata. in addition to structured and unstructured data, cloud ecosystems, and backup files, it automatically\nidentifies and categorizes personal and sensitive data based on sophisticated machine learning techniques.\ninternship final report\none of alation's primary task is tagging piis. however, the auto pii tagging feature relies on column\ntitles, so it is ineffective when tagging piis with abbreviations. by using machine learning, bigid can\nautomate the classification and tagging of pii at the physical level. furthermore, bigid can identify\npolicies, automatically flagging them for each data point and integrating them with alation. a variety of\nprivacy regulations are supported by bigid, and customs regulations can be added as well.\nthis is still in the research phase and is not finalised if it's going to be used. meanwhile, i am researching\ndifferent tools to be used for automating pii tagging.\nweek 7\nthis week was a continuation of week 6, besides my regular task of data onboarding, staff meeting or\nslack chats for data collection and looking into onboarding requests and pii assessment. this week i did\nresearch on automating the collection of glossaries from different confluence articles to alations.\nfor the initial data collection, i utilized python and confluence apis to gather data and employed\nbeautifulsoup to format it in a specific way for importing into a pandas dataframe. subsequently, the\ndataframe was converted to a csv file. this initial approach will be further developed to create a fully\nautomated system. additionally, i began researching openai's policies to explore their potential for\nenhancing metadata descriptors. openai is a company that specializes in developing advanced artificial\nintelligence technologies, which could potentially improve the efficiency and accuracy of metadata\ntagging and management. this large language model (llm) can generate metadata for various data\nsources by comprehending the context of articles. gpt-3.5 can analyze and extract relevant information\nfrom large volumes of text by leveraging its advanced natural language processing capabilities,\npotentially leading to more accurate and efficient metadata generation and management.\noverall, this week has been immersive and resourceful. according to my internship plan, this week\naccording to the plan i was supposed to assist with the collection and collation of required data across\nvarious data environments (snowflake, aws, tableau and salesforce) and document data mappings,\nbusiness glossaries, data ownership and processes. as i am already near the end of that process i took on\nsome additional responsibilities which were automating importing data glossaries and researching pii\ntaggings.\nautomating data health updates is another task on my list of responsibilities, and it requires collaboration\nwith data stewards and smes. since they are responsible for setting up rules for data health, their input is\ncrucial. each data source is different and has unique data health parameters, so their expertise is essential\nin ensuring that the automation process accurately reflects each data asset's specific needs and\ncharacteristics.\ninternship final report\nweek 8\nin week 8, after a break from my internship, i returned to work with a clear focus. even though i did not\ncontinue my internship during the break, i was able to transition successfully back into work.\nin the beginning of the week, i updated previous data articles in alation based on feedback from my\nmanager. this step was crucial to ensure that the data articles in our data catalogue are accurate and\nrelevant.\nfurthermore, i also revisited the data requests and pii assessments that were received during the break to\ngain a better understanding of the current situation.\nduring my time in alation, i explored how rules could be set to monitor data health. however, the\ncomplexity of this task soon became apparent to me. the process proved too complicated for us to\nimplement immediately because each data owner had to set their own unique rules. as a result, we\ndecided to postpone it until a later date with the intention of revisiting and implementing it at a later date.\noverall, the week was successful and allowed me to catch up on tasks accumulated during my break\ndespite this minor setback.\nweek 9\nin week 8, i handled data onboarding, staff meetings for data collection, onboarding requests, and pii\nassessments. my primary focus was on automating the collection of glossaries from confluence articles\nby alation. to gather initial data last week, i used python and confluence apis, formatted the data frame\nwith beautifulsoup for pandas data frames, and converted the data frame into a csv format. this forms\nthe foundation for a future fully automated system. as different articles contain glossary terms in different\nformats and some articles have duplicate glossaries, i spent this week automating the entire process and\nscraping data from the confluence. the formatting and cleaning of the data were challenging, and some\nmanual intervention was required to accomplish the task, which was accomplished by using pandas.\na number of meetings with stakeholders were organized and more data articles were added to the alation\ndata catalogue from the data source registry.\nin this week's work, over 500 glossary data has been compiled into a single csv and submitted for\nreview. it was important for me to understand how confluence's api works (reading the documentation\nand experimenting with a variety of approaches) as well as the role of the alation data catalogue in our\ndata governance system. our data governance leaders, chantelle robertson and coey zeng, wrapped up\nthe week with a rewarding discussion.\nweek 10\nmy journey continued in week 10 as i continued to manage data onboarding, staff meetings, and pii\nassessments. additionally, i incorporated a new goal into my plan: uploading business metrics to alation.\ninternship final report\nthe progress of each department is measured by business metrics in an organization as large and diverse\nas domain. the company's vast portfolio consists of a wide variety of business metrics that reflect the\ngoals and performance of each entity. this week, i was tasked with validating and uploading these\nbusiness metrics.\nduring my discussions with my manager, i cross-referenced the business metrics with documents\nobtained from several stakeholder meetings to ensure their accuracy. in order to ensure accurate,\nup-to-date, and valuable metrics were uploaded for each department's performance evaluation, this\nprocess was crucial.\nwhile my work was majorly focused on these tasks, i also found time to partake in a community event\nnamed solar buddy. this event was a wonderful opportunity to give back to society, as we built\nsolar-powered flashlights for underprivileged children.\nreflecting on the week, the outcomes were quite rewarding. i was able to collect valuable insights from\ndifferent departments regarding their data resources, and further enriched the alation data catalogue by\nonboarding more articles, inclusive of project descriptions, snowflake links, policies, and data owners.\nthe uploading of several business metrics onto alation was a significant achievement, marking a\nsuccessful week. a standout moment was the fulfilling experience of participating in the solar buddy\nevent, where i contributed to a cause beyond the realm of my professional responsibilities.\nweek 11\nafter an unexpected health problem occurred in week 11, i had to adjust my work schedule. despite\ntesting positive for covid-19, i managed to continue with some of my tasks remotely while prioritizing\nrest and recovery. my supervisors were extremely supportive and told me to take a break and only\ncontinue if i felt better.\nduring this period, i continued to explore the alation api documentation, specifically how to upload\nglossaries to the catalogue. in order to accomplish this task, i created an api user and refresh token, then\nconducted various exercises to gain familiarity with the process. additionally, i participated in alation\nuniversity courses to further my knowledge.\nin spite of my health issues, i uploaded internal data articles to alation, a task that i could conveniently\ncomplete remotely, even though i did not take on new responsibilities. i continued to gather information\nand insights from different departments regarding the data resources they utilize. i onboarded more\narticles into the alation data catalogue, including descriptions of projects, links to snowflake, policies,\nand data owners, as part of this task.\nin summary, despite my health condition, i was able to contribute productively, furthering our project\ngoals and enhancing my learning.\ninternship final report\nweek 12\nas i continued to recover from the effects of covid-19 during week 12, i was able to remain productive\nand contribute to my team despite the slower pace. it was my primary responsibility to upload information\nabout some internal data sources to alation, ensuring that our catalogue remained current.\nbesides these ongoing tasks, i provided my manager with a progress update, which enables us to align our\nexpectations, assess my current abilities, and plan for the upcoming weeks.\nfurthermore, despite not taking on any new responsibilities due to my health, i took this time to expand\nmy knowledge. in particular, i researched alteryx, a no-code and low-code data analysis tool. alteryx\nsimplifies data analysis with its no-code and low-code interface. the tool makes it easy for teams with\nvarying skill levels to perform complex data manipulations and analytics tasks. in future data projects, i\nlook forward to leveraging alteryx's capabilities as i continue to learn and explore it.\nweek 13\nthis was the culmination of all my hard work and experiences over the past few months. my\nresponsibilities included submitting all my work and wrapping up the internship. returning the equipment\nprovided by domain. i undertook the essential step of cleaning all the data assets from the laptop to\nensure data security.\na highlight of the week was the opportunity to present my findings to the entire data team. this audience\nincluded influential data leaders like chantelle robertson, head of data governance, and pooyan asgari,\nchief data officer. their positive feedback regarding my work and the impressed reactions of the team\nwere gratifying and validated the effort i had put into this internship.\npost-presentation, we had an informal gathering with snacks, during which kieran goldsworthy, hr\nmanager, offered us invaluable guidance. his tips on preparing for the future, crafting an effective\nresume, and acing interviews were particularly insightful.\nthe week concluded on a high note, with a photoshoot and heartfelt farewells from everyone. despite the\nend of this journey, the experiences and learnings from my time at domain will continue to influence my\nprofessional path ahead.\n",
    "page": null,
    "goal": "Reflective journal Entities",
    "children": [
        {
            "id": "1.1",
            "name": "week 1",
            "nodeType": "title",
            "text": "week 1",
            "page": null,
            "goal": "week 1",
            "children": []
        },
        {
            "id": "1.2",
            "name": "week 1 during",
            "nodeType": "paragraph",
            "text": "week 1\nduring week one of the internship, i was introduced to the teams, got an overview of the company's best\npractices, discussed the project with the team leader, and researched information relevant to the project. i\nobserved a very welcoming and encouraging atmosphere during induction and the first week.\ni started on training materials related to data governance and the alation data catalog during the first\nweek of my internship at domain. i also acquired an office laptop and accessed alation, snowflake,\ntableau, aws s3, and salesforce. i also earned a completion badge for completing the alation beginner\ntraining.\nthe alation data catalogue is an enterprise data cataloguing platform that improves the discovery,\nunderstanding, and usage of data. using this resource, data assets from multiple sources can be searched\nand organized, providing a central repository for metadata. a database, a data lake, or a cloud storage\nservice are some of these sources.\nthis experience provided me with a basic understanding of data governance principles, data safety, and\ndata cataloguing. as a result, i developed a greater understanding of the various stakeholders within the\ndomain and gained a better understanding of the critical role data governance plays in an established\norganization. there were challenges associated with adjusting to the hybrid environment, but there were\nalso opportunities for growth.\nmy experience with the company's data governance framework and internal software tools was enhanced\nby the onboarding and training process. support and guidance from my supervisor were crucial to\novercoming the challenges i faced during this learning process.\nwith these skills and knowledge gained, i plan to apply them to my future work at domain. to be a\nvaluable contributor to the organization and the data management industry, i intend to continue learning\nand expanding my understanding of data governance. furthermore, i will improve my ability to adapt to a\nvariety of environments and situations, which will serve me well professionally.\ninternship final report",
            "page": null,
            "goal": "week 1\nduring week one of the internship, i was introduced to the teams, got an overview of the company's best\npractices, discussed the project with the team leader, and researched information relevant to the project. i\nobserved a very welcoming and encouraging atmosphere during induction and the first week.\ni started on training materials related to data governance and the alation data catalog during the first\nweek of my internship at domain. i also acquired an office laptop and accessed alation, snowflake,\ntableau, aws s3, and salesforce. i also earned a completion badge for completing the alation beginner\ntraining.\nthe alation data catalogue is an enterprise data cataloguing platform that improves the discovery,\nunderstanding, and usage of data. using this resource, data assets from multiple sources can be searched\nand organized, providing a central repository for metadata. a database, a data lake, or a cloud storage\nservice are some of these sources.\nthis experience provided me with a basic understanding of data governance principles, data safety, and\ndata cataloguing. as a result, i developed a greater understanding of the various stakeholders within the\ndomain and gained a better understanding of the critical role data governance plays in an established\norganization. there were challenges associated with adjusting to the hybrid environment, but there were\nalso opportunities for growth.\nmy experience with the company's data governance framework and internal software tools was enhanced\nby the onboarding and training process. support and guidance from my supervisor were crucial to\novercoming the challenges i faced during this learning process.\nwith these skills and knowledge gained, i plan to apply them to my future work at domain. to be a\nvaluable contributor to the organization and the data management industry, i intend to continue learning\nand expanding my understanding of data governance. furthermore, i will improve my ability to adapt to a\nvariety of environments and situations, which will serve me well professionally.\ninternship final report",
            "children": []
        },
        {
            "id": "1.3",
            "name": "week 2",
            "nodeType": "title",
            "text": "week 2",
            "page": null,
            "goal": "week 2",
            "children": []
        },
        {
            "id": "1.4",
            "name": "week 2 in",
            "nodeType": "paragraph",
            "text": "week 2\nin the second week of my internship, i focused on familiarizing myself with the alation data catalogue\nand understanding the different features of alation and how they fall in domain use cases, completing\nworkshop recordings, and starting the alation university data steward training. in addition, i gained an\nunderstanding of a variety of data privacy policies and began onboarding data to the alation data\ncatalogue. besides that, i also did training on advanced sql to increase my abilities to query in alation\ncompose and did a course on hands-on snowflake.\nthe different policies i got to read about are the following.\n●\naustralian privacy principles (apps): australian privacy principles (app) are a set of 13\nprinciples that guide businesses and government agencies in protecting individuals' privacy.\n●\ncalifornia consumer privacy act (ccpa): as of january 1, 2020, california residents are entitled\nto certain privacy rights related to their personal information, and companies operating within the\nstate must comply with it.\n●\ngeneral data protection regulation (gdpr): the general data protection regulation (gdpr)\nregulates data protection and privacy in the european union, giving individuals more control over\ntheir personal data and harmonizing data protection laws across the member states since may 25,\n2018.\ni got to understand different roles in domain, like data owner, data steward and\ndata smes for different data sources. i also understood the basics of what a data steward is and how a\ndata steward manages data resources in the domain.\ndata onboarding is a major part of the domain right start program. during data onboarding, new data\nsources are imported, integrated, and prepared for use within an organization's existing data ecosystem.\nthis ensures that incoming data is accurate, consistent, and compatible with existing data structures and\nsystems. one of my tasks for me as a data governance intern was to collect collate and onboard metadata\ninto alation to strengthen data governance at domain. so in week 2 in learned to onboard metadata about\ndifferent data sources",
            "page": null,
            "goal": "week 2\nin the second week of my internship, i focused on familiarizing myself with the alation data catalogue\nand understanding the different features of alation and how they fall in domain use cases, completing\nworkshop recordings, and starting the alation university data steward training. in addition, i gained an\nunderstanding of a variety of data privacy policies and began onboarding data to the alation data\ncatalogue. besides that, i also did training on advanced sql to increase my abilities to query in alation\ncompose and did a course on hands-on snowflake.\nthe different policies i got to read about are the following.\n●\naustralian privacy principles (apps): australian privacy principles (app) are a set of 13\nprinciples that guide businesses and government agencies in protecting individuals' privacy.\n●\ncalifornia consumer privacy act (ccpa): as of january 1, 2020, california residents are entitled\nto certain privacy rights related to their personal information, and companies operating within the\nstate must comply with it.\n●\ngeneral data protection regulation (gdpr): the general data protection regulation (gdpr)\nregulates data protection and privacy in the european union, giving individuals more control over\ntheir personal data and harmonizing data protection laws across the member states since may 25,\n2018.\ni got to understand different roles in domain, like data owner, data steward and\ndata smes for different data sources. i also understood the basics of what a data steward is and how a\ndata steward manages data resources in the domain.\ndata onboarding is a major part of the domain right start program. during data onboarding, new data\nsources are imported, integrated, and prepared for use within an organization's existing data ecosystem.\nthis ensures that incoming data is accurate, consistent, and compatible with existing data structures and\nsystems. one of my tasks for me as a data governance intern was to collect collate and onboard metadata\ninto alation to strengthen data governance at domain. so in week 2 in learned to onboard metadata about\ndifferent data sources",
            "children": []
        },
        {
            "id": "1.5",
            "name": "week 3",
            "nodeType": "title",
            "text": "week 3",
            "page": null,
            "goal": "week 3",
            "children": []
        },
        {
            "id": "1.6",
            "name": "week 3 during",
            "nodeType": "paragraph",
            "text": "week 3\nduring my third week at domain, i focused on importing data into alation from the data source registry,\nan excel file containing information about various third-party vendors that collaborate with domain. in\naddition to attending regular meetings for updates and discussing any issues with my supervisor, i found it\nrewarding to seek guidance and learn from their expertise.\na core responsibility as a data governance intern was to onboard data about different third-party vendors\ninto alation, which is essential for the successful implementation of the data catalogue. this task allowed\ninternship final report\nme to gain a deeper understanding of domain's data sources and the projects they work on, as well as how\nthe data catalogue supports teams in accessing crucial information about projects, data sources, data\nlineage, policies, and data owners.\nto accomplish this, i conducted interviews with various data smes at domain to gather insights about the\ndata they manage. this experience not only broadened my knowledge about the diverse data sources\nutilized for different tasks within domain but also taught me how to interpret contracts and privacy\npolicies to comprehend the permitted use and retention period for each data source.\nfurthermore, my communication skills were significantly enhanced through these interviews with data\nsmes and data stewards, contributing to my overall professional development in the field of data\ngovernance.",
            "page": null,
            "goal": "week 3\nduring my third week at domain, i focused on importing data into alation from the data source registry,\nan excel file containing information about various third-party vendors that collaborate with domain. in\naddition to attending regular meetings for updates and discussing any issues with my supervisor, i found it\nrewarding to seek guidance and learn from their expertise.\na core responsibility as a data governance intern was to onboard data about different third-party vendors\ninto alation, which is essential for the successful implementation of the data catalogue. this task allowed\ninternship final report\nme to gain a deeper understanding of domain's data sources and the projects they work on, as well as how\nthe data catalogue supports teams in accessing crucial information about projects, data sources, data\nlineage, policies, and data owners.\nto accomplish this, i conducted interviews with various data smes at domain to gather insights about the\ndata they manage. this experience not only broadened my knowledge about the diverse data sources\nutilized for different tasks within domain but also taught me how to interpret contracts and privacy\npolicies to comprehend the permitted use and retention period for each data source.\nfurthermore, my communication skills were significantly enhanced through these interviews with data\nsmes and data stewards, contributing to my overall professional development in the field of data\ngovernance.",
            "children": []
        },
        {
            "id": "1.7",
            "name": "week 4",
            "nodeType": "title",
            "text": "week 4",
            "page": null,
            "goal": "week 4",
            "children": []
        },
        {
            "id": "1.8",
            "name": "week 4 the",
            "nodeType": "paragraph",
            "text": "week 4\nthe goals for this week 4 continued from last week's task which was to keep onboarding metadata like a\ndescription, permitted uses, data owners, data stewards, data smes, and data located on different\nlocations like aws, snowflake and tableau. besides that, i also had to write sample queries in alation\ncompose for data users and flag depreciated data sources with warnings.\nother than that i also got access to the data onboarding requests and pii(personal identifiable information\n) assessments. i was tasked with understanding the process steps of data onboarding and what are the\nsteps that are required to do if the data had piis.\ndata onboarding requests are formal requests made within the domain or to a third-party vendor to\nimport, integrate, or ingest specific data sets into a new system, platform, or tool. these requests usually\ninvolve specifying the data sources, data types, and any required transformations or mapping to ensure\nseamless integration with the existing data infrastructure.\nif domain imports data from any third-party vendors or vice versa, the first step is to send a datao\nnboarding request which is then analysed by the data governance team and legal team to see it the data\nmeets all the requirements and the use cases. after that a pii assessment form is required if the data\ncontains piis, which is any information that can identify, contact, or locate an individual, either on its own\nor when combined with other relevant data. examples of pii include names, addresses, phone numbers,\nemail addresses, medicare numbers etc. only when the data request passes the pii can it be onboarded.\noverall this week was challenging yet enjoyable. i learned about the processes and requirements involved\nin data onboarding requests and pii assessments. this experience helped me gain a deeper understanding\nof data governance and the importance of protecting sensitive information in an organization.",
            "page": null,
            "goal": "week 4\nthe goals for this week 4 continued from last week's task which was to keep onboarding metadata like a\ndescription, permitted uses, data owners, data stewards, data smes, and data located on different\nlocations like aws, snowflake and tableau. besides that, i also had to write sample queries in alation\ncompose for data users and flag depreciated data sources with warnings.\nother than that i also got access to the data onboarding requests and pii(personal identifiable information\n) assessments. i was tasked with understanding the process steps of data onboarding and what are the\nsteps that are required to do if the data had piis.\ndata onboarding requests are formal requests made within the domain or to a third-party vendor to\nimport, integrate, or ingest specific data sets into a new system, platform, or tool. these requests usually\ninvolve specifying the data sources, data types, and any required transformations or mapping to ensure\nseamless integration with the existing data infrastructure.\nif domain imports data from any third-party vendors or vice versa, the first step is to send a datao\nnboarding request which is then analysed by the data governance team and legal team to see it the data\nmeets all the requirements and the use cases. after that a pii assessment form is required if the data\ncontains piis, which is any information that can identify, contact, or locate an individual, either on its own\nor when combined with other relevant data. examples of pii include names, addresses, phone numbers,\nemail addresses, medicare numbers etc. only when the data request passes the pii can it be onboarded.\noverall this week was challenging yet enjoyable. i learned about the processes and requirements involved\nin data onboarding requests and pii assessments. this experience helped me gain a deeper understanding\nof data governance and the importance of protecting sensitive information in an organization.",
            "children": []
        },
        {
            "id": "1.9",
            "name": "week 5",
            "nodeType": "title",
            "text": "week 5",
            "page": null,
            "goal": "week 5",
            "children": []
        },
        {
            "id": "1.10",
            "name": "week 5 internship",
            "nodeType": "paragraph",
            "text": "week 5\ninternship final report\nthe goals for this week 5 continued from last week's task which was to keep onboarding metadata from\ndifferent documents and meetings with internal stakeholders to gather information regarding the software\nand data they use and the reason onto alation. besides following conversations in the data onboarding\nrequests between coey( data governance manager, legal team and third-party vendors ) to better\nunderstand how to evaluate them. i also did regular standups with my supervisor and other interns in\ndomain’s data team regarding our updates. i also had a training session on advanced tableau which was\ntaken by our head of the business intelligence team.\none of the best ways to learn skills is to follow someone's experience and that was my goal for this week\nwhich was to follow coey and how she asses the data onboarding requests and assesses piis which was a\nmajor task for the data governance team. the data governance team is responsible for ensuring that the\ndata entering domain is complete, accurate, consistent, and compliant with relevant policies and\nregulations.\nanother activity we engaged in was advanced tableau training in-house. tableau is one of the primary\nsoftware applications used for business intelligence (bi) at domain. it is a powerful data visualization\nand analytics tool that allows users to create interactive and shareable dashboards, transforming raw data\ninto easily understandable and actionable insights. by participating in this training, we gained a deeper\nunderstanding of tableau's advanced features, enhancing our ability to analyze, interpret, and present data\neffectively within the organization.",
            "page": null,
            "goal": "week 5\ninternship final report\nthe goals for this week 5 continued from last week's task which was to keep onboarding metadata from\ndifferent documents and meetings with internal stakeholders to gather information regarding the software\nand data they use and the reason onto alation. besides following conversations in the data onboarding\nrequests between coey( data governance manager, legal team and third-party vendors ) to better\nunderstand how to evaluate them. i also did regular standups with my supervisor and other interns in\ndomain’s data team regarding our updates. i also had a training session on advanced tableau which was\ntaken by our head of the business intelligence team.\none of the best ways to learn skills is to follow someone's experience and that was my goal for this week\nwhich was to follow coey and how she asses the data onboarding requests and assesses piis which was a\nmajor task for the data governance team. the data governance team is responsible for ensuring that the\ndata entering domain is complete, accurate, consistent, and compliant with relevant policies and\nregulations.\nanother activity we engaged in was advanced tableau training in-house. tableau is one of the primary\nsoftware applications used for business intelligence (bi) at domain. it is a powerful data visualization\nand analytics tool that allows users to create interactive and shareable dashboards, transforming raw data\ninto easily understandable and actionable insights. by participating in this training, we gained a deeper\nunderstanding of tableau's advanced features, enhancing our ability to analyze, interpret, and present data\neffectively within the organization.",
            "children": []
        },
        {
            "id": "1.11",
            "name": "week 6",
            "nodeType": "title",
            "text": "week 6",
            "page": null,
            "goal": "week 6",
            "children": []
        },
        {
            "id": "1.12",
            "name": "week 6 the",
            "nodeType": "paragraph",
            "text": "week 6\nthe goals for this week 5 continued from last week's task which was to keep onboarding metadata from\ndifferent documents and meetings with internal stakeholders to gather information regarding the software\nand data they use and the reason for alation and also viewing data onboarding requests and pii\nassessments asking questions and queries if i didn't understand something to my supervisor. the new\nthing in week 6 was to research 3 new tasks that were really important for domain right start a program\nto be successful.\n1)\nimplementing rules to data sources to monitor data health meaning implementing rules to data\nsources to monitor data health refers to the process of establishing and applying specific criteria\nor guidelines to assess the quality, accuracy, and consistency of data within an organization's data\nsources.\n2)\nresearching a way to automate the process of flagging pii data via the use of metadata and\nphysical data. which will drastically decrease the time it needs to flagg thousands of data tables.\n3)\nautomating importing glossary terms into alation from different data locations like aws ,\nconfluence articles.\nthis week i did research on the second task which is automatically flag piis. the software i looked into\nwas bigid.using bigid, domain can find and secure sensitive information by discovering and managing\ndata. in addition to structured and unstructured data, cloud ecosystems, and backup files, it automatically\nidentifies and categorizes personal and sensitive data based on sophisticated machine learning techniques.\ninternship final report\none of alation's primary task is tagging piis. however, the auto pii tagging feature relies on column\ntitles, so it is ineffective when tagging piis with abbreviations. by using machine learning, bigid can\nautomate the classification and tagging of pii at the physical level. furthermore, bigid can identify\npolicies, automatically flagging them for each data point and integrating them with alation. a variety of\nprivacy regulations are supported by bigid, and customs regulations can be added as well.\nthis is still in the research phase and is not finalised if it's going to be used. meanwhile, i am researching\ndifferent tools to be used for automating pii tagging.",
            "page": null,
            "goal": "week 6\nthe goals for this week 5 continued from last week's task which was to keep onboarding metadata from\ndifferent documents and meetings with internal stakeholders to gather information regarding the software\nand data they use and the reason for alation and also viewing data onboarding requests and pii\nassessments asking questions and queries if i didn't understand something to my supervisor. the new\nthing in week 6 was to research 3 new tasks that were really important for domain right start a program\nto be successful.\n1)\nimplementing rules to data sources to monitor data health meaning implementing rules to data\nsources to monitor data health refers to the process of establishing and applying specific criteria\nor guidelines to assess the quality, accuracy, and consistency of data within an organization's data\nsources.\n2)\nresearching a way to automate the process of flagging pii data via the use of metadata and\nphysical data. which will drastically decrease the time it needs to flagg thousands of data tables.\n3)\nautomating importing glossary terms into alation from different data locations like aws ,\nconfluence articles.\nthis week i did research on the second task which is automatically flag piis. the software i looked into\nwas bigid.using bigid, domain can find and secure sensitive information by discovering and managing\ndata. in addition to structured and unstructured data, cloud ecosystems, and backup files, it automatically\nidentifies and categorizes personal and sensitive data based on sophisticated machine learning techniques.\ninternship final report\none of alation's primary task is tagging piis. however, the auto pii tagging feature relies on column\ntitles, so it is ineffective when tagging piis with abbreviations. by using machine learning, bigid can\nautomate the classification and tagging of pii at the physical level. furthermore, bigid can identify\npolicies, automatically flagging them for each data point and integrating them with alation. a variety of\nprivacy regulations are supported by bigid, and customs regulations can be added as well.\nthis is still in the research phase and is not finalised if it's going to be used. meanwhile, i am researching\ndifferent tools to be used for automating pii tagging.",
            "children": []
        },
        {
            "id": "1.13",
            "name": "week 7",
            "nodeType": "title",
            "text": "week 7",
            "page": null,
            "goal": "week 7",
            "children": []
        },
        {
            "id": "1.14",
            "name": "week 7 this",
            "nodeType": "paragraph",
            "text": "week 7\nthis week was a continuation of week 6, besides my regular task of data onboarding, staff meeting or\nslack chats for data collection and looking into onboarding requests and pii assessment. this week i did\nresearch on automating the collection of glossaries from different confluence articles to alations.\nfor the initial data collection, i utilized python and confluence apis to gather data and employed\nbeautifulsoup to format it in a specific way for importing into a pandas dataframe. subsequently, the\ndataframe was converted to a csv file. this initial approach will be further developed to create a fully\nautomated system. additionally, i began researching openai's policies to explore their potential for\nenhancing metadata descriptors. openai is a company that specializes in developing advanced artificial\nintelligence technologies, which could potentially improve the efficiency and accuracy of metadata\ntagging and management. this large language model (llm) can generate metadata for various data\nsources by comprehending the context of articles. gpt-3.5 can analyze and extract relevant information\nfrom large volumes of text by leveraging its advanced natural language processing capabilities,\npotentially leading to more accurate and efficient metadata generation and management.\noverall, this week has been immersive and resourceful. according to my internship plan, this week\naccording to the plan i was supposed to assist with the collection and collation of required data across\nvarious data environments (snowflake, aws, tableau and salesforce) and document data mappings,\nbusiness glossaries, data ownership and processes. as i am already near the end of that process i took on\nsome additional responsibilities which were automating importing data glossaries and researching pii\ntaggings.\nautomating data health updates is another task on my list of responsibilities, and it requires collaboration\nwith data stewards and smes. since they are responsible for setting up rules for data health, their input is\ncrucial. each data source is different and has unique data health parameters, so their expertise is essential\nin ensuring that the automation process accurately reflects each data asset's specific needs and\ncharacteristics.\ninternship final report",
            "page": null,
            "goal": "week 7\nthis week was a continuation of week 6, besides my regular task of data onboarding, staff meeting or\nslack chats for data collection and looking into onboarding requests and pii assessment. this week i did\nresearch on automating the collection of glossaries from different confluence articles to alations.\nfor the initial data collection, i utilized python and confluence apis to gather data and employed\nbeautifulsoup to format it in a specific way for importing into a pandas dataframe. subsequently, the\ndataframe was converted to a csv file. this initial approach will be further developed to create a fully\nautomated system. additionally, i began researching openai's policies to explore their potential for\nenhancing metadata descriptors. openai is a company that specializes in developing advanced artificial\nintelligence technologies, which could potentially improve the efficiency and accuracy of metadata\ntagging and management. this large language model (llm) can generate metadata for various data\nsources by comprehending the context of articles. gpt-3.5 can analyze and extract relevant information\nfrom large volumes of text by leveraging its advanced natural language processing capabilities,\npotentially leading to more accurate and efficient metadata generation and management.\noverall, this week has been immersive and resourceful. according to my internship plan, this week\naccording to the plan i was supposed to assist with the collection and collation of required data across\nvarious data environments (snowflake, aws, tableau and salesforce) and document data mappings,\nbusiness glossaries, data ownership and processes. as i am already near the end of that process i took on\nsome additional responsibilities which were automating importing data glossaries and researching pii\ntaggings.\nautomating data health updates is another task on my list of responsibilities, and it requires collaboration\nwith data stewards and smes. since they are responsible for setting up rules for data health, their input is\ncrucial. each data source is different and has unique data health parameters, so their expertise is essential\nin ensuring that the automation process accurately reflects each data asset's specific needs and\ncharacteristics.\ninternship final report",
            "children": []
        },
        {
            "id": "1.15",
            "name": "week 8",
            "nodeType": "title",
            "text": "week 8",
            "page": null,
            "goal": "week 8",
            "children": []
        },
        {
            "id": "1.16",
            "name": "week 8 in",
            "nodeType": "paragraph",
            "text": "week 8\nin week 8, after a break from my internship, i returned to work with a clear focus. even though i did not\ncontinue my internship during the break, i was able to transition successfully back into work.\nin the beginning of the week, i updated previous data articles in alation based on feedback from my\nmanager. this step was crucial to ensure that the data articles in our data catalogue are accurate and\nrelevant.\nfurthermore, i also revisited the data requests and pii assessments that were received during the break to\ngain a better understanding of the current situation.\nduring my time in alation, i explored how rules could be set to monitor data health. however, the\ncomplexity of this task soon became apparent to me. the process proved too complicated for us to\nimplement immediately because each data owner had to set their own unique rules. as a result, we\ndecided to postpone it until a later date with the intention of revisiting and implementing it at a later date.\noverall, the week was successful and allowed me to catch up on tasks accumulated during my break\ndespite this minor setback.",
            "page": null,
            "goal": "week 8\nin week 8, after a break from my internship, i returned to work with a clear focus. even though i did not\ncontinue my internship during the break, i was able to transition successfully back into work.\nin the beginning of the week, i updated previous data articles in alation based on feedback from my\nmanager. this step was crucial to ensure that the data articles in our data catalogue are accurate and\nrelevant.\nfurthermore, i also revisited the data requests and pii assessments that were received during the break to\ngain a better understanding of the current situation.\nduring my time in alation, i explored how rules could be set to monitor data health. however, the\ncomplexity of this task soon became apparent to me. the process proved too complicated for us to\nimplement immediately because each data owner had to set their own unique rules. as a result, we\ndecided to postpone it until a later date with the intention of revisiting and implementing it at a later date.\noverall, the week was successful and allowed me to catch up on tasks accumulated during my break\ndespite this minor setback.",
            "children": []
        },
        {
            "id": "1.17",
            "name": "week 9",
            "nodeType": "title",
            "text": "week 9",
            "page": null,
            "goal": "week 9",
            "children": []
        },
        {
            "id": "1.18",
            "name": "week 9 in",
            "nodeType": "paragraph",
            "text": "week 9\nin week 8, i handled data onboarding, staff meetings for data collection, onboarding requests, and pii\nassessments. my primary focus was on automating the collection of glossaries from confluence articles\nby alation. to gather initial data last week, i used python and confluence apis, formatted the data frame\nwith beautifulsoup for pandas data frames, and converted the data frame into a csv format. this forms\nthe foundation for a future fully automated system. as different articles contain glossary terms in different\nformats and some articles have duplicate glossaries, i spent this week automating the entire process and\nscraping data from the confluence. the formatting and cleaning of the data were challenging, and some\nmanual intervention was required to accomplish the task, which was accomplished by using pandas.\na number of meetings with stakeholders were organized and more data articles were added to the alation\ndata catalogue from the data source registry.\nin this week's work, over 500 glossary data has been compiled into a single csv and submitted for\nreview. it was important for me to understand how confluence's api works (reading the documentation\nand experimenting with a variety of approaches) as well as the role of the alation data catalogue in our\ndata governance system. our data governance leaders, chantelle robertson and coey zeng, wrapped up\nthe week with a rewarding discussion.",
            "page": null,
            "goal": "week 9\nin week 8, i handled data onboarding, staff meetings for data collection, onboarding requests, and pii\nassessments. my primary focus was on automating the collection of glossaries from confluence articles\nby alation. to gather initial data last week, i used python and confluence apis, formatted the data frame\nwith beautifulsoup for pandas data frames, and converted the data frame into a csv format. this forms\nthe foundation for a future fully automated system. as different articles contain glossary terms in different\nformats and some articles have duplicate glossaries, i spent this week automating the entire process and\nscraping data from the confluence. the formatting and cleaning of the data were challenging, and some\nmanual intervention was required to accomplish the task, which was accomplished by using pandas.\na number of meetings with stakeholders were organized and more data articles were added to the alation\ndata catalogue from the data source registry.\nin this week's work, over 500 glossary data has been compiled into a single csv and submitted for\nreview. it was important for me to understand how confluence's api works (reading the documentation\nand experimenting with a variety of approaches) as well as the role of the alation data catalogue in our\ndata governance system. our data governance leaders, chantelle robertson and coey zeng, wrapped up\nthe week with a rewarding discussion.",
            "children": []
        },
        {
            "id": "1.19",
            "name": "week 10",
            "nodeType": "title",
            "text": "week 10",
            "page": null,
            "goal": "week 10",
            "children": []
        },
        {
            "id": "1.20",
            "name": "week 10 my",
            "nodeType": "paragraph",
            "text": "week 10\nmy journey continued in week 10 as i continued to manage data onboarding, staff meetings, and pii\nassessments. additionally, i incorporated a new goal into my plan: uploading business metrics to alation.\ninternship final report\nthe progress of each department is measured by business metrics in an organization as large and diverse\nas domain. the company's vast portfolio consists of a wide variety of business metrics that reflect the\ngoals and performance of each entity. this week, i was tasked with validating and uploading these\nbusiness metrics.\nduring my discussions with my manager, i cross-referenced the business metrics with documents\nobtained from several stakeholder meetings to ensure their accuracy. in order to ensure accurate,\nup-to-date, and valuable metrics were uploaded for each department's performance evaluation, this\nprocess was crucial.\nwhile my work was majorly focused on these tasks, i also found time to partake in a community event\nnamed solar buddy. this event was a wonderful opportunity to give back to society, as we built\nsolar-powered flashlights for underprivileged children.\nreflecting on the week, the outcomes were quite rewarding. i was able to collect valuable insights from\ndifferent departments regarding their data resources, and further enriched the alation data catalogue by\nonboarding more articles, inclusive of project descriptions, snowflake links, policies, and data owners.\nthe uploading of several business metrics onto alation was a significant achievement, marking a\nsuccessful week. a standout moment was the fulfilling experience of participating in the solar buddy\nevent, where i contributed to a cause beyond the realm of my professional responsibilities.",
            "page": null,
            "goal": "week 10\nmy journey continued in week 10 as i continued to manage data onboarding, staff meetings, and pii\nassessments. additionally, i incorporated a new goal into my plan: uploading business metrics to alation.\ninternship final report\nthe progress of each department is measured by business metrics in an organization as large and diverse\nas domain. the company's vast portfolio consists of a wide variety of business metrics that reflect the\ngoals and performance of each entity. this week, i was tasked with validating and uploading these\nbusiness metrics.\nduring my discussions with my manager, i cross-referenced the business metrics with documents\nobtained from several stakeholder meetings to ensure their accuracy. in order to ensure accurate,\nup-to-date, and valuable metrics were uploaded for each department's performance evaluation, this\nprocess was crucial.\nwhile my work was majorly focused on these tasks, i also found time to partake in a community event\nnamed solar buddy. this event was a wonderful opportunity to give back to society, as we built\nsolar-powered flashlights for underprivileged children.\nreflecting on the week, the outcomes were quite rewarding. i was able to collect valuable insights from\ndifferent departments regarding their data resources, and further enriched the alation data catalogue by\nonboarding more articles, inclusive of project descriptions, snowflake links, policies, and data owners.\nthe uploading of several business metrics onto alation was a significant achievement, marking a\nsuccessful week. a standout moment was the fulfilling experience of participating in the solar buddy\nevent, where i contributed to a cause beyond the realm of my professional responsibilities.",
            "children": []
        },
        {
            "id": "1.21",
            "name": "week 11",
            "nodeType": "title",
            "text": "week 11",
            "page": null,
            "goal": "week 11",
            "children": []
        },
        {
            "id": "1.22",
            "name": "week 11 after",
            "nodeType": "paragraph",
            "text": "week 11\nafter an unexpected health problem occurred in week 11, i had to adjust my work schedule. despite\ntesting positive for covid-19, i managed to continue with some of my tasks remotely while prioritizing\nrest and recovery. my supervisors were extremely supportive and told me to take a break and only\ncontinue if i felt better.\nduring this period, i continued to explore the alation api documentation, specifically how to upload\nglossaries to the catalogue. in order to accomplish this task, i created an api user and refresh token, then\nconducted various exercises to gain familiarity with the process. additionally, i participated in alation\nuniversity courses to further my knowledge.\nin spite of my health issues, i uploaded internal data articles to alation, a task that i could conveniently\ncomplete remotely, even though i did not take on new responsibilities. i continued to gather information\nand insights from different departments regarding the data resources they utilize. i onboarded more\narticles into the alation data catalogue, including descriptions of projects, links to snowflake, policies,\nand data owners, as part of this task.\nin summary, despite my health condition, i was able to contribute productively, furthering our project\ngoals and enhancing my learning.\ninternship final report",
            "page": null,
            "goal": "week 11\nafter an unexpected health problem occurred in week 11, i had to adjust my work schedule. despite\ntesting positive for covid-19, i managed to continue with some of my tasks remotely while prioritizing\nrest and recovery. my supervisors were extremely supportive and told me to take a break and only\ncontinue if i felt better.\nduring this period, i continued to explore the alation api documentation, specifically how to upload\nglossaries to the catalogue. in order to accomplish this task, i created an api user and refresh token, then\nconducted various exercises to gain familiarity with the process. additionally, i participated in alation\nuniversity courses to further my knowledge.\nin spite of my health issues, i uploaded internal data articles to alation, a task that i could conveniently\ncomplete remotely, even though i did not take on new responsibilities. i continued to gather information\nand insights from different departments regarding the data resources they utilize. i onboarded more\narticles into the alation data catalogue, including descriptions of projects, links to snowflake, policies,\nand data owners, as part of this task.\nin summary, despite my health condition, i was able to contribute productively, furthering our project\ngoals and enhancing my learning.\ninternship final report",
            "children": []
        },
        {
            "id": "1.23",
            "name": "week 12",
            "nodeType": "title",
            "text": "week 12",
            "page": null,
            "goal": "week 12",
            "children": []
        },
        {
            "id": "1.24",
            "name": "week 12 as",
            "nodeType": "paragraph",
            "text": "week 12\nas i continued to recover from the effects of covid-19 during week 12, i was able to remain productive\nand contribute to my team despite the slower pace. it was my primary responsibility to upload information\nabout some internal data sources to alation, ensuring that our catalogue remained current.\nbesides these ongoing tasks, i provided my manager with a progress update, which enables us to align our\nexpectations, assess my current abilities, and plan for the upcoming weeks.\nfurthermore, despite not taking on any new responsibilities due to my health, i took this time to expand\nmy knowledge. in particular, i researched alteryx, a no-code and low-code data analysis tool. alteryx\nsimplifies data analysis with its no-code and low-code interface. the tool makes it easy for teams with\nvarying skill levels to perform complex data manipulations and analytics tasks. in future data projects, i\nlook forward to leveraging alteryx's capabilities as i continue to learn and explore it.",
            "page": null,
            "goal": "week 12\nas i continued to recover from the effects of covid-19 during week 12, i was able to remain productive\nand contribute to my team despite the slower pace. it was my primary responsibility to upload information\nabout some internal data sources to alation, ensuring that our catalogue remained current.\nbesides these ongoing tasks, i provided my manager with a progress update, which enables us to align our\nexpectations, assess my current abilities, and plan for the upcoming weeks.\nfurthermore, despite not taking on any new responsibilities due to my health, i took this time to expand\nmy knowledge. in particular, i researched alteryx, a no-code and low-code data analysis tool. alteryx\nsimplifies data analysis with its no-code and low-code interface. the tool makes it easy for teams with\nvarying skill levels to perform complex data manipulations and analytics tasks. in future data projects, i\nlook forward to leveraging alteryx's capabilities as i continue to learn and explore it.",
            "children": []
        },
        {
            "id": "1.25",
            "name": "week 13",
            "nodeType": "title",
            "text": "week 13",
            "page": null,
            "goal": "week 13",
            "children": []
        },
        {
            "id": "1.26",
            "name": "week 13 this",
            "nodeType": "paragraph",
            "text": "week 13\nthis was the culmination of all my hard work and experiences over the past few months. my\nresponsibilities included submitting all my work and wrapping up the internship. returning the equipment\nprovided by domain. i undertook the essential step of cleaning all the data assets from the laptop to\nensure data security.\na highlight of the week was the opportunity to present my findings to the entire data team. this audience\nincluded influential data leaders like chantelle robertson, head of data governance, and pooyan asgari,\nchief data officer. their positive feedback regarding my work and the impressed reactions of the team\nwere gratifying and validated the effort i had put into this internship.\npost-presentation, we had an informal gathering with snacks, during which kieran goldsworthy, hr\nmanager, offered us invaluable guidance. his tips on preparing for the future, crafting an effective\nresume, and acing interviews were particularly insightful.\nthe week concluded on a high note, with a photoshoot and heartfelt farewells from everyone. despite the\nend of this journey, the experiences and learnings from my time at domain will continue to influence my\nprofessional path ahead.",
            "page": null,
            "goal": "week 13\nthis was the culmination of all my hard work and experiences over the past few months. my\nresponsibilities included submitting all my work and wrapping up the internship. returning the equipment\nprovided by domain. i undertook the essential step of cleaning all the data assets from the laptop to\nensure data security.\na highlight of the week was the opportunity to present my findings to the entire data team. this audience\nincluded influential data leaders like chantelle robertson, head of data governance, and pooyan asgari,\nchief data officer. their positive feedback regarding my work and the impressed reactions of the team\nwere gratifying and validated the effort i had put into this internship.\npost-presentation, we had an informal gathering with snacks, during which kieran goldsworthy, hr\nmanager, offered us invaluable guidance. his tips on preparing for the future, crafting an effective\nresume, and acing interviews were particularly insightful.\nthe week concluded on a high note, with a photoshoot and heartfelt farewells from everyone. despite the\nend of this journey, the experiences and learnings from my time at domain will continue to influence my\nprofessional path ahead.",
            "children": []
        }
    ]
}