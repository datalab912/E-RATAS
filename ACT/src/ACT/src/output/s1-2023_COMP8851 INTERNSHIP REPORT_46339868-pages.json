{
    "id": "1",
    "name": "Reflective journal Entities",
    "nodeType": "section",
    "text": "week 1 \n \ndescribe \n \nin week 1, i met my team members and my supervisor masoud saifian who is a phd student \nat macquarie. got a chance to interact with the ceo stephen elbourn and discuss about the \nproject which we will be working on. had a brief presentation about the company and their \npolicies, how they deal with the clients and how the organization works. we were provided the \nproject timeline and a general discussion on how we will be implementing the concept into the \nproject. had a little office tour on the first day to see the office environment. i went through \nsome basic concepts of data analysis and the introduction to some new tools such as gradescope \nbecause that is similar to our project, just to see the functionality and the interface of that tool. \nwe were then given the access to different tools to communicate such as confluence, microsoft \nteams and itic systems internal login credentials. \n \ninterpret \n \ni worked with new tools and gained some knowledge about different concepts of data \ngeneration. did some work with chatgpt for the request of json format data samples which \nare mentioned below. \n \nthe chatgpt api accepts and returns data in json format, which stands for javascript object \nnotation. json is a lightweight and widely used data interchange format that is easy to read \nand write for both humans and machines. here is an example of how a request to the chatgpt \napi might look in json format. \n \n \nswift \ncopy code \n{ \n\"model\": \"text-davinci-002\", \n\"prompt\": \"what is the capital of france?\", \n\"temperature\": 0.7, \n\"max_tokens\": 50, \n\"stop\": [\"\\n\"] \n} \n \nin this example, the request includes the following parameters: \n \n\"model\": specifies the language model to use (in this case, \"text-davinci-002\"). \n\"prompt\": provides the input text to generate a response for (in this case, \"what \nis the capital of france?\"). \n\"temperature\": controls the randomness and creativity of the generated \nresponse (in this case, 0.7). \n\"max_tokens\": specifies the maximum length of the generated response (in this \ncase, 50 tokens). \n \n\"stop\": specifies a list of strings that, if generated, will be used to signal the end of \nthe generated response (in this case, [\"\\n\"], which represents a newline \ncharacter). \nthe response from the chatgpt api will also be in json format and will include \nthe generated text in a \"choices\" array: \n \njson \ncopy code \n{ \n\"choices\": [ \n{ \n\"text\": \"the capital of france is paris.\", \n\"index\": 0, \n\"logprobs\": null, \n\"finish_reason\": \"stop\" \n} \n] \n} \n \nin this example, the response includes the generated text as well as additional information about \nthe response, such as the index of the selected response, the log probabilities of each choice, \nand the reason for finishing the response. \n \n \n \n \nevaluate \n \ni considered this week to be helpful for the future after learning about organization, working, \nand projects because i was receiving experience working in a team within a firm and learning \nabout the business model given by the company professionals. i discovered the importance of \nteamwork through various forms of communication by examining the employees of the \norganisation.  \n \nplan \n \nall the insights which i took this week was new to me and i gained a lot of knowledge about \nthe different tools and techniques which we will implement in the upcoming weeks. this helped \nme in better understanding of the work and while giving the presentation and sharing my ideas \ni gained a lot of confidence in my work. i will continue to learn and implement those ideas for \nthe rest of the training period. \n \n \nweek 2 \n \ndescribe \n \nfor week 2, we focused on preparing the project management plan to keep the track of our \nprogress. then i researched on some commercial tools to check the functionality, facilities, \nmethods, and technologies which are provided in those products such as e-rater scoring engine, \ndeep score and gradescope. after that i did some code analysis about the gan method for \ngenerating essay datasets and compared them to the current scoring ml approaches. \n \ninterpret \n \nfor this week, i worked on the project management plan and wireframe design for the project \nwhich included project documentation plan where we worked on the document versioning \nstructure and selecting the tool for the document management. i updated the project gantt chart \nand the project progress report. i was also assigned the task for the data source generation using \nchatgpt. i used json formats to gather data from chatgpt by recognising the limitations of \nthe problem question and selecting the sample question for different fields such as data science, \ncyber security, and networking. then i prepared a dummy dataset for the evaluation of the \nchatgpt questions. for the weekly friday presentation, i worked on the wireframe designs of \nthe commercial tools to check their functionality in different roles and users, also their scope \nand ui functions. compared the ui and scope of these tools and did a gap analysis with mock \nup for the design wireframe.  \n \n \ndesign ideas for the custom rubrics and marking criteria. \n \n \n \n \nanalysis of the questions according to the different tools and rubrics which are marked by the \ntutor. \n \nevaluate \n \nhad a busy week working on project management tasks, researching commercial tools, \nanalysing code, generating data sources using chatgpt, and preparing wireframe designs for \ncommercial tools. i gained a good level of knowledge and skills in project management, data \nanalysis, and ui design. i was also able to recognize the limitations of the problem question \nand select appropriate sample questions for different fields. overall, i had a productive and \nfulfilling week, working on various aspects of the project, and contributing to its progress. \n \n \n \n \nplan \n \nwhile working with the dummy dataset i noticed that chatgpt was not able to generate large \ndataset of questions and some of the questions were also repeating. this was one of the \nchallenges which i faced while generating the dataset. for the next week my plan is to work \non the existing gan for the generation of essay dataset and research on the existing ml \nmethods for essay scoring. \n \n \nweek 3 \n \ndescribe \n \nfor week 3, my task was to work on existing gan for essay dataset generation (research and \ncode analysis) and had to research on different ml techniques on essay scoring followed by \nthe code analysis. i also worked on the bert model and was involved with the kaggle analysis \nof the public dataset. generated a list of questions which can provide a list of available domains \nand provide a number of essays and corresponding prompts.  \n \n \ninterpret \n \nessaygan has multiple generators and a discriminator to generate essays rated with different \nscores, where each generator is dedicated to producing essays with a specific score. the \ndiscriminator is trained to distinguish between real and generated essays. essaygan predicts \nthe next sentence in the sequence of the essay, rather than the next word, as done in ordinary-\ntext gan models. the proposed method can solve the high cost of collecting human-rated \nessays by generating essays automatically for training the aes system. \n \n \n \nautomated essay scoring (aes) is a popular area of research in learning analytics due to the \npotential for artificial intelligence to supplement human creativity. however, much of the \nresearch focuses solely on holistic scoring without delving into the rubric level, which is the \nfoundation of how holistic scores are determined. this lack of attention to rubric-level scoring \nhas made the aes black box difficult to understand. although some explainable artificial \nintelligence algorithms have been developed recently, none have explored the potential role of \nthese explanation models in understanding the decision-making process behind aes, \nimproving predictive models, or providing personalized feedback to students during the writing \nprocess. \n \nevalaute \n \nthe proposed essaygan system appears to be a promising solution to the problem of high \ncost and limited availability of human-rated essays for training automated essay scoring \nsystems. by using multiple generators dedicated to producing essays with specific scores, \nessaygan can generate a large volume of training data that closely resembles real essays. \nadditionally, the use of a discriminator helps to ensure that the generated essays are of high \nquality and similar to real essays. \non the other hand, it has a significant limitation in current aes research, which is the lack of \nattention to rubric-level scoring. this means that the underlying decision-making process of \naes remains unclear, and the aes black box is difficult to understand. the development of \nexplainable artificial intelligence algorithms may help to address this limitation by providing \ninsights into the decision-making process of aes models and improving their generalizability \nand interpretability. \nfinally, i can say that there is potential for ai to provide personalized, formative, and fine-\ngrained feedback to students during the writing process. this could help to improve the quality \nof student writing and support their learning and development. \n \n \nplan \n \nbased on the insights, my recommendation would be to continue exploring the use of \nessaygan and other ai-based approaches for generating training data for automated essay \nscoring systems. at the same time, researchers should focus on developing explainable \nartificial intelligence algorithms to gain a better understanding of the underlying decision-\nmaking process of aes models and improve their interpretability and generalizability. \nmoreover, it is crucial to pay attention to rubric-level scoring to develop a more comprehensive \nunderstanding of how holistic scores are determined. finally, it would be beneficial to explore \nhow ai can provide personalized, formative, and fine-grained feedback to students during the \nwriting process to support their learning and development. \n \n \nweek 4 \n \ndescribe \n \nfor week 4, i worked on the architecture to detect contract cheating and to generate ai reports \nand essays related to the submitted assignment or exam. i also had to research about some ml \nlibraries for the project such as tensorflow, theono, torch, scikit-learn and caffe. my \nsupervisor presented some slides for the architecture, and we had a brief discussion about the \nconcepts for the ai generated essay and reports. then i was assigned to finalise the architecture \nand all the concepts which are required.  \n \ninterpret \n \n \n \n \nthe proposed concept regarding a potential alternative to detect contract cheating and ai \ngenerated reports and essays. \n \nin this part, we can analyse the submitted paper and then summarise that using ai tools to \ngenerate few questions related to the exam or assignment. then we can send an email invite to \nthe student which will be automated so that they can join the video call or webchat to give the \nsummary about their report. we can use facial recognition to match the face on webcam and \ncan use image library to map the studentid. all the interaction will be recorded and students’ \nresponse will then be analysed and compared to his previous responses to predict the quality \nof the answers. this can be used to analyse and give a better result to the students and avoid \nany form of plagiarism or contract cheating. \nfor the ml libraries, i did some research and gave a presentation on the different libraries \nwhich are mentioned above in the description.  \n \n \ni compared and analysed different ml libraries and their use cases in different tasks to access \nthe models and their uses. tensorflow is based on the possibility of using it both for research \nand recurring machine learning tasks. it allows you to have a full control over models and train \nthem using your own dataset. while theono is a low-level library for scientific computing \nbased on python, which is used to target deep learning tasks related to defining, optimizing, \nand evaluating mathematical expressions. torch is a simple scripting language, and a helpful \ncommunity sharing an impressive array of tutorials and packages for almost any deep learning \npurpose. scikit-learn is a high-level framework designed for supervised and unsupervised \nmachine learning algorithms. caffe encourages users to get familiar with the datasets provided \nby both the industry and other users.  \n \nevaluate \n \nthe proposed approach of using ai tools to generate questions and analyse student responses \nusing facial recognition and image libraries to avoid plagiarism and contract cheating is an \ninteresting idea. however, it raises concerns about privacy and data security. it is important to \nensure that student data is protected and used only for educational purposes. \nin terms of the ml libraries comparison, each library has its own strengths and weaknesses. \ntensorflow is a popular and flexible library that allows for customization, while theano is a \nlow-level library that is well-suited for deep learning tasks. torch is known for its simplicity \nand a supportive community, and scikit-learn is designed for supervised and unsupervised \nlearning algorithms. caffe is focused on deep learning and encourages the use of existing \ndatasets. it is important to carefully consider the specific use case and the requirements of the \nproject before selecting an ml library. additionally, it is crucial to have a good understanding \nof the underlying algorithms and to have a solid grasp of programming and statistics to \neffectively use these tools. \nplan \n \nmy recommendation would be to carefully consider the specific needs of the project and the \nexpertise of the team members before selecting an ml library. it is important to choose a library \nthat is well-suited for the task at hand and that the team is comfortable working with. \nadditionally, it is crucial to prioritize data privacy and security and ensure that student data is \nused only for educational purposes and is protected from unauthorized access. finally, it is \nimportant to have a good understanding of the underlying algorithms and to have a solid grasp \nof programming and statistics to effectively use these tools. \n \n \nweek 5 \n \ndescribe \n \nfor week 5, i worked on the language understanding models and bert overview. i did some \nresearch on the transformer architecture overview and encoder part technical details. \n \n \n \n \n \n \ninterpret \n \n \n \ngot familiarised with the concept of pre-training in nlp. word embeddings are the basis of \ndeep learning for nlp, these are often pre-trained on text corpus from co-occurrence statistics. \nthey are applied in a context free manner. pre-training phase has language understanding while \nthe model tuning consists of classification, question answering, passage topic and sentiment \nanalysis. after the tuning the model transforms into encoder and decoder. i got to learn about \ndifferent networks such as recurrent neural networks and lstm (long short-term memory \nnetworks). recurrent neural network (rnn) is a type of artificial neural network which uses \nsequential data or time series data. while lstm are predominantly used to learn, process, and \nclassify sequential data because these networks can learn long-term dependencies between time \nsteps of data. common lstm applications include sentiment analysis, language modeling, \nspeech recognition, and video analysis. \n \nevaluate \n \npre-training has become a crucial step in natural language processing, as it allows for the \ncreation of more effective deep learning models. word embeddings are a key component of \nthis process, as they provide a way to represent words in a way that is more easily processed \nby machine learning algorithms. pre-training is typically done using large text corpora, which \nallow the model to learn the nuances of natural language in a way that is more representative \nof real-world usage. \nthe process of fine-tuning the pre-trained model involves training it on a specific task, such as \nsentiment analysis or question answering. this allows the model to specialize in a particular \ntask while still retaining its ability to understand natural language more broadly. \nrecurrent neural networks and lstm networks are two types of deep learning architectures \ncommonly used in nlp. rnns are designed to work with sequential data, making them well-\nsuited for natural language processing. lstm networks, on the other hand, are particularly \neffective at learning long-term dependencies in sequential data, which is useful in tasks such \nas language modeling or speech recognition. these types of networks are essential for natural \nlanguage processing tasks, as they allow for the processing of complex and nuanced language \ndata. \n \n \n \n \nplan \n \nmy recommendation would be to further explore and experiment with pre-training and fine-\ntuning techniques in nlp, as these methods have shown great promise in improving the \nperformance of deep learning models. it would also be beneficial to explore different types of \nneural networks and their applications in nlp tasks, as different networks may be more \neffective for certain tasks. finally, it is important to continue to stay up-to-date with the latest \nadvancements in nlp research and to actively incorporate them into any projects or \napplications in order to ensure that the most effective and innovative techniques are being \nutilized. \n \n \nweek 6 \n \ndescribe \n \nfor week 6, i worked on the bert and transformer. bert (bidirectional encoder \nrepresentations from transformers) is a powerful pre-trained deep learning model for natural \nlanguage processing. it uses a transformer architecture, which is a type of neural network \ndesigned to process sequential data, such as language. bert is unique in that it is pre-trained \nusing two tasks: masked language modeling and next sentence prediction. this allows it to \ndevelop a deeper understanding of the relationships between words in a sentence, and between \nsentences in a text. \ntransformers are a type of neural network architecture that have become increasingly popular \nin natural language processing tasks. they are designed to process sequential data, such as text, \nby breaking it down into smaller segments and analyzing the relationships between them. \ntransformers have proven to be highly effective at tasks such as language translation and \nquestion answering. in addition to bert, other notable transformer-based models include \ngpt-3 (generative pre-trained transformer 3) and t5 (text-to-text transfer transformer). \noverall, transformers have revolutionized the field of natural language processing, providing a \npowerful tool for developing highly accurate and efficient models for a wide range of \napplications. bert, in particular, has become a widely used model for a variety of nlp tasks \ndue to its high accuracy and ability to handle complex relationships within language data. \n \ninterpret \n \nthe architecture of the transformer is based on the concept of self-attention, which allows the \nmodel to weigh different parts of the input sequence differently when making predictions. the \nmodel consists of an encoder and a decoder, each of which contains multiple layers of self-\nattention and feedforward neural networks. \nthe encoder takes the input sequence and creates a sequence of hidden states, where each \nhidden state contains information about the input sequence up to a certain point. the decoder \ntakes the output sequence of the encoder and generates the final output sequence one token at \na time, while also attending to the input sequence using self-attention. \nthe self-attention mechanism is used in multiple layers of the transformer. each self-attention \nlayer computes a weighted sum of the input sequence, where the weights are determined by the \nsimilarity between each input token and every other token in the sequence. this allows the \nmodel to focus on the most relevant parts of the input sequence when making predictions. \nin addition to self-attention, the transformer also uses feedforward neural networks to process \nthe hidden states at each layer. the feedforward neural networks are applied to each hidden \nstate independently, allowing the model to learn non-linear transformations of the input \nsequence. \noverall, the transformer architecture has shown to be highly effective for a range of natural \nlanguage processing tasks and has been used as the basis for many state-of-the-art models, such \nas bert and gpt-3. \n \n \n \n \n \n \nthe flow of data through the transformer begins with the input sequence being passed through \nthe encoder. in each layer of the encoder, self-attention is used to compute a weighted sum of \nthe input sequence, which is then used to update the representation of each token in the \nsequence. the output of the encoder is a set of context vectors, each of which contains \ninformation about the entire input sequence. \n \n \n \n \nevaluate \n \nthe transformer architecture introduced a new approach to natural language processing by \nleveraging the power of self-attention. this approach allows the model to process an entire \ninput sequence at once, rather than processing it sequentially, which was the traditional \napproach. by using self-attention, the model can weigh different parts of the input sequence \ndifferently based on their importance, allowing for more accurate predictions. \nthe transformer model is highly parallelizable, which means that it can be trained much faster \nthan traditional sequential models. this is because each token in the input sequence can be \nprocessed independently of the others, which allows for more efficient use of computing \nresources. \nthe use of feedforward neural networks in the transformer architecture allows the model to \nlearn non-linear transformations of the input sequence, which is essential for processing natural \nlanguage. these non-linear transformations can capture complex patterns in the input data, \nallowing the model to make more accurate predictions. \nthe transformer has been used as the basis for many state-of-the-art models, such as bert \nand gpt-3. these models have achieved ground-breaking performance on a range of natural \nlanguage processing tasks, such as question answering, language translation, and text \ngeneration. \nin summary, the transformer architecture is a powerful tool for natural language processing, \nwhich has led to significant advancements in the field. its use of self-attention and feedforward \nneural networks allows for more accurate predictions and faster training times, making it a \npopular choice for many researchers and practitioners. \n \nplan \n \nmy recommendation would be to continue exploring the transformer architecture and its \napplications in natural language processing, particularly in the context of specific tasks or \napplications. it's also important to keep in mind that the field of nlp is rapidly evolving, so \nstaying up to date with the latest research and developments can be helpful for identifying new \nand innovative approaches to solving nlp problems. \n \n \nweek 7 \n \ndescribe \n \nfor week 7, i worked on adding new features to the exam marking project and to avoid the \ncase of contract cheating. i also used the bbc dataset to analyse the data with different machine \nlearning models to get better accuracies. we also discussed the transformer decoder block \nworkflow to implement that into our project. it is an essential component of the transformer \nmodel, which is used in natural language processing tasks such as machine learning translation, \ntext generation and sentiment analysis. i was also able to improve the accuracy of the model \nfrom the previous week and cleaned the data so that it does not overfit the model. we discussed \nthe concepts of transformer architecture and researched on different datasets to train and \nevaluate. \n \n \n \n \ninterpret \n \nwith the training and evaluating the model, i was able to make new insights into the dataset \nand contribute towards the project goals. i worked on the features list of the project and made \nseveral contributions towards the contract cheating cases and fine-tuned the model accordingly. \ni analysed the dataset using various machine learning algorithms and models to get a higher \naccuracy. the transformer decoder block was a crucial component which needed a high-level \nunderstanding of the concept and with all the discussions we were able to implement such \ncomplex concepts into our tasks. there was a significant improvement in the accuracy and the \nperformance of the model.  \n \nevaluate \n \ni actively worked on improving the features list, addressing contract cheating cases, and fine-\ntuning the model accordingly. by analysing the dataset using various machine learning \nalgorithms and models, i was able to achieve higher accuracy and by implementing the \ntransformer decoder block is indeed a complex task, requiring a high-level understanding of \nthe concept. i had a productive discussion and successfully incorporated this component into \nmy tasks. the significant improvement in accuracy and model performance was a positive \noutcome. overall, i was proactive, diligent, and successful in my tasks for this week, making \nvaluable contributions towards the project goals and achieving improved results. \n \nplan \n \nmy recommendations would be to further explore the model enhancements as were improving \nthe model accuracy and performances, it’s important to explore more ways of hyperparameter \ntuning and some advanced techniques to gain better results. we should also focus on expanding \nthe dataset and exploring more datasets which are relevant for the project.  \n \nweek 8 \n \ndescribe \n \nfor week 8, we were giving the task to explore the new asap dataset and analyse the data \nwith the bert model. i cleaned and processed the dataset so that i could train the model with \ndifferent parameters. this dataset contained the marks of the students from an english exam \nand different domain experts.  \n \ninterpret \n \nthis asap dataset was different from the bbc dataset and in order to compare the results i \nhad to test the machine learning model for both the datasets. after data pre-processing, i used \nthe bert pre-trained model to test the asap dataset and then compared the accuracy with the \nbbc dataset. both the models showed good results, but the bbc dataset performed better than \nthe asap dataset. \n \nevaluate \n \nthe task for this week was just to work on the asap dataset and to analyse the data so that \nthere are no noises in the dataset and the model could perform better on different machine \nlearning algorithms. the performance of the models is based on a specific metrics and the \naccuracy is just one metric to consider, we can also assess other metrics for both the models to \ncheck which one performed better. since both the datasets were trained on the same bert \nmodel, we can compare these results within the broader context of the project. \n \nplan \n \nmy recommendation would be to ensure that we analyse both the datasets carefully so that \nthere is not noise in the dataset because that could change the result accuracy of the model. \nremoving the outliers from the dataset is important so that it does not have any inconsistencies. \nfor the performance metrics, we could also check the roc curve and f1 scores from these \ndatasets to evaluate the model.  \n \n \nweek 9 \n \ndescribe \n \nfor week 9, i continued to work with bbc and asap datasets and tried to make the api for \nthe models which we have trained so that we could display all the techniques used in the data \nprocessing part and the model evaluation.  \n \ninterpret \n \ndue to the class imbalance in both the datasets it was difficult to further tune the model for \nbetter accuracy as asap dataset only got 56.3% test accuracy while the bbc dataset got 98% \ntest accuracy. but the bbc dataset was overfitting and because of different categories which \nwere distributed randomly it was hard to further clean the data. all the labels were changed \nand improved from before and the scoring rubrics was also updated for both the datasets. it \nwas a bit challenging to analyse the dataset with different characteristics. \n \nevaluate \n \nwell, these types of challenges which came with the datasets were common with such type of \ndata, so i need to further analyse and research about different techniques to improve this one. \ni’ll continue to redefine the data pre-processing methods addressing this issue of class \nimbalance. just need to add more advanced techniques for further testing of the model. \n \nplan \n \nmy recommendation would be to apply data augmentation techniques such as oversampling \nthe minority class, undersampling the majority class and generating some samples to balance \nthe class distribution. i will try to overcome the problem of overfitting in the dataset by cross \nvalidating the model and analysing the errors so that the model can give improvements in the \ndataset while training. \n \n \n \n \n \nweek 10 \n \ndescribe \n \nfor week 10, i worked on a new corpus dataset which contained english test score results of \ncandidates from an english exam in uk. the dataset had two different sets of answers with \nthe candidate scores. \n \ninterpret \n \nthis dataset was a bit different from the others because it was not well structured, there were \nmore than 1200 different files in the dataset from different years and were in different folders. \nto make this data readable i had to merge few xml files into one and saved those variables to \nanalyse the data. i had many problems with this dataset as it was not scaled properly, and the \nxml file was not good for the analyses of the model, so i cleaned the dataset further to optimise \nthe model performance. \n \nevaluate \n \nthe corpus dataset performed very bad in the initial test set as it resulted only 4% training \naccuracy because the dataset was not pre-processed properly. i further tried to merge more files \nand then did the dataset evaluation for the training on the bert model. \n \nplan \n \nmy recommendation would be to get all the data into one csv file and then try to analyse the \ndataset rather than having multiple xml files for the dataset. more data pre-processing is \nrequired for this dataset and need to check if there is any irrelevant information which needs to \nbe removed from the dataset before testing. \n \n \nweek 11 \n \ndescribe \n \nfor week 11, i finalised my report on all the datasets which i used for the evaluation of the \nmachine learning model. also, i was able to convert the corpus dataset xml files into one csv \nto analyse the data. \n \ninterpret \n \nafter generating a single csv file for the dataset, i did some data visualisation and data cleaning \nto run the bert model on this dataset. this dataset did not perform well on the model accuracy \nand further analysis of the model is required. also made a final report for the bbc and asap \ndataset to submit during the friday meeting session. i also made few plots to visualise both the \ndatasets and did continue with the testing of all three models. \n \n \n \nevaluate \n \nthe data visualisation and cleaning helped in gaining the new insights about the dataset and i \naddressed some issues like inconsistencies in the data and the missing values that could affect \nthe performance of the model. i did all the model testing and saved the accuracy results to \ncompare between the models and completed the final report for the project by visualising the \ndata and identifying potential areas of improvement or further analysis. i made some progress \nwith the dataset training, cleaning the dataset and running the bert model.  \n \nplan \n \nmy recommendation would be to apply data augmentation techniques such as oversampling \nthe minority class, undersampling the majority class and generating some samples to balance \nthe class distribution. i will try to overcome the problem of overfitting in the dataset by cross \nvalidating the model and analysing the errors so that the model can give improvements in the \ndataset while training. further testing of the dataset is required in order to compare the results. \n \n \nweek 12 \n \ndescribe \n \nfor the week 12, i did all the submission to my manager for the tasks which i performed during \nmy internship. since it was the last week, we didn’t have any tasks to perform. i finalised my \ninternship report and we had a nice office lunch to end the internship. \n \n \ninterpret \n \ni saved all the checkpoints and the files for the project on the itic learning portal for the future \ninternship team to work on and mailed the performance reports for the models to my manager.  \n \nevaluate \n \ni had a great experience working with the itic team on this internship project i have gained a \nlot of knowledge about the model testing and training using different parameters. i learned how \nto develop ui designs and develop apis for any model. \n \nplan \n \nmy recommendation would be to continue working on this automatic exam marking project \nand improve the quality of the dataset and the features of the ui design. as i move forward in \nmy career, this will be an important learning outcome for me, and i really enjoyed working at \nitic. \n \n \n \n",
    "page": null,
    "goal": "Reflective journal Entities",
    "children": [
        {
            "id": "1.1",
            "name": "week 1",
            "nodeType": "title",
            "text": "week 1",
            "page": null,
            "goal": "week 1",
            "children": []
        },
        {
            "id": "1.2",
            "name": "week 1 describe",
            "nodeType": "paragraph",
            "text": "week 1 \n \ndescribe \n \nin week 1, i met my team members and my supervisor masoud saifian who is a phd student \nat macquarie. got a chance to interact with the ceo stephen elbourn and discuss about the \nproject which we will be working on. had a brief presentation about the company and their \npolicies, how they deal with the clients and how the organization works. we were provided the \nproject timeline and a general discussion on how we will be implementing the concept into the \nproject. had a little office tour on the first day to see the office environment. i went through \nsome basic concepts of data analysis and the introduction to some new tools such as gradescope \nbecause that is similar to our project, just to see the functionality and the interface of that tool. \nwe were then given the access to different tools to communicate such as confluence, microsoft \nteams and itic systems internal login credentials. \n \ninterpret \n \ni worked with new tools and gained some knowledge about different concepts of data \ngeneration. did some work with chatgpt for the request of json format data samples which \nare mentioned below. \n \nthe chatgpt api accepts and returns data in json format, which stands for javascript object \nnotation. json is a lightweight and widely used data interchange format that is easy to read \nand write for both humans and machines. here is an example of how a request to the chatgpt \napi might look in json format. \n \n \nswift \ncopy code \n{ \n\"model\": \"text-davinci-002\", \n\"prompt\": \"what is the capital of france?\", \n\"temperature\": 0.7, \n\"max_tokens\": 50, \n\"stop\": [\"\\n\"] \n} \n \nin this example, the request includes the following parameters: \n \n\"model\": specifies the language model to use (in this case, \"text-davinci-002\"). \n\"prompt\": provides the input text to generate a response for (in this case, \"what \nis the capital of france?\"). \n\"temperature\": controls the randomness and creativity of the generated \nresponse (in this case, 0.7). \n\"max_tokens\": specifies the maximum length of the generated response (in this \ncase, 50 tokens). \n \n\"stop\": specifies a list of strings that, if generated, will be used to signal the end of \nthe generated response (in this case, [\"\\n\"], which represents a newline \ncharacter). \nthe response from the chatgpt api will also be in json format and will include \nthe generated text in a \"choices\" array: \n \njson \ncopy code \n{ \n\"choices\": [ \n{ \n\"text\": \"the capital of france is paris.\", \n\"index\": 0, \n\"logprobs\": null, \n\"finish_reason\": \"stop\" \n} \n] \n} \n \nin this example, the response includes the generated text as well as additional information about \nthe response, such as the index of the selected response, the log probabilities of each choice, \nand the reason for finishing the response. \n \n \n \n \nevaluate \n \ni considered this week to be helpful for the future after learning about organization, working, \nand projects because i was receiving experience working in a team within a firm and learning \nabout the business model given by the company professionals. i discovered the importance of \nteamwork through various forms of communication by examining the employees of the \norganisation.  \n \nplan \n \nall the insights which i took this week was new to me and i gained a lot of knowledge about \nthe different tools and techniques which we will implement in the upcoming weeks. this helped \nme in better understanding of the work and while giving the presentation and sharing my ideas \ni gained a lot of confidence in my work. i will continue to learn and implement those ideas for \nthe rest of the training period.",
            "page": null,
            "goal": "week 1 \n \ndescribe \n \nin week 1, i met my team members and my supervisor masoud saifian who is a phd student \nat macquarie. got a chance to interact with the ceo stephen elbourn and discuss about the \nproject which we will be working on. had a brief presentation about the company and their \npolicies, how they deal with the clients and how the organization works. we were provided the \nproject timeline and a general discussion on how we will be implementing the concept into the \nproject. had a little office tour on the first day to see the office environment. i went through \nsome basic concepts of data analysis and the introduction to some new tools such as gradescope \nbecause that is similar to our project, just to see the functionality and the interface of that tool. \nwe were then given the access to different tools to communicate such as confluence, microsoft \nteams and itic systems internal login credentials. \n \ninterpret \n \ni worked with new tools and gained some knowledge about different concepts of data \ngeneration. did some work with chatgpt for the request of json format data samples which \nare mentioned below. \n \nthe chatgpt api accepts and returns data in json format, which stands for javascript object \nnotation. json is a lightweight and widely used data interchange format that is easy to read \nand write for both humans and machines. here is an example of how a request to the chatgpt \napi might look in json format. \n \n \nswift \ncopy code \n{ \n\"model\": \"text-davinci-002\", \n\"prompt\": \"what is the capital of france?\", \n\"temperature\": 0.7, \n\"max_tokens\": 50, \n\"stop\": [\"\\n\"] \n} \n \nin this example, the request includes the following parameters: \n \n\"model\": specifies the language model to use (in this case, \"text-davinci-002\"). \n\"prompt\": provides the input text to generate a response for (in this case, \"what \nis the capital of france?\"). \n\"temperature\": controls the randomness and creativity of the generated \nresponse (in this case, 0.7). \n\"max_tokens\": specifies the maximum length of the generated response (in this \ncase, 50 tokens). \n \n\"stop\": specifies a list of strings that, if generated, will be used to signal the end of \nthe generated response (in this case, [\"\\n\"], which represents a newline \ncharacter). \nthe response from the chatgpt api will also be in json format and will include \nthe generated text in a \"choices\" array: \n \njson \ncopy code \n{ \n\"choices\": [ \n{ \n\"text\": \"the capital of france is paris.\", \n\"index\": 0, \n\"logprobs\": null, \n\"finish_reason\": \"stop\" \n} \n] \n} \n \nin this example, the response includes the generated text as well as additional information about \nthe response, such as the index of the selected response, the log probabilities of each choice, \nand the reason for finishing the response. \n \n \n \n \nevaluate \n \ni considered this week to be helpful for the future after learning about organization, working, \nand projects because i was receiving experience working in a team within a firm and learning \nabout the business model given by the company professionals. i discovered the importance of \nteamwork through various forms of communication by examining the employees of the \norganisation.  \n \nplan \n \nall the insights which i took this week was new to me and i gained a lot of knowledge about \nthe different tools and techniques which we will implement in the upcoming weeks. this helped \nme in better understanding of the work and while giving the presentation and sharing my ideas \ni gained a lot of confidence in my work. i will continue to learn and implement those ideas for \nthe rest of the training period.",
            "children": []
        },
        {
            "id": "1.3",
            "name": "week 2",
            "nodeType": "title",
            "text": "week 2",
            "page": null,
            "goal": "week 2",
            "children": []
        },
        {
            "id": "1.4",
            "name": "week 2 describe",
            "nodeType": "paragraph",
            "text": "week 2 \n \ndescribe \n \nfor week 2, we focused on preparing the project management plan to keep the track of our \nprogress. then i researched on some commercial tools to check the functionality, facilities, \nmethods, and technologies which are provided in those products such as e-rater scoring engine, \ndeep score and gradescope. after that i did some code analysis about the gan method for \ngenerating essay datasets and compared them to the current scoring ml approaches. \n \ninterpret \n \nfor this week, i worked on the project management plan and wireframe design for the project \nwhich included project documentation plan where we worked on the document versioning \nstructure and selecting the tool for the document management. i updated the project gantt chart \nand the project progress report. i was also assigned the task for the data source generation using \nchatgpt. i used json formats to gather data from chatgpt by recognising the limitations of \nthe problem question and selecting the sample question for different fields such as data science, \ncyber security, and networking. then i prepared a dummy dataset for the evaluation of the \nchatgpt questions. for the weekly friday presentation, i worked on the wireframe designs of \nthe commercial tools to check their functionality in different roles and users, also their scope \nand ui functions. compared the ui and scope of these tools and did a gap analysis with mock \nup for the design wireframe.  \n \n \ndesign ideas for the custom rubrics and marking criteria. \n \n \n \n \nanalysis of the questions according to the different tools and rubrics which are marked by the \ntutor. \n \nevaluate \n \nhad a busy week working on project management tasks, researching commercial tools, \nanalysing code, generating data sources using chatgpt, and preparing wireframe designs for \ncommercial tools. i gained a good level of knowledge and skills in project management, data \nanalysis, and ui design. i was also able to recognize the limitations of the problem question \nand select appropriate sample questions for different fields. overall, i had a productive and \nfulfilling week, working on various aspects of the project, and contributing to its progress. \n \n \n \n \nplan \n \nwhile working with the dummy dataset i noticed that chatgpt was not able to generate large \ndataset of questions and some of the questions were also repeating. this was one of the \nchallenges which i faced while generating the dataset. for the next week my plan is to work \non the existing gan for the generation of essay dataset and research on the existing ml \nmethods for essay scoring.",
            "page": null,
            "goal": "week 2 \n \ndescribe \n \nfor week 2, we focused on preparing the project management plan to keep the track of our \nprogress. then i researched on some commercial tools to check the functionality, facilities, \nmethods, and technologies which are provided in those products such as e-rater scoring engine, \ndeep score and gradescope. after that i did some code analysis about the gan method for \ngenerating essay datasets and compared them to the current scoring ml approaches. \n \ninterpret \n \nfor this week, i worked on the project management plan and wireframe design for the project \nwhich included project documentation plan where we worked on the document versioning \nstructure and selecting the tool for the document management. i updated the project gantt chart \nand the project progress report. i was also assigned the task for the data source generation using \nchatgpt. i used json formats to gather data from chatgpt by recognising the limitations of \nthe problem question and selecting the sample question for different fields such as data science, \ncyber security, and networking. then i prepared a dummy dataset for the evaluation of the \nchatgpt questions. for the weekly friday presentation, i worked on the wireframe designs of \nthe commercial tools to check their functionality in different roles and users, also their scope \nand ui functions. compared the ui and scope of these tools and did a gap analysis with mock \nup for the design wireframe.  \n \n \ndesign ideas for the custom rubrics and marking criteria. \n \n \n \n \nanalysis of the questions according to the different tools and rubrics which are marked by the \ntutor. \n \nevaluate \n \nhad a busy week working on project management tasks, researching commercial tools, \nanalysing code, generating data sources using chatgpt, and preparing wireframe designs for \ncommercial tools. i gained a good level of knowledge and skills in project management, data \nanalysis, and ui design. i was also able to recognize the limitations of the problem question \nand select appropriate sample questions for different fields. overall, i had a productive and \nfulfilling week, working on various aspects of the project, and contributing to its progress. \n \n \n \n \nplan \n \nwhile working with the dummy dataset i noticed that chatgpt was not able to generate large \ndataset of questions and some of the questions were also repeating. this was one of the \nchallenges which i faced while generating the dataset. for the next week my plan is to work \non the existing gan for the generation of essay dataset and research on the existing ml \nmethods for essay scoring.",
            "children": []
        },
        {
            "id": "1.5",
            "name": "week 3",
            "nodeType": "title",
            "text": "week 3",
            "page": null,
            "goal": "week 3",
            "children": []
        },
        {
            "id": "1.6",
            "name": "week 3 describe",
            "nodeType": "paragraph",
            "text": "week 3 \n \ndescribe \n \nfor week 3, my task was to work on existing gan for essay dataset generation (research and \ncode analysis) and had to research on different ml techniques on essay scoring followed by \nthe code analysis. i also worked on the bert model and was involved with the kaggle analysis \nof the public dataset. generated a list of questions which can provide a list of available domains \nand provide a number of essays and corresponding prompts.  \n \n \ninterpret \n \nessaygan has multiple generators and a discriminator to generate essays rated with different \nscores, where each generator is dedicated to producing essays with a specific score. the \ndiscriminator is trained to distinguish between real and generated essays. essaygan predicts \nthe next sentence in the sequence of the essay, rather than the next word, as done in ordinary-\ntext gan models. the proposed method can solve the high cost of collecting human-rated \nessays by generating essays automatically for training the aes system. \n \n \n \nautomated essay scoring (aes) is a popular area of research in learning analytics due to the \npotential for artificial intelligence to supplement human creativity. however, much of the \nresearch focuses solely on holistic scoring without delving into the rubric level, which is the \nfoundation of how holistic scores are determined. this lack of attention to rubric-level scoring \nhas made the aes black box difficult to understand. although some explainable artificial \nintelligence algorithms have been developed recently, none have explored the potential role of \nthese explanation models in understanding the decision-making process behind aes, \nimproving predictive models, or providing personalized feedback to students during the writing \nprocess. \n \nevalaute \n \nthe proposed essaygan system appears to be a promising solution to the problem of high \ncost and limited availability of human-rated essays for training automated essay scoring \nsystems. by using multiple generators dedicated to producing essays with specific scores, \nessaygan can generate a large volume of training data that closely resembles real essays. \nadditionally, the use of a discriminator helps to ensure that the generated essays are of high \nquality and similar to real essays. \non the other hand, it has a significant limitation in current aes research, which is the lack of \nattention to rubric-level scoring. this means that the underlying decision-making process of \naes remains unclear, and the aes black box is difficult to understand. the development of \nexplainable artificial intelligence algorithms may help to address this limitation by providing \ninsights into the decision-making process of aes models and improving their generalizability \nand interpretability. \nfinally, i can say that there is potential for ai to provide personalized, formative, and fine-\ngrained feedback to students during the writing process. this could help to improve the quality \nof student writing and support their learning and development. \n \n \nplan \n \nbased on the insights, my recommendation would be to continue exploring the use of \nessaygan and other ai-based approaches for generating training data for automated essay \nscoring systems. at the same time, researchers should focus on developing explainable \nartificial intelligence algorithms to gain a better understanding of the underlying decision-\nmaking process of aes models and improve their interpretability and generalizability. \nmoreover, it is crucial to pay attention to rubric-level scoring to develop a more comprehensive \nunderstanding of how holistic scores are determined. finally, it would be beneficial to explore \nhow ai can provide personalized, formative, and fine-grained feedback to students during the \nwriting process to support their learning and development.",
            "page": null,
            "goal": "week 3 \n \ndescribe \n \nfor week 3, my task was to work on existing gan for essay dataset generation (research and \ncode analysis) and had to research on different ml techniques on essay scoring followed by \nthe code analysis. i also worked on the bert model and was involved with the kaggle analysis \nof the public dataset. generated a list of questions which can provide a list of available domains \nand provide a number of essays and corresponding prompts.  \n \n \ninterpret \n \nessaygan has multiple generators and a discriminator to generate essays rated with different \nscores, where each generator is dedicated to producing essays with a specific score. the \ndiscriminator is trained to distinguish between real and generated essays. essaygan predicts \nthe next sentence in the sequence of the essay, rather than the next word, as done in ordinary-\ntext gan models. the proposed method can solve the high cost of collecting human-rated \nessays by generating essays automatically for training the aes system. \n \n \n \nautomated essay scoring (aes) is a popular area of research in learning analytics due to the \npotential for artificial intelligence to supplement human creativity. however, much of the \nresearch focuses solely on holistic scoring without delving into the rubric level, which is the \nfoundation of how holistic scores are determined. this lack of attention to rubric-level scoring \nhas made the aes black box difficult to understand. although some explainable artificial \nintelligence algorithms have been developed recently, none have explored the potential role of \nthese explanation models in understanding the decision-making process behind aes, \nimproving predictive models, or providing personalized feedback to students during the writing \nprocess. \n \nevalaute \n \nthe proposed essaygan system appears to be a promising solution to the problem of high \ncost and limited availability of human-rated essays for training automated essay scoring \nsystems. by using multiple generators dedicated to producing essays with specific scores, \nessaygan can generate a large volume of training data that closely resembles real essays. \nadditionally, the use of a discriminator helps to ensure that the generated essays are of high \nquality and similar to real essays. \non the other hand, it has a significant limitation in current aes research, which is the lack of \nattention to rubric-level scoring. this means that the underlying decision-making process of \naes remains unclear, and the aes black box is difficult to understand. the development of \nexplainable artificial intelligence algorithms may help to address this limitation by providing \ninsights into the decision-making process of aes models and improving their generalizability \nand interpretability. \nfinally, i can say that there is potential for ai to provide personalized, formative, and fine-\ngrained feedback to students during the writing process. this could help to improve the quality \nof student writing and support their learning and development. \n \n \nplan \n \nbased on the insights, my recommendation would be to continue exploring the use of \nessaygan and other ai-based approaches for generating training data for automated essay \nscoring systems. at the same time, researchers should focus on developing explainable \nartificial intelligence algorithms to gain a better understanding of the underlying decision-\nmaking process of aes models and improve their interpretability and generalizability. \nmoreover, it is crucial to pay attention to rubric-level scoring to develop a more comprehensive \nunderstanding of how holistic scores are determined. finally, it would be beneficial to explore \nhow ai can provide personalized, formative, and fine-grained feedback to students during the \nwriting process to support their learning and development.",
            "children": []
        },
        {
            "id": "1.7",
            "name": "week 4",
            "nodeType": "title",
            "text": "week 4",
            "page": null,
            "goal": "week 4",
            "children": []
        },
        {
            "id": "1.8",
            "name": "week 4 describe",
            "nodeType": "paragraph",
            "text": "week 4 \n \ndescribe \n \nfor week 4, i worked on the architecture to detect contract cheating and to generate ai reports \nand essays related to the submitted assignment or exam. i also had to research about some ml \nlibraries for the project such as tensorflow, theono, torch, scikit-learn and caffe. my \nsupervisor presented some slides for the architecture, and we had a brief discussion about the \nconcepts for the ai generated essay and reports. then i was assigned to finalise the architecture \nand all the concepts which are required.  \n \ninterpret \n \n \n \n \nthe proposed concept regarding a potential alternative to detect contract cheating and ai \ngenerated reports and essays. \n \nin this part, we can analyse the submitted paper and then summarise that using ai tools to \ngenerate few questions related to the exam or assignment. then we can send an email invite to \nthe student which will be automated so that they can join the video call or webchat to give the \nsummary about their report. we can use facial recognition to match the face on webcam and \ncan use image library to map the studentid. all the interaction will be recorded and students’ \nresponse will then be analysed and compared to his previous responses to predict the quality \nof the answers. this can be used to analyse and give a better result to the students and avoid \nany form of plagiarism or contract cheating. \nfor the ml libraries, i did some research and gave a presentation on the different libraries \nwhich are mentioned above in the description.  \n \n \ni compared and analysed different ml libraries and their use cases in different tasks to access \nthe models and their uses. tensorflow is based on the possibility of using it both for research \nand recurring machine learning tasks. it allows you to have a full control over models and train \nthem using your own dataset. while theono is a low-level library for scientific computing \nbased on python, which is used to target deep learning tasks related to defining, optimizing, \nand evaluating mathematical expressions. torch is a simple scripting language, and a helpful \ncommunity sharing an impressive array of tutorials and packages for almost any deep learning \npurpose. scikit-learn is a high-level framework designed for supervised and unsupervised \nmachine learning algorithms. caffe encourages users to get familiar with the datasets provided \nby both the industry and other users.  \n \nevaluate \n \nthe proposed approach of using ai tools to generate questions and analyse student responses \nusing facial recognition and image libraries to avoid plagiarism and contract cheating is an \ninteresting idea. however, it raises concerns about privacy and data security. it is important to \nensure that student data is protected and used only for educational purposes. \nin terms of the ml libraries comparison, each library has its own strengths and weaknesses. \ntensorflow is a popular and flexible library that allows for customization, while theano is a \nlow-level library that is well-suited for deep learning tasks. torch is known for its simplicity \nand a supportive community, and scikit-learn is designed for supervised and unsupervised \nlearning algorithms. caffe is focused on deep learning and encourages the use of existing \ndatasets. it is important to carefully consider the specific use case and the requirements of the \nproject before selecting an ml library. additionally, it is crucial to have a good understanding \nof the underlying algorithms and to have a solid grasp of programming and statistics to \neffectively use these tools. \nplan \n \nmy recommendation would be to carefully consider the specific needs of the project and the \nexpertise of the team members before selecting an ml library. it is important to choose a library \nthat is well-suited for the task at hand and that the team is comfortable working with. \nadditionally, it is crucial to prioritize data privacy and security and ensure that student data is \nused only for educational purposes and is protected from unauthorized access. finally, it is \nimportant to have a good understanding of the underlying algorithms and to have a solid grasp \nof programming and statistics to effectively use these tools.",
            "page": null,
            "goal": "week 4 \n \ndescribe \n \nfor week 4, i worked on the architecture to detect contract cheating and to generate ai reports \nand essays related to the submitted assignment or exam. i also had to research about some ml \nlibraries for the project such as tensorflow, theono, torch, scikit-learn and caffe. my \nsupervisor presented some slides for the architecture, and we had a brief discussion about the \nconcepts for the ai generated essay and reports. then i was assigned to finalise the architecture \nand all the concepts which are required.  \n \ninterpret \n \n \n \n \nthe proposed concept regarding a potential alternative to detect contract cheating and ai \ngenerated reports and essays. \n \nin this part, we can analyse the submitted paper and then summarise that using ai tools to \ngenerate few questions related to the exam or assignment. then we can send an email invite to \nthe student which will be automated so that they can join the video call or webchat to give the \nsummary about their report. we can use facial recognition to match the face on webcam and \ncan use image library to map the studentid. all the interaction will be recorded and students’ \nresponse will then be analysed and compared to his previous responses to predict the quality \nof the answers. this can be used to analyse and give a better result to the students and avoid \nany form of plagiarism or contract cheating. \nfor the ml libraries, i did some research and gave a presentation on the different libraries \nwhich are mentioned above in the description.  \n \n \ni compared and analysed different ml libraries and their use cases in different tasks to access \nthe models and their uses. tensorflow is based on the possibility of using it both for research \nand recurring machine learning tasks. it allows you to have a full control over models and train \nthem using your own dataset. while theono is a low-level library for scientific computing \nbased on python, which is used to target deep learning tasks related to defining, optimizing, \nand evaluating mathematical expressions. torch is a simple scripting language, and a helpful \ncommunity sharing an impressive array of tutorials and packages for almost any deep learning \npurpose. scikit-learn is a high-level framework designed for supervised and unsupervised \nmachine learning algorithms. caffe encourages users to get familiar with the datasets provided \nby both the industry and other users.  \n \nevaluate \n \nthe proposed approach of using ai tools to generate questions and analyse student responses \nusing facial recognition and image libraries to avoid plagiarism and contract cheating is an \ninteresting idea. however, it raises concerns about privacy and data security. it is important to \nensure that student data is protected and used only for educational purposes. \nin terms of the ml libraries comparison, each library has its own strengths and weaknesses. \ntensorflow is a popular and flexible library that allows for customization, while theano is a \nlow-level library that is well-suited for deep learning tasks. torch is known for its simplicity \nand a supportive community, and scikit-learn is designed for supervised and unsupervised \nlearning algorithms. caffe is focused on deep learning and encourages the use of existing \ndatasets. it is important to carefully consider the specific use case and the requirements of the \nproject before selecting an ml library. additionally, it is crucial to have a good understanding \nof the underlying algorithms and to have a solid grasp of programming and statistics to \neffectively use these tools. \nplan \n \nmy recommendation would be to carefully consider the specific needs of the project and the \nexpertise of the team members before selecting an ml library. it is important to choose a library \nthat is well-suited for the task at hand and that the team is comfortable working with. \nadditionally, it is crucial to prioritize data privacy and security and ensure that student data is \nused only for educational purposes and is protected from unauthorized access. finally, it is \nimportant to have a good understanding of the underlying algorithms and to have a solid grasp \nof programming and statistics to effectively use these tools.",
            "children": []
        },
        {
            "id": "1.9",
            "name": "week 5",
            "nodeType": "title",
            "text": "week 5",
            "page": null,
            "goal": "week 5",
            "children": []
        },
        {
            "id": "1.10",
            "name": "week 5 describe",
            "nodeType": "paragraph",
            "text": "week 5 \n \ndescribe \n \nfor week 5, i worked on the language understanding models and bert overview. i did some \nresearch on the transformer architecture overview and encoder part technical details. \n \n \n \n \n \n \ninterpret \n \n \n \ngot familiarised with the concept of pre-training in nlp. word embeddings are the basis of \ndeep learning for nlp, these are often pre-trained on text corpus from co-occurrence statistics. \nthey are applied in a context free manner. pre-training phase has language understanding while \nthe model tuning consists of classification, question answering, passage topic and sentiment \nanalysis. after the tuning the model transforms into encoder and decoder. i got to learn about \ndifferent networks such as recurrent neural networks and lstm (long short-term memory \nnetworks). recurrent neural network (rnn) is a type of artificial neural network which uses \nsequential data or time series data. while lstm are predominantly used to learn, process, and \nclassify sequential data because these networks can learn long-term dependencies between time \nsteps of data. common lstm applications include sentiment analysis, language modeling, \nspeech recognition, and video analysis. \n \nevaluate \n \npre-training has become a crucial step in natural language processing, as it allows for the \ncreation of more effective deep learning models. word embeddings are a key component of \nthis process, as they provide a way to represent words in a way that is more easily processed \nby machine learning algorithms. pre-training is typically done using large text corpora, which \nallow the model to learn the nuances of natural language in a way that is more representative \nof real-world usage. \nthe process of fine-tuning the pre-trained model involves training it on a specific task, such as \nsentiment analysis or question answering. this allows the model to specialize in a particular \ntask while still retaining its ability to understand natural language more broadly. \nrecurrent neural networks and lstm networks are two types of deep learning architectures \ncommonly used in nlp. rnns are designed to work with sequential data, making them well-\nsuited for natural language processing. lstm networks, on the other hand, are particularly \neffective at learning long-term dependencies in sequential data, which is useful in tasks such \nas language modeling or speech recognition. these types of networks are essential for natural \nlanguage processing tasks, as they allow for the processing of complex and nuanced language \ndata. \n \n \n \n \nplan \n \nmy recommendation would be to further explore and experiment with pre-training and fine-\ntuning techniques in nlp, as these methods have shown great promise in improving the \nperformance of deep learning models. it would also be beneficial to explore different types of \nneural networks and their applications in nlp tasks, as different networks may be more \neffective for certain tasks. finally, it is important to continue to stay up-to-date with the latest \nadvancements in nlp research and to actively incorporate them into any projects or \napplications in order to ensure that the most effective and innovative techniques are being \nutilized.",
            "page": null,
            "goal": "week 5 \n \ndescribe \n \nfor week 5, i worked on the language understanding models and bert overview. i did some \nresearch on the transformer architecture overview and encoder part technical details. \n \n \n \n \n \n \ninterpret \n \n \n \ngot familiarised with the concept of pre-training in nlp. word embeddings are the basis of \ndeep learning for nlp, these are often pre-trained on text corpus from co-occurrence statistics. \nthey are applied in a context free manner. pre-training phase has language understanding while \nthe model tuning consists of classification, question answering, passage topic and sentiment \nanalysis. after the tuning the model transforms into encoder and decoder. i got to learn about \ndifferent networks such as recurrent neural networks and lstm (long short-term memory \nnetworks). recurrent neural network (rnn) is a type of artificial neural network which uses \nsequential data or time series data. while lstm are predominantly used to learn, process, and \nclassify sequential data because these networks can learn long-term dependencies between time \nsteps of data. common lstm applications include sentiment analysis, language modeling, \nspeech recognition, and video analysis. \n \nevaluate \n \npre-training has become a crucial step in natural language processing, as it allows for the \ncreation of more effective deep learning models. word embeddings are a key component of \nthis process, as they provide a way to represent words in a way that is more easily processed \nby machine learning algorithms. pre-training is typically done using large text corpora, which \nallow the model to learn the nuances of natural language in a way that is more representative \nof real-world usage. \nthe process of fine-tuning the pre-trained model involves training it on a specific task, such as \nsentiment analysis or question answering. this allows the model to specialize in a particular \ntask while still retaining its ability to understand natural language more broadly. \nrecurrent neural networks and lstm networks are two types of deep learning architectures \ncommonly used in nlp. rnns are designed to work with sequential data, making them well-\nsuited for natural language processing. lstm networks, on the other hand, are particularly \neffective at learning long-term dependencies in sequential data, which is useful in tasks such \nas language modeling or speech recognition. these types of networks are essential for natural \nlanguage processing tasks, as they allow for the processing of complex and nuanced language \ndata. \n \n \n \n \nplan \n \nmy recommendation would be to further explore and experiment with pre-training and fine-\ntuning techniques in nlp, as these methods have shown great promise in improving the \nperformance of deep learning models. it would also be beneficial to explore different types of \nneural networks and their applications in nlp tasks, as different networks may be more \neffective for certain tasks. finally, it is important to continue to stay up-to-date with the latest \nadvancements in nlp research and to actively incorporate them into any projects or \napplications in order to ensure that the most effective and innovative techniques are being \nutilized.",
            "children": []
        },
        {
            "id": "1.11",
            "name": "week 6",
            "nodeType": "title",
            "text": "week 6",
            "page": null,
            "goal": "week 6",
            "children": []
        },
        {
            "id": "1.12",
            "name": "week 6 describe",
            "nodeType": "paragraph",
            "text": "week 6 \n \ndescribe \n \nfor week 6, i worked on the bert and transformer. bert (bidirectional encoder \nrepresentations from transformers) is a powerful pre-trained deep learning model for natural \nlanguage processing. it uses a transformer architecture, which is a type of neural network \ndesigned to process sequential data, such as language. bert is unique in that it is pre-trained \nusing two tasks: masked language modeling and next sentence prediction. this allows it to \ndevelop a deeper understanding of the relationships between words in a sentence, and between \nsentences in a text. \ntransformers are a type of neural network architecture that have become increasingly popular \nin natural language processing tasks. they are designed to process sequential data, such as text, \nby breaking it down into smaller segments and analyzing the relationships between them. \ntransformers have proven to be highly effective at tasks such as language translation and \nquestion answering. in addition to bert, other notable transformer-based models include \ngpt-3 (generative pre-trained transformer 3) and t5 (text-to-text transfer transformer). \noverall, transformers have revolutionized the field of natural language processing, providing a \npowerful tool for developing highly accurate and efficient models for a wide range of \napplications. bert, in particular, has become a widely used model for a variety of nlp tasks \ndue to its high accuracy and ability to handle complex relationships within language data. \n \ninterpret \n \nthe architecture of the transformer is based on the concept of self-attention, which allows the \nmodel to weigh different parts of the input sequence differently when making predictions. the \nmodel consists of an encoder and a decoder, each of which contains multiple layers of self-\nattention and feedforward neural networks. \nthe encoder takes the input sequence and creates a sequence of hidden states, where each \nhidden state contains information about the input sequence up to a certain point. the decoder \ntakes the output sequence of the encoder and generates the final output sequence one token at \na time, while also attending to the input sequence using self-attention. \nthe self-attention mechanism is used in multiple layers of the transformer. each self-attention \nlayer computes a weighted sum of the input sequence, where the weights are determined by the \nsimilarity between each input token and every other token in the sequence. this allows the \nmodel to focus on the most relevant parts of the input sequence when making predictions. \nin addition to self-attention, the transformer also uses feedforward neural networks to process \nthe hidden states at each layer. the feedforward neural networks are applied to each hidden \nstate independently, allowing the model to learn non-linear transformations of the input \nsequence. \noverall, the transformer architecture has shown to be highly effective for a range of natural \nlanguage processing tasks and has been used as the basis for many state-of-the-art models, such \nas bert and gpt-3. \n \n \n \n \n \n \nthe flow of data through the transformer begins with the input sequence being passed through \nthe encoder. in each layer of the encoder, self-attention is used to compute a weighted sum of \nthe input sequence, which is then used to update the representation of each token in the \nsequence. the output of the encoder is a set of context vectors, each of which contains \ninformation about the entire input sequence. \n \n \n \n \nevaluate \n \nthe transformer architecture introduced a new approach to natural language processing by \nleveraging the power of self-attention. this approach allows the model to process an entire \ninput sequence at once, rather than processing it sequentially, which was the traditional \napproach. by using self-attention, the model can weigh different parts of the input sequence \ndifferently based on their importance, allowing for more accurate predictions. \nthe transformer model is highly parallelizable, which means that it can be trained much faster \nthan traditional sequential models. this is because each token in the input sequence can be \nprocessed independently of the others, which allows for more efficient use of computing \nresources. \nthe use of feedforward neural networks in the transformer architecture allows the model to \nlearn non-linear transformations of the input sequence, which is essential for processing natural \nlanguage. these non-linear transformations can capture complex patterns in the input data, \nallowing the model to make more accurate predictions. \nthe transformer has been used as the basis for many state-of-the-art models, such as bert \nand gpt-3. these models have achieved ground-breaking performance on a range of natural \nlanguage processing tasks, such as question answering, language translation, and text \ngeneration. \nin summary, the transformer architecture is a powerful tool for natural language processing, \nwhich has led to significant advancements in the field. its use of self-attention and feedforward \nneural networks allows for more accurate predictions and faster training times, making it a \npopular choice for many researchers and practitioners. \n \nplan \n \nmy recommendation would be to continue exploring the transformer architecture and its \napplications in natural language processing, particularly in the context of specific tasks or \napplications. it's also important to keep in mind that the field of nlp is rapidly evolving, so \nstaying up to date with the latest research and developments can be helpful for identifying new \nand innovative approaches to solving nlp problems.",
            "page": null,
            "goal": "week 6 \n \ndescribe \n \nfor week 6, i worked on the bert and transformer. bert (bidirectional encoder \nrepresentations from transformers) is a powerful pre-trained deep learning model for natural \nlanguage processing. it uses a transformer architecture, which is a type of neural network \ndesigned to process sequential data, such as language. bert is unique in that it is pre-trained \nusing two tasks: masked language modeling and next sentence prediction. this allows it to \ndevelop a deeper understanding of the relationships between words in a sentence, and between \nsentences in a text. \ntransformers are a type of neural network architecture that have become increasingly popular \nin natural language processing tasks. they are designed to process sequential data, such as text, \nby breaking it down into smaller segments and analyzing the relationships between them. \ntransformers have proven to be highly effective at tasks such as language translation and \nquestion answering. in addition to bert, other notable transformer-based models include \ngpt-3 (generative pre-trained transformer 3) and t5 (text-to-text transfer transformer). \noverall, transformers have revolutionized the field of natural language processing, providing a \npowerful tool for developing highly accurate and efficient models for a wide range of \napplications. bert, in particular, has become a widely used model for a variety of nlp tasks \ndue to its high accuracy and ability to handle complex relationships within language data. \n \ninterpret \n \nthe architecture of the transformer is based on the concept of self-attention, which allows the \nmodel to weigh different parts of the input sequence differently when making predictions. the \nmodel consists of an encoder and a decoder, each of which contains multiple layers of self-\nattention and feedforward neural networks. \nthe encoder takes the input sequence and creates a sequence of hidden states, where each \nhidden state contains information about the input sequence up to a certain point. the decoder \ntakes the output sequence of the encoder and generates the final output sequence one token at \na time, while also attending to the input sequence using self-attention. \nthe self-attention mechanism is used in multiple layers of the transformer. each self-attention \nlayer computes a weighted sum of the input sequence, where the weights are determined by the \nsimilarity between each input token and every other token in the sequence. this allows the \nmodel to focus on the most relevant parts of the input sequence when making predictions. \nin addition to self-attention, the transformer also uses feedforward neural networks to process \nthe hidden states at each layer. the feedforward neural networks are applied to each hidden \nstate independently, allowing the model to learn non-linear transformations of the input \nsequence. \noverall, the transformer architecture has shown to be highly effective for a range of natural \nlanguage processing tasks and has been used as the basis for many state-of-the-art models, such \nas bert and gpt-3. \n \n \n \n \n \n \nthe flow of data through the transformer begins with the input sequence being passed through \nthe encoder. in each layer of the encoder, self-attention is used to compute a weighted sum of \nthe input sequence, which is then used to update the representation of each token in the \nsequence. the output of the encoder is a set of context vectors, each of which contains \ninformation about the entire input sequence. \n \n \n \n \nevaluate \n \nthe transformer architecture introduced a new approach to natural language processing by \nleveraging the power of self-attention. this approach allows the model to process an entire \ninput sequence at once, rather than processing it sequentially, which was the traditional \napproach. by using self-attention, the model can weigh different parts of the input sequence \ndifferently based on their importance, allowing for more accurate predictions. \nthe transformer model is highly parallelizable, which means that it can be trained much faster \nthan traditional sequential models. this is because each token in the input sequence can be \nprocessed independently of the others, which allows for more efficient use of computing \nresources. \nthe use of feedforward neural networks in the transformer architecture allows the model to \nlearn non-linear transformations of the input sequence, which is essential for processing natural \nlanguage. these non-linear transformations can capture complex patterns in the input data, \nallowing the model to make more accurate predictions. \nthe transformer has been used as the basis for many state-of-the-art models, such as bert \nand gpt-3. these models have achieved ground-breaking performance on a range of natural \nlanguage processing tasks, such as question answering, language translation, and text \ngeneration. \nin summary, the transformer architecture is a powerful tool for natural language processing, \nwhich has led to significant advancements in the field. its use of self-attention and feedforward \nneural networks allows for more accurate predictions and faster training times, making it a \npopular choice for many researchers and practitioners. \n \nplan \n \nmy recommendation would be to continue exploring the transformer architecture and its \napplications in natural language processing, particularly in the context of specific tasks or \napplications. it's also important to keep in mind that the field of nlp is rapidly evolving, so \nstaying up to date with the latest research and developments can be helpful for identifying new \nand innovative approaches to solving nlp problems.",
            "children": []
        },
        {
            "id": "1.13",
            "name": "week 7",
            "nodeType": "title",
            "text": "week 7",
            "page": null,
            "goal": "week 7",
            "children": []
        },
        {
            "id": "1.14",
            "name": "week 7 describe",
            "nodeType": "paragraph",
            "text": "week 7 \n \ndescribe \n \nfor week 7, i worked on adding new features to the exam marking project and to avoid the \ncase of contract cheating. i also used the bbc dataset to analyse the data with different machine \nlearning models to get better accuracies. we also discussed the transformer decoder block \nworkflow to implement that into our project. it is an essential component of the transformer \nmodel, which is used in natural language processing tasks such as machine learning translation, \ntext generation and sentiment analysis. i was also able to improve the accuracy of the model \nfrom the previous week and cleaned the data so that it does not overfit the model. we discussed \nthe concepts of transformer architecture and researched on different datasets to train and \nevaluate. \n \n \n \n \ninterpret \n \nwith the training and evaluating the model, i was able to make new insights into the dataset \nand contribute towards the project goals. i worked on the features list of the project and made \nseveral contributions towards the contract cheating cases and fine-tuned the model accordingly. \ni analysed the dataset using various machine learning algorithms and models to get a higher \naccuracy. the transformer decoder block was a crucial component which needed a high-level \nunderstanding of the concept and with all the discussions we were able to implement such \ncomplex concepts into our tasks. there was a significant improvement in the accuracy and the \nperformance of the model.  \n \nevaluate \n \ni actively worked on improving the features list, addressing contract cheating cases, and fine-\ntuning the model accordingly. by analysing the dataset using various machine learning \nalgorithms and models, i was able to achieve higher accuracy and by implementing the \ntransformer decoder block is indeed a complex task, requiring a high-level understanding of \nthe concept. i had a productive discussion and successfully incorporated this component into \nmy tasks. the significant improvement in accuracy and model performance was a positive \noutcome. overall, i was proactive, diligent, and successful in my tasks for this week, making \nvaluable contributions towards the project goals and achieving improved results. \n \nplan \n \nmy recommendations would be to further explore the model enhancements as were improving \nthe model accuracy and performances, it’s important to explore more ways of hyperparameter \ntuning and some advanced techniques to gain better results. we should also focus on expanding \nthe dataset and exploring more datasets which are relevant for the project.",
            "page": null,
            "goal": "week 7 \n \ndescribe \n \nfor week 7, i worked on adding new features to the exam marking project and to avoid the \ncase of contract cheating. i also used the bbc dataset to analyse the data with different machine \nlearning models to get better accuracies. we also discussed the transformer decoder block \nworkflow to implement that into our project. it is an essential component of the transformer \nmodel, which is used in natural language processing tasks such as machine learning translation, \ntext generation and sentiment analysis. i was also able to improve the accuracy of the model \nfrom the previous week and cleaned the data so that it does not overfit the model. we discussed \nthe concepts of transformer architecture and researched on different datasets to train and \nevaluate. \n \n \n \n \ninterpret \n \nwith the training and evaluating the model, i was able to make new insights into the dataset \nand contribute towards the project goals. i worked on the features list of the project and made \nseveral contributions towards the contract cheating cases and fine-tuned the model accordingly. \ni analysed the dataset using various machine learning algorithms and models to get a higher \naccuracy. the transformer decoder block was a crucial component which needed a high-level \nunderstanding of the concept and with all the discussions we were able to implement such \ncomplex concepts into our tasks. there was a significant improvement in the accuracy and the \nperformance of the model.  \n \nevaluate \n \ni actively worked on improving the features list, addressing contract cheating cases, and fine-\ntuning the model accordingly. by analysing the dataset using various machine learning \nalgorithms and models, i was able to achieve higher accuracy and by implementing the \ntransformer decoder block is indeed a complex task, requiring a high-level understanding of \nthe concept. i had a productive discussion and successfully incorporated this component into \nmy tasks. the significant improvement in accuracy and model performance was a positive \noutcome. overall, i was proactive, diligent, and successful in my tasks for this week, making \nvaluable contributions towards the project goals and achieving improved results. \n \nplan \n \nmy recommendations would be to further explore the model enhancements as were improving \nthe model accuracy and performances, it’s important to explore more ways of hyperparameter \ntuning and some advanced techniques to gain better results. we should also focus on expanding \nthe dataset and exploring more datasets which are relevant for the project.",
            "children": []
        },
        {
            "id": "1.15",
            "name": "week 8",
            "nodeType": "title",
            "text": "week 8",
            "page": null,
            "goal": "week 8",
            "children": []
        },
        {
            "id": "1.16",
            "name": "week 8 describe",
            "nodeType": "paragraph",
            "text": "week 8 \n \ndescribe \n \nfor week 8, we were giving the task to explore the new asap dataset and analyse the data \nwith the bert model. i cleaned and processed the dataset so that i could train the model with \ndifferent parameters. this dataset contained the marks of the students from an english exam \nand different domain experts.  \n \ninterpret \n \nthis asap dataset was different from the bbc dataset and in order to compare the results i \nhad to test the machine learning model for both the datasets. after data pre-processing, i used \nthe bert pre-trained model to test the asap dataset and then compared the accuracy with the \nbbc dataset. both the models showed good results, but the bbc dataset performed better than \nthe asap dataset. \n \nevaluate \n \nthe task for this week was just to work on the asap dataset and to analyse the data so that \nthere are no noises in the dataset and the model could perform better on different machine \nlearning algorithms. the performance of the models is based on a specific metrics and the \naccuracy is just one metric to consider, we can also assess other metrics for both the models to \ncheck which one performed better. since both the datasets were trained on the same bert \nmodel, we can compare these results within the broader context of the project. \n \nplan \n \nmy recommendation would be to ensure that we analyse both the datasets carefully so that \nthere is not noise in the dataset because that could change the result accuracy of the model. \nremoving the outliers from the dataset is important so that it does not have any inconsistencies. \nfor the performance metrics, we could also check the roc curve and f1 scores from these \ndatasets to evaluate the model.",
            "page": null,
            "goal": "week 8 \n \ndescribe \n \nfor week 8, we were giving the task to explore the new asap dataset and analyse the data \nwith the bert model. i cleaned and processed the dataset so that i could train the model with \ndifferent parameters. this dataset contained the marks of the students from an english exam \nand different domain experts.  \n \ninterpret \n \nthis asap dataset was different from the bbc dataset and in order to compare the results i \nhad to test the machine learning model for both the datasets. after data pre-processing, i used \nthe bert pre-trained model to test the asap dataset and then compared the accuracy with the \nbbc dataset. both the models showed good results, but the bbc dataset performed better than \nthe asap dataset. \n \nevaluate \n \nthe task for this week was just to work on the asap dataset and to analyse the data so that \nthere are no noises in the dataset and the model could perform better on different machine \nlearning algorithms. the performance of the models is based on a specific metrics and the \naccuracy is just one metric to consider, we can also assess other metrics for both the models to \ncheck which one performed better. since both the datasets were trained on the same bert \nmodel, we can compare these results within the broader context of the project. \n \nplan \n \nmy recommendation would be to ensure that we analyse both the datasets carefully so that \nthere is not noise in the dataset because that could change the result accuracy of the model. \nremoving the outliers from the dataset is important so that it does not have any inconsistencies. \nfor the performance metrics, we could also check the roc curve and f1 scores from these \ndatasets to evaluate the model.",
            "children": []
        },
        {
            "id": "1.17",
            "name": "week 9",
            "nodeType": "title",
            "text": "week 9",
            "page": null,
            "goal": "week 9",
            "children": []
        },
        {
            "id": "1.18",
            "name": "week 9 describe",
            "nodeType": "paragraph",
            "text": "week 9 \n \ndescribe \n \nfor week 9, i continued to work with bbc and asap datasets and tried to make the api for \nthe models which we have trained so that we could display all the techniques used in the data \nprocessing part and the model evaluation.  \n \ninterpret \n \ndue to the class imbalance in both the datasets it was difficult to further tune the model for \nbetter accuracy as asap dataset only got 56.3% test accuracy while the bbc dataset got 98% \ntest accuracy. but the bbc dataset was overfitting and because of different categories which \nwere distributed randomly it was hard to further clean the data. all the labels were changed \nand improved from before and the scoring rubrics was also updated for both the datasets. it \nwas a bit challenging to analyse the dataset with different characteristics. \n \nevaluate \n \nwell, these types of challenges which came with the datasets were common with such type of \ndata, so i need to further analyse and research about different techniques to improve this one. \ni’ll continue to redefine the data pre-processing methods addressing this issue of class \nimbalance. just need to add more advanced techniques for further testing of the model. \n \nplan \n \nmy recommendation would be to apply data augmentation techniques such as oversampling \nthe minority class, undersampling the majority class and generating some samples to balance \nthe class distribution. i will try to overcome the problem of overfitting in the dataset by cross \nvalidating the model and analysing the errors so that the model can give improvements in the \ndataset while training.",
            "page": null,
            "goal": "week 9 \n \ndescribe \n \nfor week 9, i continued to work with bbc and asap datasets and tried to make the api for \nthe models which we have trained so that we could display all the techniques used in the data \nprocessing part and the model evaluation.  \n \ninterpret \n \ndue to the class imbalance in both the datasets it was difficult to further tune the model for \nbetter accuracy as asap dataset only got 56.3% test accuracy while the bbc dataset got 98% \ntest accuracy. but the bbc dataset was overfitting and because of different categories which \nwere distributed randomly it was hard to further clean the data. all the labels were changed \nand improved from before and the scoring rubrics was also updated for both the datasets. it \nwas a bit challenging to analyse the dataset with different characteristics. \n \nevaluate \n \nwell, these types of challenges which came with the datasets were common with such type of \ndata, so i need to further analyse and research about different techniques to improve this one. \ni’ll continue to redefine the data pre-processing methods addressing this issue of class \nimbalance. just need to add more advanced techniques for further testing of the model. \n \nplan \n \nmy recommendation would be to apply data augmentation techniques such as oversampling \nthe minority class, undersampling the majority class and generating some samples to balance \nthe class distribution. i will try to overcome the problem of overfitting in the dataset by cross \nvalidating the model and analysing the errors so that the model can give improvements in the \ndataset while training.",
            "children": []
        },
        {
            "id": "1.19",
            "name": "week 10",
            "nodeType": "title",
            "text": "week 10",
            "page": null,
            "goal": "week 10",
            "children": []
        },
        {
            "id": "1.20",
            "name": "week 10 describe",
            "nodeType": "paragraph",
            "text": "week 10 \n \ndescribe \n \nfor week 10, i worked on a new corpus dataset which contained english test score results of \ncandidates from an english exam in uk. the dataset had two different sets of answers with \nthe candidate scores. \n \ninterpret \n \nthis dataset was a bit different from the others because it was not well structured, there were \nmore than 1200 different files in the dataset from different years and were in different folders. \nto make this data readable i had to merge few xml files into one and saved those variables to \nanalyse the data. i had many problems with this dataset as it was not scaled properly, and the \nxml file was not good for the analyses of the model, so i cleaned the dataset further to optimise \nthe model performance. \n \nevaluate \n \nthe corpus dataset performed very bad in the initial test set as it resulted only 4% training \naccuracy because the dataset was not pre-processed properly. i further tried to merge more files \nand then did the dataset evaluation for the training on the bert model. \n \nplan \n \nmy recommendation would be to get all the data into one csv file and then try to analyse the \ndataset rather than having multiple xml files for the dataset. more data pre-processing is \nrequired for this dataset and need to check if there is any irrelevant information which needs to \nbe removed from the dataset before testing.",
            "page": null,
            "goal": "week 10 \n \ndescribe \n \nfor week 10, i worked on a new corpus dataset which contained english test score results of \ncandidates from an english exam in uk. the dataset had two different sets of answers with \nthe candidate scores. \n \ninterpret \n \nthis dataset was a bit different from the others because it was not well structured, there were \nmore than 1200 different files in the dataset from different years and were in different folders. \nto make this data readable i had to merge few xml files into one and saved those variables to \nanalyse the data. i had many problems with this dataset as it was not scaled properly, and the \nxml file was not good for the analyses of the model, so i cleaned the dataset further to optimise \nthe model performance. \n \nevaluate \n \nthe corpus dataset performed very bad in the initial test set as it resulted only 4% training \naccuracy because the dataset was not pre-processed properly. i further tried to merge more files \nand then did the dataset evaluation for the training on the bert model. \n \nplan \n \nmy recommendation would be to get all the data into one csv file and then try to analyse the \ndataset rather than having multiple xml files for the dataset. more data pre-processing is \nrequired for this dataset and need to check if there is any irrelevant information which needs to \nbe removed from the dataset before testing.",
            "children": []
        },
        {
            "id": "1.21",
            "name": "week 11",
            "nodeType": "title",
            "text": "week 11",
            "page": null,
            "goal": "week 11",
            "children": []
        },
        {
            "id": "1.22",
            "name": "week 11 describe",
            "nodeType": "paragraph",
            "text": "week 11 \n \ndescribe \n \nfor week 11, i finalised my report on all the datasets which i used for the evaluation of the \nmachine learning model. also, i was able to convert the corpus dataset xml files into one csv \nto analyse the data. \n \ninterpret \n \nafter generating a single csv file for the dataset, i did some data visualisation and data cleaning \nto run the bert model on this dataset. this dataset did not perform well on the model accuracy \nand further analysis of the model is required. also made a final report for the bbc and asap \ndataset to submit during the friday meeting session. i also made few plots to visualise both the \ndatasets and did continue with the testing of all three models. \n \n \n \nevaluate \n \nthe data visualisation and cleaning helped in gaining the new insights about the dataset and i \naddressed some issues like inconsistencies in the data and the missing values that could affect \nthe performance of the model. i did all the model testing and saved the accuracy results to \ncompare between the models and completed the final report for the project by visualising the \ndata and identifying potential areas of improvement or further analysis. i made some progress \nwith the dataset training, cleaning the dataset and running the bert model.  \n \nplan \n \nmy recommendation would be to apply data augmentation techniques such as oversampling \nthe minority class, undersampling the majority class and generating some samples to balance \nthe class distribution. i will try to overcome the problem of overfitting in the dataset by cross \nvalidating the model and analysing the errors so that the model can give improvements in the \ndataset while training. further testing of the dataset is required in order to compare the results.",
            "page": null,
            "goal": "week 11 \n \ndescribe \n \nfor week 11, i finalised my report on all the datasets which i used for the evaluation of the \nmachine learning model. also, i was able to convert the corpus dataset xml files into one csv \nto analyse the data. \n \ninterpret \n \nafter generating a single csv file for the dataset, i did some data visualisation and data cleaning \nto run the bert model on this dataset. this dataset did not perform well on the model accuracy \nand further analysis of the model is required. also made a final report for the bbc and asap \ndataset to submit during the friday meeting session. i also made few plots to visualise both the \ndatasets and did continue with the testing of all three models. \n \n \n \nevaluate \n \nthe data visualisation and cleaning helped in gaining the new insights about the dataset and i \naddressed some issues like inconsistencies in the data and the missing values that could affect \nthe performance of the model. i did all the model testing and saved the accuracy results to \ncompare between the models and completed the final report for the project by visualising the \ndata and identifying potential areas of improvement or further analysis. i made some progress \nwith the dataset training, cleaning the dataset and running the bert model.  \n \nplan \n \nmy recommendation would be to apply data augmentation techniques such as oversampling \nthe minority class, undersampling the majority class and generating some samples to balance \nthe class distribution. i will try to overcome the problem of overfitting in the dataset by cross \nvalidating the model and analysing the errors so that the model can give improvements in the \ndataset while training. further testing of the dataset is required in order to compare the results.",
            "children": []
        },
        {
            "id": "1.23",
            "name": "week 12",
            "nodeType": "title",
            "text": "week 12",
            "page": null,
            "goal": "week 12",
            "children": []
        },
        {
            "id": "1.24",
            "name": "week 12 describe",
            "nodeType": "paragraph",
            "text": "week 12 \n \ndescribe \n \nfor the week 12, i did all the submission to my manager for the tasks which i performed during \nmy internship. since it was the last week, we didn’t have any tasks to perform. i finalised my \ninternship report and we had a nice office lunch to end the internship. \n \n \ninterpret \n \ni saved all the checkpoints and the files for the project on the itic learning portal for the future \ninternship team to work on and mailed the performance reports for the models to my manager.  \n \nevaluate \n \ni had a great experience working with the itic team on this internship project i have gained a \nlot of knowledge about the model testing and training using different parameters. i learned how \nto develop ui designs and develop apis for any model. \n \nplan \n \nmy recommendation would be to continue working on this automatic exam marking project \nand improve the quality of the dataset and the features of the ui design. as i move forward in \nmy career, this will be an important learning outcome for me, and i really enjoyed working at \nitic.",
            "page": null,
            "goal": "week 12 \n \ndescribe \n \nfor the week 12, i did all the submission to my manager for the tasks which i performed during \nmy internship. since it was the last week, we didn’t have any tasks to perform. i finalised my \ninternship report and we had a nice office lunch to end the internship. \n \n \ninterpret \n \ni saved all the checkpoints and the files for the project on the itic learning portal for the future \ninternship team to work on and mailed the performance reports for the models to my manager.  \n \nevaluate \n \ni had a great experience working with the itic team on this internship project i have gained a \nlot of knowledge about the model testing and training using different parameters. i learned how \nto develop ui designs and develop apis for any model. \n \nplan \n \nmy recommendation would be to continue working on this automatic exam marking project \nand improve the quality of the dataset and the features of the ui design. as i move forward in \nmy career, this will be an important learning outcome for me, and i really enjoyed working at \nitic.",
            "children": []
        }
    ]
}