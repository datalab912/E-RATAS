{
    "id": "1",
    "name": "Reflective journal Entities",
    "nodeType": "section",
    "text": "week 1 \ngoals, activities and outcomes \ngoals: my goals for week 1 are to complete all the onboarding tasks and understand how the team \nworks in an agile manner. familiar with the tools and technologies used in the domain data team. \nactivities: on the first day, i was invited to the domain office in sydney for an induction. it support \nteam ran the it induction and set up a laptop and user logins and access to company software. an \noffice administrator showed me around the office areas. i was introduced to my supervisor ken chen \nand other data members. i had discussed with ken what are our goals and what we have to do to \nreach the goal. i also participated in the team stand-up meeting to better understand what the data \narchitect team is doing. moreover, i have created 1:1 meetings with each member of the team in \norder to get to know people that i would work in the future. moreover, i was introduced to the \nsnowflake data warehouse. i completed the “getting started with snowflake - zero to snowflake” \nprovided by snowflake and “learning snowflakedb” provided by linkedin learning. \noutcomes: i had successfully completed the onboarding process and gotten myself familiarised \naround with different tools and technology platforms such as snowflake, jira, slack, and confluence, \nthat i would be actively using throughout my internship program. \nknowledge, skills and experiences \nknowledge: i learned some fundamentals of snowflake and had a better understanding of how to \nwork in an agile manner and get familiar with jira software.  \nskills: throughout the week, i improved my self-learning skills and communication skills with team \nmembers. \nexperience: the opportunity to get to know domain people and people on my team is a wonderful \nexperience i gained this week. they gave me a warm welcome. they were all friendly and helpful to \nthe guild in becoming familiar with the tasks that the team was currently working on. \nrewarding experiences \ni had a chance to learn the jira platform developed by atlassian. this platform is being used by the \ndata architecture team to organise tasks, and projects and collaborate within the team following the \nagile software development life cycle which i never had experienced before. this made me realise \nthat being a data team member in the real world, communication, and collaboration skills are \nessential skills apart from technical skills to achieve projects. \nchallenging experiences \nthere were many tools, platforms, and technologies that i never had experience using them before \nsuch as snowflake and jira. however, i could manage to explore and get familiar with these tools and \ntechnologies by myself. moreover, i always seek help from my supervisor and data team members \nwhen i have any doubts or issues. \n \n \n21 \n \nupcoming tasks \nsince my project is to create a tool for auto-generating metadata for snowflake tables and columns. \nin the next upcoming week, my goal is to generate metadata for one of the domain tables manually. \nin order to understand what kind of information we need and where we can find that information. \nadditionally, keep continues to get to know people in the data team. \n \n \nb. week 2 \ngoals, activities and outcomes \ngoals: in week 2, my objectives are to become acquainted with the tools and technologies utilised by \nthe domain data architect team and build a relationship with the team members. additionally, to \ngain a better understanding of how to work within an agile development framework. \nactivities: early this week, i was introduced to several tools and platforms that are being used by \ndomain. i was introduced to sql server integration services (ssis), which is one of the etl software \ntools that domain is using. i installed visual studio and sql server data tools (ssdt) so that i could \nexplore ssis packages and develop an understanding of the data warehouse pipelines and data \ntransformations in use. after that, ken chen, my supervisor, introduced me to snowflake column \nlineage, snowflake’s function to track data movement. in addition, i was assigned an ad-hoc task by \nrogerio roldan (director of bi and data engineering) to load data from csv files into the snowflake \ndata warehouse and then to use microsoft sql server management studio to copy it to a microsoft \nsql server database. i also started browsing and exploring documents that contain information \nabout database tables on domain’s confluence and alation sites. i investigated the sample columns \nthat were tagged with the descriptions by jibin joy (data architecture lead - property data) which \nare examples of expected metadata output. lastly, i continued with regular 1:1 meeting with each \ndata architecture team member. \noutcomes: \n \ninstalled two new software tools \n \ndeveloped a better understanding of domain’s data architecture through the ad-hoc task \n \nloaded and transferred data between aws buckets, snowflake, and microsoft sql server \nknowledge, skills and experiences \nknowledge: i got more familiarity with snowflake, microsoft sql server, and aws from hands-on \nuse, as well as more familiarity with confluence and alation.  \nskills: i continued to develop collaboration skills by working with my supervisor and other senior \nteam members and improved my communication skills with co-workers. \nexperience: i joined the “morning tea - data team day” which is arranged every wednesday of the \nfirst week of each month at domain’s office. this team activity allowed me to informally network \nwith other people in the data team. \nrewarding experiences \ni found the ad-hoc task most rewarding because it was hands-on and involved real data. before \nloading the data, it was necessary to clean all the errors in it. this reinforced for me the importance \nof data cleaning in data processing. \n22 \n \nchallenging experiences \nthere were some software accessibility issues when setting up the snowflake and microsoft sql \nserver tools for use. i needed my supervisor’s help to resolve these. \nupcoming tasks \nin the coming week, i will be focusing on the following objectives: first, i will study an existing, hand-\ncoded, yet undocumented transformation script utilized in the data pipeline to gain insights into its \noperation. second, by examining the logic employed for labelling columns, i will develop an \nunderstanding of the specific labels required by the business. finally, i will manually create labels for \nan unlabelled table by conducting exploratory data analysis to comprehend the contents of each \ncolumn, using the existing transformation script and confluence documentation as reference \nmaterials. \n \nc. week 3 \ngoals, activities and outcomes \ngoals: in week 3, my objectives are to conduct research on a range of tools and technologies with \nthe goal of generating metadata descriptions and becoming acquainted with the tools and \ntechnologies utilised by the domain data architect team and building a relationship with the team \nmembers.  \nactivities: this week, i dedicated most of my time to manually creating metadata for one of \ndomain's snowflake tables. to generate the metadata, i first utilised chatgpt to assist in generating a \ngeneral description of each column. i then delved into various sources such as confluence, github, \nalation, and snowflake to gather specific information on each column and created a first draft of the \nmetadata. additionally, i studied and explored openai, a large language model that has the potential \nto be used for auto-generating metadata for snowflake tables and columns. finally, i attended regular \nstand-up meetings with the data architecture team to discuss ongoing projects and updates. \noutcomes: i successfully created the metadata for snowflake columns manually. additionally, i \nsuccessfully explored the openai api quickstart tutorial in order to understand how to access the \nmodel and fundamentals to use the api on the question and answering task. \nknowledge, skills and experiences \nknowledge: i got a better understanding of the domain’s documentation, and data discovery by \nbrowsing and exploring through confluence, alation, and snowflake.   \nskills: i improve my exploring and researching skills while obtaining the information for generating \ncolumn descriptions and continue to develop collaboration skills by working with my supervisor and \nteam members. \nexperience:  i spent time studying openai and exploring how it can be applied to question and \nanswering tasks. through my research, i gained a better understanding of how to access the model \nand how it can be leveraged to enhance data processing and analysis. this learning experience has \nequipped me with valuable knowledge and skills that can be utilised to improve the data processing \npipeline. \n23 \n \nrewarding experiences \nthis week, i found exploring domain's documentation and studying openai question answering \nusing embeddings to be a particularly rewarding experience. i gained valuable insights that i can \nutilise to build an auto-generating metadata tool soon.  \nchallenging experiences \nduring my exploration of domain's tables and columns, i found that there is limited documentation \navailable. as a result, i had to rely on reviewing the source code in github to gain a better \nunderstanding. unfortunately, i discovered that the code lacked detailed comments or explanations, \nmaking it more challenging to comprehend. \nupcoming tasks \ni plan to review the results of my manual tagging labels with my supervisor to determine if the labels \nalign with their expectations. additionally, i intend to continue studying openai models and their \napplications to improve my skills and knowledge in this area. \n \nd. week 4 \ngoals, activities and outcomes \ngoals: my objectives for week 4 are to conduct research on how to apply openai's model for \nanswering questions from domain's internal documentation on confluence. additionally, i aim to \ndevelop better relationships with team members by attending regular stand-up meetings and \nparticipating in 1:1 meeting with relevant leads. \nactivities: after i successfully created the metadata for snowflake columns manually in the last week, \nthis week, i explored how to apply openai's gpt-3.5 model to answer questions from domain's \ninternal documentation on confluence. to do this, i examined the example provided in openai's \ngithub repository and replicated the code from a medium page \"running an openai's model on \nyour internal confluence documentation.\" therefore, i focused on improving my scraping technique \nto obtain more accurate and useful data.  \naside from these technical tasks, i also attended regular stand-up meetings and had a 1:1 meeting \nwith jibin joy, who is the data architecture lead. during our meeting, we discussed my project and \nwhat steps i should take next to further develop it. we also talked about how i can build relationships \nwith people in other data teams, which i believe will be crucial to my success in this role. \noutcomes: i successfully replicated the code from the medium page to apply openai's gpt-3.5 \nmodel to answer questions from domain's internal documentation on confluence. while the code \nworked as expected, i encountered a challenge when the data i had scraped from confluence was of \npoor quality. \nknowledge, skills and experiences \nknowledge: throughout this week, i deepened my understanding of confluence api and openai api. \nby closely studying and replicating the code from openai's github repository and the medium page \n24 \n \n\"running an openai's model on your internal confluence documentation,\" i gained valuable \nknowledge on how to use these apis effectively. \nskills: i gained programming and problem-solving skills by replicating and identifying and addressing \nissues with the data scraped from confluence. \nexperience:  i gained valuable experience working with large datasets by scraping data from \ndomain's internal confluence pages. in addition, i developed expertise in machine learning \ntechniques such as document embedding and vector similarity, which allowed me to measure \nsimilarity from embedding. \nrewarding experiences \nduring this report period, i found the experience of working with confluence api and openai api to \nbe particularly rewarding. through my own experimentation and testing, i gained a deeper \nunderstanding of how to extract data from confluence and apply openai's gpt-3.5 model to it. i also \nenjoyed the challenge of problem-solving when the data scraped from confluence was of poor \nquality and needed to be improved. \nchallenging experiences \nduring this report period, i faced challenges related to dealing with poor-quality data and unfamiliar \ntools and technology. i have to develop new skills and techniques to improve my data scraping \nprocess due to poor data quality, while also investing time and effort to learn how to use confluence \napi and openai api.  \nupcoming tasks \ni plan to focus on improving the quality of data scraped from confluence pages by deepening my \nunderstanding of confluence pages and api, and learning more about data scraping and cleaning \ntechniques to produce high-quality data that can be used for training openai's gpt-3.5 model. \nadditionally, i will be looking for opportunities to collaborate with other members of the data team \nto gain insights and feedback on my progress. \n \ne. week 5 \ngoals, activities and outcomes \ngoals: explore an alternative approach to auto-generate column descriptions. \nactivities: this week, i focused on building an auto-generating tool using openai's gpt3.5 model. i \ncreated prompts that included table names and column names, and then fed them into the openai \ncompletion model \"text-davinci-002\". the results were impressive, with the tool generating accurate \ngeneral descriptions. however, my supervisor ken suggested using the chat model \"gpt-3.5-turbo\" as \nit has similar capabilities but is more cost-effective. \nadditionally, i had a productive discussion with jibin, the data architecture lead, and anderson, the \ndata engineer, about the two approaches i experimented with using openai's model with internal \nconfluence documentation and using the generic openai. we discussed concerns regarding \n25 \n \npermission to use openai with internal documents and the budget to use openai. therefore, i \nprepared presentation slides to explain my project and what i had done to obtain approval from the \ninvolved team leaders. \noutcomes: i successfully utilised the generic openai model to generate column descriptions for 75 \ntables. while the majority of the results were accurate, i found that some model results were in \ndifferent formats. \nknowledge, skills and experiences \nknowledge: i gained a better understanding of how to use the openai gpt-3.5 model to generate \ntext and descriptions by experimenting with different prompts and techniques. \nskills: i developed my presentation skills by creating a slide deck to explain my project and seek \napproval from team leaders. \nexperience:  i had the opportunity to discuss my project with experienced professionals in data \narchitecture and engineering and gained valuable insights and feedback on my approach. \nrewarding experiences \ngenerating column descriptions for 75 tables using the openai gpt-3.5 model was a major \nachievement this week as it required a lot of effort and skills. \nchallenging experiences \ndealing with the inconsistency of the openai gpt-3.5 model: although i was able to generate column \ndescriptions for 75 tables using the model, there were times when the output was not in the \nexpected format, which made it challenging to work with.  \nanother challenge i encountered was the limitation of my openai free account credit. i require \nadditional credit to continue my work. to address this challenge, i will present my project to team \nleaders next week. \nupcoming tasks \nfor the upcoming week, my primary tasks will be focused on improving the performance of the \ngeneric openai model by addressing unexpected results. i will also work on improving data scraping \nand cleaning for the use of openai on internal confluence documents. another important task will \nbe presenting my project to team leaders to seek their approval for continued use of openai and \ncollection of data from internal references. \n \n \n \n \n \n26 \n \nf. week 6 \ngoals, activities and outcomes \ngoals: my goals this week are to prepare presentations for domain and the university and continue \ndeveloping the metadata tool. \nactivities: this week, i focused on preparing two presentation slides, one for the domain and \nanother for the university. during the presentation for the domain, i discussed the capabilities of \nopenai for data catalogue to an audience consisting of the data director, head of data governance, \ndata architecture lead, and other co-workers from the data architecture, data scientist, and data \ngovernance teams. i presented my experiment results so far, explained what metadescriptor is, and \nproposed to ask for operation and experiment budgets to use openai’s models. additionally, i \nrequested permission to supply domain internal documents to openai's models. the presentation \nwas well received, and rogerio, data director agreed to approve the budget. moreover, i need to \nprovide a list of internal documentation links that require permission to collect data. \nanother presentation was a mid-semester presentation. i gave a presentation to professor amin, \nmahdieh, and other students, which covered my company's background, my role and \nresponsibilities, my progress, and the skills i acquired. the presentation went well, but i plan to \nimprove my slides for the final presentation. \nin addition, during the week, i focused on gathering internal documentation links and sharing them \nwith team leaders to obtain permission from the chief data officer, pooyan. furthermore, i spent \ntime refining the code to address the previous week's problem of utilising a generic openai model to \ngenerate column descriptions for 75 tables. some of the outcomes were in diverse formats, and i \nworked to resolve this issue. \noutcomes: i successfully utilised the generic openai model to generate column descriptions for 75 \ntables. while the majority of the results were accurate, i found that some model results were in \ndifferent formats. \nknowledge, skills and experiences \nknowledge: over of the week, i gained valuable knowledge related to data privacy and governance, \nparticularly the importance of obtaining permission and approval when collecting and using internal \ncompany documentation to supply to third-party companies such as openai. this understanding \nhelped me become more aware of the terms of use of openai and the potential risks of non-\ncompliance. overall, i have developed a deeper understanding of the importance of data governance \nand the need to comply with regulations and policies. \nskills: i developed my presentation and communication abilities through the preparation and \ndelivery of two presentations. these experiences allowed me to hone my skills in conveying complex \ntechnical concepts to both technical and non-technical audiences. additionally, i further refined my \ncoding abilities through troubleshooting and improving the auto-generating metadata tool. \nexperience:  i had the opportunity to engage with various stakeholders, including team leaders, the \ndata director, and the chief data officer. these interactions provided me with insight into how data \nmanagement and governance are approached in a large organisation, as well as the opportunity to \nbuild relationships and networks within the company.  \n \n27 \n \nrewarding experiences \none particularly rewarding experience during this report period was giving my presentation on \nopenai capabilities for data cataloguing to the domain team. it was great to see that the audience \nwas engaged and interested in the topic and to hear that the data director was willing to approve \nthe budget for our project. this experience showed me that the work i am doing is valuable and has \nthe potential to make a real impact on the company. overall, i feel that this week was a valuable \nlearning experience that allowed me to grow both personally and professionally. \nchallenging experiences \none challenge i faced during this report period was the limitation of my openai free account credit. \nsince i used a lot of credits to experiment with different models, i ran out of credits and could not \nuse the account until my budget was approved. however, i was able to overcome this obstacle thanks \nto the support of my supervisor, ken. he kindly offered to let me use his account while waiting for my \nbudget to be approved. this allowed me to continue working on my project without any significant \ninterruption. i'm grateful for his help and support. \nupcoming tasks \ni will continue to work on refining the metadescriptor tool and improving its efficiency in generating \nmetadata for snowflake tables and columns. i will focus on improving the accuracy of data by \nenhancing the data collection and cleaning process. \n \ng. week 7 \ngoals, activities and outcomes \ngoals: this week, i focused on two main tasks: finalising my mid-semester report for the university \nand continuing to develop the metadata tool. \nactivities: for the mid-semester report, i reviewed and edited my draft, incorporating feedback from \nmy supervisor. i also made sure to follow the guidelines and formatting requirements provided by \nthe university. the report is now complete and ready to submit. \nregarding the metadata tool, firstly, i focused on improving the efficiency and accuracy of the code \nby using the \"text-davinci-002\" completion model. i analysed the results from the previous week's \nexperiment and made necessary adjustments to the code to optimise its performance. \nin addition to this, i conducted experiments with the \"gpt-3.5-turbo\" completion model as an \nalternative option. i carefully crafted the input prompts for the model and fine-tuned its parameters \nto generate accurate metadata. i tested the model with 75 table names and their corresponding \ncolumn names and found that while the results were clear and accurate, it took twice as long to \nprocess compared to text-davinci-002. however, it was cost-effective, being 10 times cheaper than \nthe previous model. \noutcomes: the week was successful as i was able to finalise my mid-semester report for the \nuniversity, and made significant progress on the development of the metadata tool. i refined the \n28 \n \ncode using the \"text-davinci-002\" completion model and experimented with the \"gpt-3.5-turbo\" \nmodel, which generated accurate metadata at a lower cost. \nknowledge, skills and experiences \nknowledge: during the week, i gained new knowledge on how to write prompts using the gpt-3.5-\nturbo model. i found that it was quite different from the text-davinci-002 model and took some time \nto understand. through searching for example code and applications on the internet, i was able to \ndevelop a better understanding of how to effectively use this model for generating metadata. this \nhas broadened my knowledge and skill set in working with different models and approaches for \nnatural language processing. \nskills: firstly, i improved my report writing skills by incorporating feedback and adhering to university \nguidelines. secondly, i developed my programming skills by refining the metadata tool's code and \nexperimenting with different models.  \nexperience:  this week, i had the chance to practice my time management skills by balancing the \ndemands of my internship and my other responsibilities. through prioritising tasks and managing my \nschedule effectively, i was able to complete my work on time and maintain a healthy work-life \nbalance. this experience has improved my ability to manage competing demands and stay organised. \nrewarding experiences \ni would say that i found the opportunity to work on the metadata tool project to be the most \nrewarding for this report period. specifically, i enjoyed the process of experimenting with different \ncompletion models and input prompts to fine-tune the tool's performance and accuracy. \nchallenging experiences \nduring this report period, i encountered a challenge while experimenting with the gpt-3.5-turbo \nmodel. i faced errors such as ratelimiterror and api error. this was an experience that left me feeling \ntired and frustrated. however, i sought guidance from ken, my supervisor, who suggested that i use \nthe try-catch technique to resume the model from the last stage and continue running it. thanks to \nhis advice, i was able to overcome the issue and successfully complete the model for the entire \ninput. \nupcoming tasks \nmy plan going forward is to further refine the metadescriptor tool, with a specific focus on \nenhancing its efficiency in generating metadata for snowflake tables and columns. this will include \nefforts to improve the accuracy of the generated data by enhancing the data collection and cleaning \nprocesses. \n \n \n \n \n29 \n \nh. week 8 \ngoals, activities and outcomes \ngoals: this week, i focused on studying alternative models to build metadata from internal reference \ndocuments. \nactivities: during the past week, my focus was to explore various models for metadata construction. \nmy objective was to gain a deep understanding of the different methodologies for metadata \nmodelling and their potential applications for this project. i conducted research on three different \ntech stacks, which included llama-index, langchain framework, and chroma vector database. \ninitially, i attempted to replicate the process using confluence loader from llama-index, but i \nencountered some issues. to address this challenge, i decided to export the confluence pages to pdf \nfiles and generate an embedding file using langchain. after that, i utilized chroma to store and index \nthe documents. the outcome was successful, and i was able to develop a question and answering \nsystem using multiple pdf files. \noutcomes: the outcome is the successful development of a question and answering system using \nmultiple pdf files through the use of chroma vector database for document indexing and storage, \nand langchain framework for generating an embedding file from the pdf files. \nknowledge, skills and experiences \nknowledge: i gained a deeper understanding of alternative models for metadata construction, \nincluding llama-index, langchain framework, and chroma vector database. through my research, i \nlearned about the advantages and limitations of each approach and gained insights into how they \ncan be applied to build efficient and scalable metadata systems. \nskills: in terms of skills, i developed proficiency in using langchain framework for generating \nembeddings from pdf files and in utilizing chroma vector database for storing and indexing \ndocuments. these skills will be valuable in future projects involving large-scale document processing \nand retrieval. \nexperience:  this week provided me with valuable experience in working through challenges and \nfinding creative solutions. when i encountered issues with the confluence loader from llama-\nindex, i was able to think outside the box and come up with an alternative approach using langchain \nand chroma. this experience reinforced the importance of adaptability and resourcefulness in \nproblem-solving. \nrewarding experiences \nlearning and using alternative models for metadata construction: exploring and learning about new \nmethodologies and technologies can be very exciting and rewarding. being able to apply that \nknowledge to your project and see it succeed can be a great source of satisfaction. \nchallenging experiences \none of the most difficult tasks was troubleshooting issues i encountered while trying to use \nconfluence loader from llama-index. despite spending significant time trying to resolve the issue, i \nultimately had to find a workaround by exporting the confluence pages to pdf files and generating \nan embedding file using langchain. this required additional effort and time and was a frustrating \nexperience. overall, these experiences provided valuable learning opportunities and allowed me to \nfurther develop my problem-solving skills. \n30 \n \nupcoming tasks \nduring the upcoming week, i intend to experiment with various embedding models and explore the \npossibility of constructing a question-and-answer system from alternative resources like json and \nexcel files. \n \ni. week 9 \ngoals, activities and outcomes \ngoals: this week, my goal is to continue exploring and experimenting with alternative models and \ntools for building metadata from internal reference documents. \nactivities: during week 9, we were waiting for the data governance team's response on the suitable \nsolution for developing metadata from internal documents. the aim is to protect sensitive data from \nunauthorized access when sharing it with third-party service providers like openai. to avoid exposing \ncritical information about the domain, we explored two possible solutions to minimize the risk of \ndata exposure, including microsoft azure openai and an in-house aws solution using an open-\nsource large language model (llm) like gpt, hosting it on sagemaker, and setting up a vector \ndatabase on either on-premises or aws server. \nwhile awaiting the data governance team's response, i also explored the usage of the llama-index \nlibrary with the huggingfaceembeddings model to analyse internal documents extracted from \nconfluence and api swagger pages. \noutcomes: i have deepened my understanding of the risks of sharing sensitive data with third-party \nservice providers and explored two potential solutions to reduce the risk of data exposure. \nadditionally, i gained valuable insights from experimenting with the llama-index library and custom \nhuggingfaceembeddings model, which i can use to compare with baseline results in the future. \nknowledge, skills and experiences \nknowledge: i gained new knowledge about data governance and security, specifically in the context \nof sharing sensitive information with third-party service providers. i learned about the potential risks \nof sharing data without a contractual agreement and the importance of minimizing the risk of data \nexposure. i also gained knowledge about the different options available to address these risks. \nskills: this week, i developed skills in experimenting with different embedding models and exploring \ntheir applications in metadata construction. specifically, i worked with the llama-index library and \nhuggingfaceembeddings model to analyse internal documents extracted from confluence and api \nswagger pages.  \nexperience:  i gained practical experience in dealing with data governance challenges and the \nimportance of effective communication between data science and governance teams. \nrewarding experiences \none of the most rewarding experiences during week 9 was gaining hands-on experience in \nconducting research and experimentation. through exploring different solutions to the problem of \nmetadata construction and data governance, i was able to gain practical knowledge that i can apply \nin future projects. additionally, i enjoyed the challenge of experimenting with different embedding \n31 \n \nmodels and exploring their potential applications. this experience helped me build confidence in my \nabilities as a data scientist and gave me a sense of satisfaction in exploring new ideas and \ntechnologies. \nchallenging experiences \nwhile our objective for this week it to discover the approach that can avoid using openai's services, i \nfound that the openai text-davinci models was still invoked during the querying process for text \ncompletion, resulting in a cost of approximately $2-3 for generating 100 column descriptions. \nadditionally, it was observed that the openai key was directly accessed from the local environment \nvariables, which could pose a potential security risk. \nupcoming tasks \nmy focus will be on cleaning the data extracted from confluence pages. this is an essential step in \nensuring the accuracy and quality of the metadata generated from these internal reference \ndocuments. additionally, i will be exploring various techniques and tools that can be used to clean \nand pre-process the data efficiently. \n \n \nj. week 10 \ngoals, activities and outcomes \ngoals: the primary goal for this week was to focus on cleaning the data extracted from confluence \npages. the aim was to ensure the accuracy and quality of the metadata generated from these \ninternal reference documents. additionally, the objective was to explore various techniques and \ntools that can be used to efficiently clean and pre-process the data. \nactivities: to achieve the set goals, i developed a program specifically designed for web content \nextraction and processing, with a focus on confluence pages. the program utilized the confluence \napi to fetch the content of each page and employed beautifulsoup4 for html parsing. it involved \nsteps such as extracting text content from headings and sections, removing unwanted elements, \nhandling special content like tables and lists, and utilizing spacy for natural language processing \ntasks. the activities also included implementing and testing the program, ensuring the correct \ninstallation of required python packages, and configuring the confluence api. \noutcomes: i successfully developed a robust program for extracting and processing content from \nconfluence pages, resulting in clean and structured data ready for analysis. \nknowledge, skills and experiences \nknowledge: i gained knowledge in web content extraction and processing. i learned how to utilize \nthe confluence api to fetch content and parse html using beautifulsoup4. \nskills: i improved my data pre-processing skills, including removing unwanted elements and \nconverting complex content into a more manageable format. \nexperience:  engaging in web content extraction and processing provided me with hands-on \nexperience in working with real-world data. i gained practical experience in utilizing web apis, \nmanipulating html structures, and addressing common challenges in data cleaning and pre-\n32 \n \nprocessing. this experience enhanced my problem-solving and debugging skills in a real-world \ncontext. \nrewarding experiences \nthe experience that was particularly rewarding during this report period was successfully developing \na program for web content extraction and processing from confluence pages. being able to fetch and \nparse the desired information, resulting in clean and structured data ready for analysis, provided a \nsense of accomplishment. gaining proficiency in utilizing the confluence api and html parsing, as \nwell as troubleshooting and debugging in a real-world context, further enhanced my skills and \nproblem-solving abilities. overall, the successful implementation of the program and the hands-on \nlearning gained from working with real data were highly rewarding experiences. \nchallenging experiences \nextracting content from confluence pages proved to be a challenging task during this report period. \nit required gaining knowledge of html format and becoming proficient in using beautifulsoup4. \nhowever, through persistent experimentation and learning, i successfully managed to extract the \ndata and structure it in a more organized format. this experience provided valuable insights and \nimproved my understanding of working with html and data extraction techniques. \nupcoming tasks \nmy primary tasks and duties will involve improving the cleaning process for the data extracted from \nconfluence pages. additionally, i will be actively participating in code reviews, testing, and the \ndeployment of the data pipeline. \n \nk. week 11 \ngoals, activities and outcomes \ngoals: during week 11, my focus was on enhancing the data cleaning process for the content \nextracted from confluence pages.  \nactivities: throughout the week, i dedicated time and effort to refine the cleaning techniques and \nensure the accuracy and quality of the extracted data. additionally, i conducted experiments with \nother large language models (llms) such as fastchat-t5 and wizardlm-7b, attempting to incorporate \nthese llms into the metadata generation workflow. \noutcomes: firstly, the data cleaning process was significantly improved, leading to enhanced \naccuracy and quality of the extracted content from confluence pages. secondly, the experiments \nwith fastchat-t5 and wizardlm-7b, although hindered by limited gpu resources, provided valuable \ninsights into the requirements and limitations of advanced language models. \nknowledge, skills and experiences \nknowledge: i acquired knowledge about different large language models (llms) such as fastchat-t5 \nand wizardlm-7b, and their potential applications in metadata generation. \n33 \n \nskills: i expanded my skills in the field of data cleaning and pre-processing. i gained a deeper \nunderstanding of the specific techniques required to clean data extracted from confluence pages, \nensuring its accuracy and reliability. \nexperience: the experience of working with different llms and attempting their integration \ndeepened my understanding of their capabilities and limitations. while the experiments were not \nsuccessful, these experiences allowed me to gain insights into the practical considerations and \nrequirements for utilizing advanced language models. \nrewarding experiences \none rewarding experience was the significant progress made in improving the data cleaning process. \nthe successful implementation of refinements and enhancements resulted in higher accuracy and \nquality of the extracted content from confluence pages. the opportunity to explore different \napproaches and techniques in data cleaning provided valuable learning experiences. \n \nchallenging experiences \nduring this report period, one particularly challenging experience was the unsuccessful attempts to \napply other large language models (llms) such as fastchat-t5 and wizardlm-7b. the lack of \navailable gpu resources posed a significant obstacle in exploring and experimenting with these \nadvanced models. this technical constraint hindered the evaluation and utilization of these models \nfor metadata generation. however, this experience provided valuable insights into the hardware \nrequirements and limitations of working with advanced language models, emphasizing the need for \nadequate resources for successful implementation. \nupcoming tasks \nmy principal tasks and duties will involve finalizing my project. this will include cleaning up my \ngithub repository by organizing and optimizing the code, documentation, and project files. \nadditionally, i will prepare for the final presentation with the university. \n \nl. week 12 \ngoals, activities and outcomes \ngoals: my primary goal was to finalize my project by focusing on two key areas: cleaning up my \ngithub repository and preparing for the final presentation with the university.  \nactivities: to achieve these goals, i dedicated my time and effort to cleaning up my github \nrepository. this involved organizing and optimizing the code, documentation, and project files. i \nreviewed the repository's structure, updated readme files, and ensured proper documentation for \nfuture reference. additionally, i spent time preparing for the final presentation with the university, \nrefining my presentation materials, and ensuring clear communication of the project's objectives, \nmethodologies, and outcomes. \noutcomes: as a result of these activities, i successfully finalized my project. the github repository is \nnow clean, well-organized, and optimized, making it easier for others to understand and contribute \nto the project. the final presentation materials are polished and effectively convey the key aspects \n34 \n \nand outcomes of the project. these outcomes contribute to the overall success of the project and lay \na strong foundation for future collaboration and development. \nknowledge, skills and experiences \nknowledge: i acquired valuable knowledge in team collaboration on github repositories. i learned \nthe intricacies of working together with a team, including how to effectively contribute to a shared \nproject by submitting pull requests. i gained a deeper understanding of version control principles, \nsuch as branching and merging, and how they contribute to efficient collaboration on github. \nskills: through the process of finalizing my project, i developed essential skills in organizing and \noptimizing code, documentation, and project files. i learned how to streamline the repository \nstructure, improve file naming conventions, and ensure clear and concise documentation. \nexperience: collaborating on a github repository provided valuable insights and experiences. \ncontributing to the project's development, navigating merge conflicts, and working with different \nbranches enhanced my understanding of github and fostered effective teamwork. these experiences \nwill be invaluable for future collaborations.  \nrewarding experiences \na notable experience during this week was the preparation of the final presentation for the project. i \ninvested time and effort in creating well-crafted presentation slides that effectively conveyed the \nproject objectives and outcomes. this experience enhanced my skills in preparing and delivering \ntechnical presentations, ensuring that the content was concise, visually appealing, and easy to \nunderstand for a technical audience. it highlighted the importance of clear communication and \npolished presentation materials in effectively conveying project insights and achievements. \nchallenging experiences \none particularly challenging experience during this report period was preparing the final \npresentation slides within a limited time frame of just 5 minutes. this task required condensing a \nsignificant amount of information and summarizing the key highlights from the project. it was a \nchallenge to effectively communicate the most important points and convey the project's objectives, \nmethodologies, and outcomes in a concise manner. however, through careful analysis and \nprioritization, i was able to overcome this difficulty and create slides that effectively captured the \nessence of the project. this experience provided valuable insights into the importance of concise \ncommunication and the ability to convey complex information into clear and impactful messages. \nupcoming tasks \ni will prepare for the presentation and showcase of my work in domain, as well as finalize the final \nreport for the university. \n \n \n \n \n \n",
    "page": null,
    "goal": "Reflective journal Entities",
    "children": [
        {
            "id": "1.1",
            "name": "week 1",
            "nodeType": "title",
            "text": "week 1",
            "page": null,
            "goal": "week 1",
            "children": []
        },
        {
            "id": "1.2",
            "name": "week 1 goals,",
            "nodeType": "paragraph",
            "text": "week 1 \ngoals, activities and outcomes \ngoals: my goals for week 1 are to complete all the onboarding tasks and understand how the team \nworks in an agile manner. familiar with the tools and technologies used in the domain data team. \nactivities: on the first day, i was invited to the domain office in sydney for an induction. it support \nteam ran the it induction and set up a laptop and user logins and access to company software. an \noffice administrator showed me around the office areas. i was introduced to my supervisor ken chen \nand other data members. i had discussed with ken what are our goals and what we have to do to \nreach the goal. i also participated in the team stand-up meeting to better understand what the data \narchitect team is doing. moreover, i have created 1:1 meetings with each member of the team in \norder to get to know people that i would work in the future. moreover, i was introduced to the \nsnowflake data warehouse. i completed the “getting started with snowflake - zero to snowflake” \nprovided by snowflake and “learning snowflakedb” provided by linkedin learning. \noutcomes: i had successfully completed the onboarding process and gotten myself familiarised \naround with different tools and technology platforms such as snowflake, jira, slack, and confluence, \nthat i would be actively using throughout my internship program. \nknowledge, skills and experiences \nknowledge: i learned some fundamentals of snowflake and had a better understanding of how to \nwork in an agile manner and get familiar with jira software.  \nskills: throughout the week, i improved my self-learning skills and communication skills with team \nmembers. \nexperience: the opportunity to get to know domain people and people on my team is a wonderful \nexperience i gained this week. they gave me a warm welcome. they were all friendly and helpful to \nthe guild in becoming familiar with the tasks that the team was currently working on. \nrewarding experiences \ni had a chance to learn the jira platform developed by atlassian. this platform is being used by the \ndata architecture team to organise tasks, and projects and collaborate within the team following the \nagile software development life cycle which i never had experienced before. this made me realise \nthat being a data team member in the real world, communication, and collaboration skills are \nessential skills apart from technical skills to achieve projects. \nchallenging experiences \nthere were many tools, platforms, and technologies that i never had experience using them before \nsuch as snowflake and jira. however, i could manage to explore and get familiar with these tools and \ntechnologies by myself. moreover, i always seek help from my supervisor and data team members \nwhen i have any doubts or issues. \n \n \n21 \n \nupcoming tasks \nsince my project is to create a tool for auto-generating metadata for snowflake tables and columns. \nin the next upcoming week, my goal is to generate metadata for one of the domain tables manually. \nin order to understand what kind of information we need and where we can find that information. \nadditionally, keep continues to get to know people in the data team. \n \n \nb.",
            "page": null,
            "goal": "week 1 \ngoals, activities and outcomes \ngoals: my goals for week 1 are to complete all the onboarding tasks and understand how the team \nworks in an agile manner. familiar with the tools and technologies used in the domain data team. \nactivities: on the first day, i was invited to the domain office in sydney for an induction. it support \nteam ran the it induction and set up a laptop and user logins and access to company software. an \noffice administrator showed me around the office areas. i was introduced to my supervisor ken chen \nand other data members. i had discussed with ken what are our goals and what we have to do to \nreach the goal. i also participated in the team stand-up meeting to better understand what the data \narchitect team is doing. moreover, i have created 1:1 meetings with each member of the team in \norder to get to know people that i would work in the future. moreover, i was introduced to the \nsnowflake data warehouse. i completed the “getting started with snowflake - zero to snowflake” \nprovided by snowflake and “learning snowflakedb” provided by linkedin learning. \noutcomes: i had successfully completed the onboarding process and gotten myself familiarised \naround with different tools and technology platforms such as snowflake, jira, slack, and confluence, \nthat i would be actively using throughout my internship program. \nknowledge, skills and experiences \nknowledge: i learned some fundamentals of snowflake and had a better understanding of how to \nwork in an agile manner and get familiar with jira software.  \nskills: throughout the week, i improved my self-learning skills and communication skills with team \nmembers. \nexperience: the opportunity to get to know domain people and people on my team is a wonderful \nexperience i gained this week. they gave me a warm welcome. they were all friendly and helpful to \nthe guild in becoming familiar with the tasks that the team was currently working on. \nrewarding experiences \ni had a chance to learn the jira platform developed by atlassian. this platform is being used by the \ndata architecture team to organise tasks, and projects and collaborate within the team following the \nagile software development life cycle which i never had experienced before. this made me realise \nthat being a data team member in the real world, communication, and collaboration skills are \nessential skills apart from technical skills to achieve projects. \nchallenging experiences \nthere were many tools, platforms, and technologies that i never had experience using them before \nsuch as snowflake and jira. however, i could manage to explore and get familiar with these tools and \ntechnologies by myself. moreover, i always seek help from my supervisor and data team members \nwhen i have any doubts or issues. \n \n \n21 \n \nupcoming tasks \nsince my project is to create a tool for auto-generating metadata for snowflake tables and columns. \nin the next upcoming week, my goal is to generate metadata for one of the domain tables manually. \nin order to understand what kind of information we need and where we can find that information. \nadditionally, keep continues to get to know people in the data team. \n \n \nb.",
            "children": []
        },
        {
            "id": "1.3",
            "name": "week 2",
            "nodeType": "title",
            "text": "week 2",
            "page": null,
            "goal": "week 2",
            "children": []
        },
        {
            "id": "1.4",
            "name": "week 2 goals,",
            "nodeType": "paragraph",
            "text": "week 2 \ngoals, activities and outcomes \ngoals: in week 2, my objectives are to become acquainted with the tools and technologies utilised by \nthe domain data architect team and build a relationship with the team members. additionally, to \ngain a better understanding of how to work within an agile development framework. \nactivities: early this week, i was introduced to several tools and platforms that are being used by \ndomain. i was introduced to sql server integration services (ssis), which is one of the etl software \ntools that domain is using. i installed visual studio and sql server data tools (ssdt) so that i could \nexplore ssis packages and develop an understanding of the data warehouse pipelines and data \ntransformations in use. after that, ken chen, my supervisor, introduced me to snowflake column \nlineage, snowflake’s function to track data movement. in addition, i was assigned an ad-hoc task by \nrogerio roldan (director of bi and data engineering) to load data from csv files into the snowflake \ndata warehouse and then to use microsoft sql server management studio to copy it to a microsoft \nsql server database. i also started browsing and exploring documents that contain information \nabout database tables on domain’s confluence and alation sites. i investigated the sample columns \nthat were tagged with the descriptions by jibin joy (data architecture lead - property data) which \nare examples of expected metadata output. lastly, i continued with regular 1:1 meeting with each \ndata architecture team member. \noutcomes: \n \ninstalled two new software tools \n \ndeveloped a better understanding of domain’s data architecture through the ad-hoc task \n \nloaded and transferred data between aws buckets, snowflake, and microsoft sql server \nknowledge, skills and experiences \nknowledge: i got more familiarity with snowflake, microsoft sql server, and aws from hands-on \nuse, as well as more familiarity with confluence and alation.  \nskills: i continued to develop collaboration skills by working with my supervisor and other senior \nteam members and improved my communication skills with co-workers. \nexperience: i joined the “morning tea - data team day” which is arranged every wednesday of the \nfirst week of each month at domain’s office. this team activity allowed me to informally network \nwith other people in the data team. \nrewarding experiences \ni found the ad-hoc task most rewarding because it was hands-on and involved real data. before \nloading the data, it was necessary to clean all the errors in it. this reinforced for me the importance \nof data cleaning in data processing. \n22 \n \nchallenging experiences \nthere were some software accessibility issues when setting up the snowflake and microsoft sql \nserver tools for use. i needed my supervisor’s help to resolve these. \nupcoming tasks \nin the coming week, i will be focusing on the following objectives: first, i will study an existing, hand-\ncoded, yet undocumented transformation script utilized in the data pipeline to gain insights into its \noperation. second, by examining the logic employed for labelling columns, i will develop an \nunderstanding of the specific labels required by the business. finally, i will manually create labels for \nan unlabelled table by conducting exploratory data analysis to comprehend the contents of each \ncolumn, using the existing transformation script and confluence documentation as reference \nmaterials. \n \nc.",
            "page": null,
            "goal": "week 2 \ngoals, activities and outcomes \ngoals: in week 2, my objectives are to become acquainted with the tools and technologies utilised by \nthe domain data architect team and build a relationship with the team members. additionally, to \ngain a better understanding of how to work within an agile development framework. \nactivities: early this week, i was introduced to several tools and platforms that are being used by \ndomain. i was introduced to sql server integration services (ssis), which is one of the etl software \ntools that domain is using. i installed visual studio and sql server data tools (ssdt) so that i could \nexplore ssis packages and develop an understanding of the data warehouse pipelines and data \ntransformations in use. after that, ken chen, my supervisor, introduced me to snowflake column \nlineage, snowflake’s function to track data movement. in addition, i was assigned an ad-hoc task by \nrogerio roldan (director of bi and data engineering) to load data from csv files into the snowflake \ndata warehouse and then to use microsoft sql server management studio to copy it to a microsoft \nsql server database. i also started browsing and exploring documents that contain information \nabout database tables on domain’s confluence and alation sites. i investigated the sample columns \nthat were tagged with the descriptions by jibin joy (data architecture lead - property data) which \nare examples of expected metadata output. lastly, i continued with regular 1:1 meeting with each \ndata architecture team member. \noutcomes: \n \ninstalled two new software tools \n \ndeveloped a better understanding of domain’s data architecture through the ad-hoc task \n \nloaded and transferred data between aws buckets, snowflake, and microsoft sql server \nknowledge, skills and experiences \nknowledge: i got more familiarity with snowflake, microsoft sql server, and aws from hands-on \nuse, as well as more familiarity with confluence and alation.  \nskills: i continued to develop collaboration skills by working with my supervisor and other senior \nteam members and improved my communication skills with co-workers. \nexperience: i joined the “morning tea - data team day” which is arranged every wednesday of the \nfirst week of each month at domain’s office. this team activity allowed me to informally network \nwith other people in the data team. \nrewarding experiences \ni found the ad-hoc task most rewarding because it was hands-on and involved real data. before \nloading the data, it was necessary to clean all the errors in it. this reinforced for me the importance \nof data cleaning in data processing. \n22 \n \nchallenging experiences \nthere were some software accessibility issues when setting up the snowflake and microsoft sql \nserver tools for use. i needed my supervisor’s help to resolve these. \nupcoming tasks \nin the coming week, i will be focusing on the following objectives: first, i will study an existing, hand-\ncoded, yet undocumented transformation script utilized in the data pipeline to gain insights into its \noperation. second, by examining the logic employed for labelling columns, i will develop an \nunderstanding of the specific labels required by the business. finally, i will manually create labels for \nan unlabelled table by conducting exploratory data analysis to comprehend the contents of each \ncolumn, using the existing transformation script and confluence documentation as reference \nmaterials. \n \nc.",
            "children": []
        },
        {
            "id": "1.5",
            "name": "week 3",
            "nodeType": "title",
            "text": "week 3",
            "page": null,
            "goal": "week 3",
            "children": []
        },
        {
            "id": "1.6",
            "name": "week 3 goals,",
            "nodeType": "paragraph",
            "text": "week 3 \ngoals, activities and outcomes \ngoals: in week 3, my objectives are to conduct research on a range of tools and technologies with \nthe goal of generating metadata descriptions and becoming acquainted with the tools and \ntechnologies utilised by the domain data architect team and building a relationship with the team \nmembers.  \nactivities: this week, i dedicated most of my time to manually creating metadata for one of \ndomain's snowflake tables. to generate the metadata, i first utilised chatgpt to assist in generating a \ngeneral description of each column. i then delved into various sources such as confluence, github, \nalation, and snowflake to gather specific information on each column and created a first draft of the \nmetadata. additionally, i studied and explored openai, a large language model that has the potential \nto be used for auto-generating metadata for snowflake tables and columns. finally, i attended regular \nstand-up meetings with the data architecture team to discuss ongoing projects and updates. \noutcomes: i successfully created the metadata for snowflake columns manually. additionally, i \nsuccessfully explored the openai api quickstart tutorial in order to understand how to access the \nmodel and fundamentals to use the api on the question and answering task. \nknowledge, skills and experiences \nknowledge: i got a better understanding of the domain’s documentation, and data discovery by \nbrowsing and exploring through confluence, alation, and snowflake.   \nskills: i improve my exploring and researching skills while obtaining the information for generating \ncolumn descriptions and continue to develop collaboration skills by working with my supervisor and \nteam members. \nexperience:  i spent time studying openai and exploring how it can be applied to question and \nanswering tasks. through my research, i gained a better understanding of how to access the model \nand how it can be leveraged to enhance data processing and analysis. this learning experience has \nequipped me with valuable knowledge and skills that can be utilised to improve the data processing \npipeline. \n23 \n \nrewarding experiences \nthis week, i found exploring domain's documentation and studying openai question answering \nusing embeddings to be a particularly rewarding experience. i gained valuable insights that i can \nutilise to build an auto-generating metadata tool soon.  \nchallenging experiences \nduring my exploration of domain's tables and columns, i found that there is limited documentation \navailable. as a result, i had to rely on reviewing the source code in github to gain a better \nunderstanding. unfortunately, i discovered that the code lacked detailed comments or explanations, \nmaking it more challenging to comprehend. \nupcoming tasks \ni plan to review the results of my manual tagging labels with my supervisor to determine if the labels \nalign with their expectations. additionally, i intend to continue studying openai models and their \napplications to improve my skills and knowledge in this area. \n \nd.",
            "page": null,
            "goal": "week 3 \ngoals, activities and outcomes \ngoals: in week 3, my objectives are to conduct research on a range of tools and technologies with \nthe goal of generating metadata descriptions and becoming acquainted with the tools and \ntechnologies utilised by the domain data architect team and building a relationship with the team \nmembers.  \nactivities: this week, i dedicated most of my time to manually creating metadata for one of \ndomain's snowflake tables. to generate the metadata, i first utilised chatgpt to assist in generating a \ngeneral description of each column. i then delved into various sources such as confluence, github, \nalation, and snowflake to gather specific information on each column and created a first draft of the \nmetadata. additionally, i studied and explored openai, a large language model that has the potential \nto be used for auto-generating metadata for snowflake tables and columns. finally, i attended regular \nstand-up meetings with the data architecture team to discuss ongoing projects and updates. \noutcomes: i successfully created the metadata for snowflake columns manually. additionally, i \nsuccessfully explored the openai api quickstart tutorial in order to understand how to access the \nmodel and fundamentals to use the api on the question and answering task. \nknowledge, skills and experiences \nknowledge: i got a better understanding of the domain’s documentation, and data discovery by \nbrowsing and exploring through confluence, alation, and snowflake.   \nskills: i improve my exploring and researching skills while obtaining the information for generating \ncolumn descriptions and continue to develop collaboration skills by working with my supervisor and \nteam members. \nexperience:  i spent time studying openai and exploring how it can be applied to question and \nanswering tasks. through my research, i gained a better understanding of how to access the model \nand how it can be leveraged to enhance data processing and analysis. this learning experience has \nequipped me with valuable knowledge and skills that can be utilised to improve the data processing \npipeline. \n23 \n \nrewarding experiences \nthis week, i found exploring domain's documentation and studying openai question answering \nusing embeddings to be a particularly rewarding experience. i gained valuable insights that i can \nutilise to build an auto-generating metadata tool soon.  \nchallenging experiences \nduring my exploration of domain's tables and columns, i found that there is limited documentation \navailable. as a result, i had to rely on reviewing the source code in github to gain a better \nunderstanding. unfortunately, i discovered that the code lacked detailed comments or explanations, \nmaking it more challenging to comprehend. \nupcoming tasks \ni plan to review the results of my manual tagging labels with my supervisor to determine if the labels \nalign with their expectations. additionally, i intend to continue studying openai models and their \napplications to improve my skills and knowledge in this area. \n \nd.",
            "children": []
        },
        {
            "id": "1.7",
            "name": "week 4",
            "nodeType": "title",
            "text": "week 4",
            "page": null,
            "goal": "week 4",
            "children": []
        },
        {
            "id": "1.8",
            "name": "week 4 goals,",
            "nodeType": "paragraph",
            "text": "week 4 \ngoals, activities and outcomes \ngoals: my objectives for week 4 are to conduct research on how to apply openai's model for \nanswering questions from domain's internal documentation on confluence. additionally, i aim to \ndevelop better relationships with team members by attending regular stand-up meetings and \nparticipating in 1:1 meeting with relevant leads. \nactivities: after i successfully created the metadata for snowflake columns manually in the last week, \nthis week, i explored how to apply openai's gpt-3.5 model to answer questions from domain's \ninternal documentation on confluence. to do this, i examined the example provided in openai's \ngithub repository and replicated the code from a medium page \"running an openai's model on \nyour internal confluence documentation.\" therefore, i focused on improving my scraping technique \nto obtain more accurate and useful data.  \naside from these technical tasks, i also attended regular stand-up meetings and had a 1:1 meeting \nwith jibin joy, who is the data architecture lead. during our meeting, we discussed my project and \nwhat steps i should take next to further develop it. we also talked about how i can build relationships \nwith people in other data teams, which i believe will be crucial to my success in this role. \noutcomes: i successfully replicated the code from the medium page to apply openai's gpt-3.5 \nmodel to answer questions from domain's internal documentation on confluence. while the code \nworked as expected, i encountered a challenge when the data i had scraped from confluence was of \npoor quality. \nknowledge, skills and experiences \nknowledge: throughout this week, i deepened my understanding of confluence api and openai api. \nby closely studying and replicating the code from openai's github repository and the medium page \n24 \n \n\"running an openai's model on your internal confluence documentation,\" i gained valuable \nknowledge on how to use these apis effectively. \nskills: i gained programming and problem-solving skills by replicating and identifying and addressing \nissues with the data scraped from confluence. \nexperience:  i gained valuable experience working with large datasets by scraping data from \ndomain's internal confluence pages. in addition, i developed expertise in machine learning \ntechniques such as document embedding and vector similarity, which allowed me to measure \nsimilarity from embedding. \nrewarding experiences \nduring this report period, i found the experience of working with confluence api and openai api to \nbe particularly rewarding. through my own experimentation and testing, i gained a deeper \nunderstanding of how to extract data from confluence and apply openai's gpt-3.5 model to it. i also \nenjoyed the challenge of problem-solving when the data scraped from confluence was of poor \nquality and needed to be improved. \nchallenging experiences \nduring this report period, i faced challenges related to dealing with poor-quality data and unfamiliar \ntools and technology. i have to develop new skills and techniques to improve my data scraping \nprocess due to poor data quality, while also investing time and effort to learn how to use confluence \napi and openai api.  \nupcoming tasks \ni plan to focus on improving the quality of data scraped from confluence pages by deepening my \nunderstanding of confluence pages and api, and learning more about data scraping and cleaning \ntechniques to produce high-quality data that can be used for training openai's gpt-3.5 model. \nadditionally, i will be looking for opportunities to collaborate with other members of the data team \nto gain insights and feedback on my progress. \n \ne.",
            "page": null,
            "goal": "week 4 \ngoals, activities and outcomes \ngoals: my objectives for week 4 are to conduct research on how to apply openai's model for \nanswering questions from domain's internal documentation on confluence. additionally, i aim to \ndevelop better relationships with team members by attending regular stand-up meetings and \nparticipating in 1:1 meeting with relevant leads. \nactivities: after i successfully created the metadata for snowflake columns manually in the last week, \nthis week, i explored how to apply openai's gpt-3.5 model to answer questions from domain's \ninternal documentation on confluence. to do this, i examined the example provided in openai's \ngithub repository and replicated the code from a medium page \"running an openai's model on \nyour internal confluence documentation.\" therefore, i focused on improving my scraping technique \nto obtain more accurate and useful data.  \naside from these technical tasks, i also attended regular stand-up meetings and had a 1:1 meeting \nwith jibin joy, who is the data architecture lead. during our meeting, we discussed my project and \nwhat steps i should take next to further develop it. we also talked about how i can build relationships \nwith people in other data teams, which i believe will be crucial to my success in this role. \noutcomes: i successfully replicated the code from the medium page to apply openai's gpt-3.5 \nmodel to answer questions from domain's internal documentation on confluence. while the code \nworked as expected, i encountered a challenge when the data i had scraped from confluence was of \npoor quality. \nknowledge, skills and experiences \nknowledge: throughout this week, i deepened my understanding of confluence api and openai api. \nby closely studying and replicating the code from openai's github repository and the medium page \n24 \n \n\"running an openai's model on your internal confluence documentation,\" i gained valuable \nknowledge on how to use these apis effectively. \nskills: i gained programming and problem-solving skills by replicating and identifying and addressing \nissues with the data scraped from confluence. \nexperience:  i gained valuable experience working with large datasets by scraping data from \ndomain's internal confluence pages. in addition, i developed expertise in machine learning \ntechniques such as document embedding and vector similarity, which allowed me to measure \nsimilarity from embedding. \nrewarding experiences \nduring this report period, i found the experience of working with confluence api and openai api to \nbe particularly rewarding. through my own experimentation and testing, i gained a deeper \nunderstanding of how to extract data from confluence and apply openai's gpt-3.5 model to it. i also \nenjoyed the challenge of problem-solving when the data scraped from confluence was of poor \nquality and needed to be improved. \nchallenging experiences \nduring this report period, i faced challenges related to dealing with poor-quality data and unfamiliar \ntools and technology. i have to develop new skills and techniques to improve my data scraping \nprocess due to poor data quality, while also investing time and effort to learn how to use confluence \napi and openai api.  \nupcoming tasks \ni plan to focus on improving the quality of data scraped from confluence pages by deepening my \nunderstanding of confluence pages and api, and learning more about data scraping and cleaning \ntechniques to produce high-quality data that can be used for training openai's gpt-3.5 model. \nadditionally, i will be looking for opportunities to collaborate with other members of the data team \nto gain insights and feedback on my progress. \n \ne.",
            "children": []
        },
        {
            "id": "1.9",
            "name": "week 5",
            "nodeType": "title",
            "text": "week 5",
            "page": null,
            "goal": "week 5",
            "children": []
        },
        {
            "id": "1.10",
            "name": "week 5 goals,",
            "nodeType": "paragraph",
            "text": "week 5 \ngoals, activities and outcomes \ngoals: explore an alternative approach to auto-generate column descriptions. \nactivities: this week, i focused on building an auto-generating tool using openai's gpt3.5 model. i \ncreated prompts that included table names and column names, and then fed them into the openai \ncompletion model \"text-davinci-002\". the results were impressive, with the tool generating accurate \ngeneral descriptions. however, my supervisor ken suggested using the chat model \"gpt-3.5-turbo\" as \nit has similar capabilities but is more cost-effective. \nadditionally, i had a productive discussion with jibin, the data architecture lead, and anderson, the \ndata engineer, about the two approaches i experimented with using openai's model with internal \nconfluence documentation and using the generic openai. we discussed concerns regarding \n25 \n \npermission to use openai with internal documents and the budget to use openai. therefore, i \nprepared presentation slides to explain my project and what i had done to obtain approval from the \ninvolved team leaders. \noutcomes: i successfully utilised the generic openai model to generate column descriptions for 75 \ntables. while the majority of the results were accurate, i found that some model results were in \ndifferent formats. \nknowledge, skills and experiences \nknowledge: i gained a better understanding of how to use the openai gpt-3.5 model to generate \ntext and descriptions by experimenting with different prompts and techniques. \nskills: i developed my presentation skills by creating a slide deck to explain my project and seek \napproval from team leaders. \nexperience:  i had the opportunity to discuss my project with experienced professionals in data \narchitecture and engineering and gained valuable insights and feedback on my approach. \nrewarding experiences \ngenerating column descriptions for 75 tables using the openai gpt-3.5 model was a major \nachievement this week as it required a lot of effort and skills. \nchallenging experiences \ndealing with the inconsistency of the openai gpt-3.5 model: although i was able to generate column \ndescriptions for 75 tables using the model, there were times when the output was not in the \nexpected format, which made it challenging to work with.  \nanother challenge i encountered was the limitation of my openai free account credit. i require \nadditional credit to continue my work. to address this challenge, i will present my project to team \nleaders next week. \nupcoming tasks \nfor the upcoming week, my primary tasks will be focused on improving the performance of the \ngeneric openai model by addressing unexpected results. i will also work on improving data scraping \nand cleaning for the use of openai on internal confluence documents. another important task will \nbe presenting my project to team leaders to seek their approval for continued use of openai and \ncollection of data from internal references. \n \n \n \n \n \n26 \n \nf.",
            "page": null,
            "goal": "week 5 \ngoals, activities and outcomes \ngoals: explore an alternative approach to auto-generate column descriptions. \nactivities: this week, i focused on building an auto-generating tool using openai's gpt3.5 model. i \ncreated prompts that included table names and column names, and then fed them into the openai \ncompletion model \"text-davinci-002\". the results were impressive, with the tool generating accurate \ngeneral descriptions. however, my supervisor ken suggested using the chat model \"gpt-3.5-turbo\" as \nit has similar capabilities but is more cost-effective. \nadditionally, i had a productive discussion with jibin, the data architecture lead, and anderson, the \ndata engineer, about the two approaches i experimented with using openai's model with internal \nconfluence documentation and using the generic openai. we discussed concerns regarding \n25 \n \npermission to use openai with internal documents and the budget to use openai. therefore, i \nprepared presentation slides to explain my project and what i had done to obtain approval from the \ninvolved team leaders. \noutcomes: i successfully utilised the generic openai model to generate column descriptions for 75 \ntables. while the majority of the results were accurate, i found that some model results were in \ndifferent formats. \nknowledge, skills and experiences \nknowledge: i gained a better understanding of how to use the openai gpt-3.5 model to generate \ntext and descriptions by experimenting with different prompts and techniques. \nskills: i developed my presentation skills by creating a slide deck to explain my project and seek \napproval from team leaders. \nexperience:  i had the opportunity to discuss my project with experienced professionals in data \narchitecture and engineering and gained valuable insights and feedback on my approach. \nrewarding experiences \ngenerating column descriptions for 75 tables using the openai gpt-3.5 model was a major \nachievement this week as it required a lot of effort and skills. \nchallenging experiences \ndealing with the inconsistency of the openai gpt-3.5 model: although i was able to generate column \ndescriptions for 75 tables using the model, there were times when the output was not in the \nexpected format, which made it challenging to work with.  \nanother challenge i encountered was the limitation of my openai free account credit. i require \nadditional credit to continue my work. to address this challenge, i will present my project to team \nleaders next week. \nupcoming tasks \nfor the upcoming week, my primary tasks will be focused on improving the performance of the \ngeneric openai model by addressing unexpected results. i will also work on improving data scraping \nand cleaning for the use of openai on internal confluence documents. another important task will \nbe presenting my project to team leaders to seek their approval for continued use of openai and \ncollection of data from internal references. \n \n \n \n \n \n26 \n \nf.",
            "children": []
        },
        {
            "id": "1.11",
            "name": "week 6",
            "nodeType": "title",
            "text": "week 6",
            "page": null,
            "goal": "week 6",
            "children": []
        },
        {
            "id": "1.12",
            "name": "week 6 goals,",
            "nodeType": "paragraph",
            "text": "week 6 \ngoals, activities and outcomes \ngoals: my goals this week are to prepare presentations for domain and the university and continue \ndeveloping the metadata tool. \nactivities: this week, i focused on preparing two presentation slides, one for the domain and \nanother for the university. during the presentation for the domain, i discussed the capabilities of \nopenai for data catalogue to an audience consisting of the data director, head of data governance, \ndata architecture lead, and other co-workers from the data architecture, data scientist, and data \ngovernance teams. i presented my experiment results so far, explained what metadescriptor is, and \nproposed to ask for operation and experiment budgets to use openai’s models. additionally, i \nrequested permission to supply domain internal documents to openai's models. the presentation \nwas well received, and rogerio, data director agreed to approve the budget. moreover, i need to \nprovide a list of internal documentation links that require permission to collect data. \nanother presentation was a mid-semester presentation. i gave a presentation to professor amin, \nmahdieh, and other students, which covered my company's background, my role and \nresponsibilities, my progress, and the skills i acquired. the presentation went well, but i plan to \nimprove my slides for the final presentation. \nin addition, during the week, i focused on gathering internal documentation links and sharing them \nwith team leaders to obtain permission from the chief data officer, pooyan. furthermore, i spent \ntime refining the code to address the previous week's problem of utilising a generic openai model to \ngenerate column descriptions for 75 tables. some of the outcomes were in diverse formats, and i \nworked to resolve this issue. \noutcomes: i successfully utilised the generic openai model to generate column descriptions for 75 \ntables. while the majority of the results were accurate, i found that some model results were in \ndifferent formats. \nknowledge, skills and experiences \nknowledge: over of the week, i gained valuable knowledge related to data privacy and governance, \nparticularly the importance of obtaining permission and approval when collecting and using internal \ncompany documentation to supply to third-party companies such as openai. this understanding \nhelped me become more aware of the terms of use of openai and the potential risks of non-\ncompliance. overall, i have developed a deeper understanding of the importance of data governance \nand the need to comply with regulations and policies. \nskills: i developed my presentation and communication abilities through the preparation and \ndelivery of two presentations. these experiences allowed me to hone my skills in conveying complex \ntechnical concepts to both technical and non-technical audiences. additionally, i further refined my \ncoding abilities through troubleshooting and improving the auto-generating metadata tool. \nexperience:  i had the opportunity to engage with various stakeholders, including team leaders, the \ndata director, and the chief data officer. these interactions provided me with insight into how data \nmanagement and governance are approached in a large organisation, as well as the opportunity to \nbuild relationships and networks within the company.  \n \n27 \n \nrewarding experiences \none particularly rewarding experience during this report period was giving my presentation on \nopenai capabilities for data cataloguing to the domain team. it was great to see that the audience \nwas engaged and interested in the topic and to hear that the data director was willing to approve \nthe budget for our project. this experience showed me that the work i am doing is valuable and has \nthe potential to make a real impact on the company. overall, i feel that this week was a valuable \nlearning experience that allowed me to grow both personally and professionally. \nchallenging experiences \none challenge i faced during this report period was the limitation of my openai free account credit. \nsince i used a lot of credits to experiment with different models, i ran out of credits and could not \nuse the account until my budget was approved. however, i was able to overcome this obstacle thanks \nto the support of my supervisor, ken. he kindly offered to let me use his account while waiting for my \nbudget to be approved. this allowed me to continue working on my project without any significant \ninterruption. i'm grateful for his help and support. \nupcoming tasks \ni will continue to work on refining the metadescriptor tool and improving its efficiency in generating \nmetadata for snowflake tables and columns. i will focus on improving the accuracy of data by \nenhancing the data collection and cleaning process. \n \ng.",
            "page": null,
            "goal": "week 6 \ngoals, activities and outcomes \ngoals: my goals this week are to prepare presentations for domain and the university and continue \ndeveloping the metadata tool. \nactivities: this week, i focused on preparing two presentation slides, one for the domain and \nanother for the university. during the presentation for the domain, i discussed the capabilities of \nopenai for data catalogue to an audience consisting of the data director, head of data governance, \ndata architecture lead, and other co-workers from the data architecture, data scientist, and data \ngovernance teams. i presented my experiment results so far, explained what metadescriptor is, and \nproposed to ask for operation and experiment budgets to use openai’s models. additionally, i \nrequested permission to supply domain internal documents to openai's models. the presentation \nwas well received, and rogerio, data director agreed to approve the budget. moreover, i need to \nprovide a list of internal documentation links that require permission to collect data. \nanother presentation was a mid-semester presentation. i gave a presentation to professor amin, \nmahdieh, and other students, which covered my company's background, my role and \nresponsibilities, my progress, and the skills i acquired. the presentation went well, but i plan to \nimprove my slides for the final presentation. \nin addition, during the week, i focused on gathering internal documentation links and sharing them \nwith team leaders to obtain permission from the chief data officer, pooyan. furthermore, i spent \ntime refining the code to address the previous week's problem of utilising a generic openai model to \ngenerate column descriptions for 75 tables. some of the outcomes were in diverse formats, and i \nworked to resolve this issue. \noutcomes: i successfully utilised the generic openai model to generate column descriptions for 75 \ntables. while the majority of the results were accurate, i found that some model results were in \ndifferent formats. \nknowledge, skills and experiences \nknowledge: over of the week, i gained valuable knowledge related to data privacy and governance, \nparticularly the importance of obtaining permission and approval when collecting and using internal \ncompany documentation to supply to third-party companies such as openai. this understanding \nhelped me become more aware of the terms of use of openai and the potential risks of non-\ncompliance. overall, i have developed a deeper understanding of the importance of data governance \nand the need to comply with regulations and policies. \nskills: i developed my presentation and communication abilities through the preparation and \ndelivery of two presentations. these experiences allowed me to hone my skills in conveying complex \ntechnical concepts to both technical and non-technical audiences. additionally, i further refined my \ncoding abilities through troubleshooting and improving the auto-generating metadata tool. \nexperience:  i had the opportunity to engage with various stakeholders, including team leaders, the \ndata director, and the chief data officer. these interactions provided me with insight into how data \nmanagement and governance are approached in a large organisation, as well as the opportunity to \nbuild relationships and networks within the company.  \n \n27 \n \nrewarding experiences \none particularly rewarding experience during this report period was giving my presentation on \nopenai capabilities for data cataloguing to the domain team. it was great to see that the audience \nwas engaged and interested in the topic and to hear that the data director was willing to approve \nthe budget for our project. this experience showed me that the work i am doing is valuable and has \nthe potential to make a real impact on the company. overall, i feel that this week was a valuable \nlearning experience that allowed me to grow both personally and professionally. \nchallenging experiences \none challenge i faced during this report period was the limitation of my openai free account credit. \nsince i used a lot of credits to experiment with different models, i ran out of credits and could not \nuse the account until my budget was approved. however, i was able to overcome this obstacle thanks \nto the support of my supervisor, ken. he kindly offered to let me use his account while waiting for my \nbudget to be approved. this allowed me to continue working on my project without any significant \ninterruption. i'm grateful for his help and support. \nupcoming tasks \ni will continue to work on refining the metadescriptor tool and improving its efficiency in generating \nmetadata for snowflake tables and columns. i will focus on improving the accuracy of data by \nenhancing the data collection and cleaning process. \n \ng.",
            "children": []
        },
        {
            "id": "1.13",
            "name": "week 7",
            "nodeType": "title",
            "text": "week 7",
            "page": null,
            "goal": "week 7",
            "children": []
        },
        {
            "id": "1.14",
            "name": "week 7 goals,",
            "nodeType": "paragraph",
            "text": "week 7 \ngoals, activities and outcomes \ngoals: this week, i focused on two main tasks: finalising my mid-semester report for the university \nand continuing to develop the metadata tool. \nactivities: for the mid-semester report, i reviewed and edited my draft, incorporating feedback from \nmy supervisor. i also made sure to follow the guidelines and formatting requirements provided by \nthe university. the report is now complete and ready to submit. \nregarding the metadata tool, firstly, i focused on improving the efficiency and accuracy of the code \nby using the \"text-davinci-002\" completion model. i analysed the results from the previous week's \nexperiment and made necessary adjustments to the code to optimise its performance. \nin addition to this, i conducted experiments with the \"gpt-3.5-turbo\" completion model as an \nalternative option. i carefully crafted the input prompts for the model and fine-tuned its parameters \nto generate accurate metadata. i tested the model with 75 table names and their corresponding \ncolumn names and found that while the results were clear and accurate, it took twice as long to \nprocess compared to text-davinci-002. however, it was cost-effective, being 10 times cheaper than \nthe previous model. \noutcomes: the week was successful as i was able to finalise my mid-semester report for the \nuniversity, and made significant progress on the development of the metadata tool. i refined the \n28 \n \ncode using the \"text-davinci-002\" completion model and experimented with the \"gpt-3.5-turbo\" \nmodel, which generated accurate metadata at a lower cost. \nknowledge, skills and experiences \nknowledge: during the week, i gained new knowledge on how to write prompts using the gpt-3.5-\nturbo model. i found that it was quite different from the text-davinci-002 model and took some time \nto understand. through searching for example code and applications on the internet, i was able to \ndevelop a better understanding of how to effectively use this model for generating metadata. this \nhas broadened my knowledge and skill set in working with different models and approaches for \nnatural language processing. \nskills: firstly, i improved my report writing skills by incorporating feedback and adhering to university \nguidelines. secondly, i developed my programming skills by refining the metadata tool's code and \nexperimenting with different models.  \nexperience:  this week, i had the chance to practice my time management skills by balancing the \ndemands of my internship and my other responsibilities. through prioritising tasks and managing my \nschedule effectively, i was able to complete my work on time and maintain a healthy work-life \nbalance. this experience has improved my ability to manage competing demands and stay organised. \nrewarding experiences \ni would say that i found the opportunity to work on the metadata tool project to be the most \nrewarding for this report period. specifically, i enjoyed the process of experimenting with different \ncompletion models and input prompts to fine-tune the tool's performance and accuracy. \nchallenging experiences \nduring this report period, i encountered a challenge while experimenting with the gpt-3.5-turbo \nmodel. i faced errors such as ratelimiterror and api error. this was an experience that left me feeling \ntired and frustrated. however, i sought guidance from ken, my supervisor, who suggested that i use \nthe try-catch technique to resume the model from the last stage and continue running it. thanks to \nhis advice, i was able to overcome the issue and successfully complete the model for the entire \ninput. \nupcoming tasks \nmy plan going forward is to further refine the metadescriptor tool, with a specific focus on \nenhancing its efficiency in generating metadata for snowflake tables and columns. this will include \nefforts to improve the accuracy of the generated data by enhancing the data collection and cleaning \nprocesses. \n \n \n \n \n29 \n \nh.",
            "page": null,
            "goal": "week 7 \ngoals, activities and outcomes \ngoals: this week, i focused on two main tasks: finalising my mid-semester report for the university \nand continuing to develop the metadata tool. \nactivities: for the mid-semester report, i reviewed and edited my draft, incorporating feedback from \nmy supervisor. i also made sure to follow the guidelines and formatting requirements provided by \nthe university. the report is now complete and ready to submit. \nregarding the metadata tool, firstly, i focused on improving the efficiency and accuracy of the code \nby using the \"text-davinci-002\" completion model. i analysed the results from the previous week's \nexperiment and made necessary adjustments to the code to optimise its performance. \nin addition to this, i conducted experiments with the \"gpt-3.5-turbo\" completion model as an \nalternative option. i carefully crafted the input prompts for the model and fine-tuned its parameters \nto generate accurate metadata. i tested the model with 75 table names and their corresponding \ncolumn names and found that while the results were clear and accurate, it took twice as long to \nprocess compared to text-davinci-002. however, it was cost-effective, being 10 times cheaper than \nthe previous model. \noutcomes: the week was successful as i was able to finalise my mid-semester report for the \nuniversity, and made significant progress on the development of the metadata tool. i refined the \n28 \n \ncode using the \"text-davinci-002\" completion model and experimented with the \"gpt-3.5-turbo\" \nmodel, which generated accurate metadata at a lower cost. \nknowledge, skills and experiences \nknowledge: during the week, i gained new knowledge on how to write prompts using the gpt-3.5-\nturbo model. i found that it was quite different from the text-davinci-002 model and took some time \nto understand. through searching for example code and applications on the internet, i was able to \ndevelop a better understanding of how to effectively use this model for generating metadata. this \nhas broadened my knowledge and skill set in working with different models and approaches for \nnatural language processing. \nskills: firstly, i improved my report writing skills by incorporating feedback and adhering to university \nguidelines. secondly, i developed my programming skills by refining the metadata tool's code and \nexperimenting with different models.  \nexperience:  this week, i had the chance to practice my time management skills by balancing the \ndemands of my internship and my other responsibilities. through prioritising tasks and managing my \nschedule effectively, i was able to complete my work on time and maintain a healthy work-life \nbalance. this experience has improved my ability to manage competing demands and stay organised. \nrewarding experiences \ni would say that i found the opportunity to work on the metadata tool project to be the most \nrewarding for this report period. specifically, i enjoyed the process of experimenting with different \ncompletion models and input prompts to fine-tune the tool's performance and accuracy. \nchallenging experiences \nduring this report period, i encountered a challenge while experimenting with the gpt-3.5-turbo \nmodel. i faced errors such as ratelimiterror and api error. this was an experience that left me feeling \ntired and frustrated. however, i sought guidance from ken, my supervisor, who suggested that i use \nthe try-catch technique to resume the model from the last stage and continue running it. thanks to \nhis advice, i was able to overcome the issue and successfully complete the model for the entire \ninput. \nupcoming tasks \nmy plan going forward is to further refine the metadescriptor tool, with a specific focus on \nenhancing its efficiency in generating metadata for snowflake tables and columns. this will include \nefforts to improve the accuracy of the generated data by enhancing the data collection and cleaning \nprocesses. \n \n \n \n \n29 \n \nh.",
            "children": []
        },
        {
            "id": "1.15",
            "name": "week 8",
            "nodeType": "title",
            "text": "week 8",
            "page": null,
            "goal": "week 8",
            "children": []
        },
        {
            "id": "1.16",
            "name": "week 8 goals,",
            "nodeType": "paragraph",
            "text": "week 8 \ngoals, activities and outcomes \ngoals: this week, i focused on studying alternative models to build metadata from internal reference \ndocuments. \nactivities: during the past week, my focus was to explore various models for metadata construction. \nmy objective was to gain a deep understanding of the different methodologies for metadata \nmodelling and their potential applications for this project. i conducted research on three different \ntech stacks, which included llama-index, langchain framework, and chroma vector database. \ninitially, i attempted to replicate the process using confluence loader from llama-index, but i \nencountered some issues. to address this challenge, i decided to export the confluence pages to pdf \nfiles and generate an embedding file using langchain. after that, i utilized chroma to store and index \nthe documents. the outcome was successful, and i was able to develop a question and answering \nsystem using multiple pdf files. \noutcomes: the outcome is the successful development of a question and answering system using \nmultiple pdf files through the use of chroma vector database for document indexing and storage, \nand langchain framework for generating an embedding file from the pdf files. \nknowledge, skills and experiences \nknowledge: i gained a deeper understanding of alternative models for metadata construction, \nincluding llama-index, langchain framework, and chroma vector database. through my research, i \nlearned about the advantages and limitations of each approach and gained insights into how they \ncan be applied to build efficient and scalable metadata systems. \nskills: in terms of skills, i developed proficiency in using langchain framework for generating \nembeddings from pdf files and in utilizing chroma vector database for storing and indexing \ndocuments. these skills will be valuable in future projects involving large-scale document processing \nand retrieval. \nexperience:  this week provided me with valuable experience in working through challenges and \nfinding creative solutions. when i encountered issues with the confluence loader from llama-\nindex, i was able to think outside the box and come up with an alternative approach using langchain \nand chroma. this experience reinforced the importance of adaptability and resourcefulness in \nproblem-solving. \nrewarding experiences \nlearning and using alternative models for metadata construction: exploring and learning about new \nmethodologies and technologies can be very exciting and rewarding. being able to apply that \nknowledge to your project and see it succeed can be a great source of satisfaction. \nchallenging experiences \none of the most difficult tasks was troubleshooting issues i encountered while trying to use \nconfluence loader from llama-index. despite spending significant time trying to resolve the issue, i \nultimately had to find a workaround by exporting the confluence pages to pdf files and generating \nan embedding file using langchain. this required additional effort and time and was a frustrating \nexperience. overall, these experiences provided valuable learning opportunities and allowed me to \nfurther develop my problem-solving skills. \n30 \n \nupcoming tasks \nduring the upcoming week, i intend to experiment with various embedding models and explore the \npossibility of constructing a question-and-answer system from alternative resources like json and \nexcel files. \n \ni.",
            "page": null,
            "goal": "week 8 \ngoals, activities and outcomes \ngoals: this week, i focused on studying alternative models to build metadata from internal reference \ndocuments. \nactivities: during the past week, my focus was to explore various models for metadata construction. \nmy objective was to gain a deep understanding of the different methodologies for metadata \nmodelling and their potential applications for this project. i conducted research on three different \ntech stacks, which included llama-index, langchain framework, and chroma vector database. \ninitially, i attempted to replicate the process using confluence loader from llama-index, but i \nencountered some issues. to address this challenge, i decided to export the confluence pages to pdf \nfiles and generate an embedding file using langchain. after that, i utilized chroma to store and index \nthe documents. the outcome was successful, and i was able to develop a question and answering \nsystem using multiple pdf files. \noutcomes: the outcome is the successful development of a question and answering system using \nmultiple pdf files through the use of chroma vector database for document indexing and storage, \nand langchain framework for generating an embedding file from the pdf files. \nknowledge, skills and experiences \nknowledge: i gained a deeper understanding of alternative models for metadata construction, \nincluding llama-index, langchain framework, and chroma vector database. through my research, i \nlearned about the advantages and limitations of each approach and gained insights into how they \ncan be applied to build efficient and scalable metadata systems. \nskills: in terms of skills, i developed proficiency in using langchain framework for generating \nembeddings from pdf files and in utilizing chroma vector database for storing and indexing \ndocuments. these skills will be valuable in future projects involving large-scale document processing \nand retrieval. \nexperience:  this week provided me with valuable experience in working through challenges and \nfinding creative solutions. when i encountered issues with the confluence loader from llama-\nindex, i was able to think outside the box and come up with an alternative approach using langchain \nand chroma. this experience reinforced the importance of adaptability and resourcefulness in \nproblem-solving. \nrewarding experiences \nlearning and using alternative models for metadata construction: exploring and learning about new \nmethodologies and technologies can be very exciting and rewarding. being able to apply that \nknowledge to your project and see it succeed can be a great source of satisfaction. \nchallenging experiences \none of the most difficult tasks was troubleshooting issues i encountered while trying to use \nconfluence loader from llama-index. despite spending significant time trying to resolve the issue, i \nultimately had to find a workaround by exporting the confluence pages to pdf files and generating \nan embedding file using langchain. this required additional effort and time and was a frustrating \nexperience. overall, these experiences provided valuable learning opportunities and allowed me to \nfurther develop my problem-solving skills. \n30 \n \nupcoming tasks \nduring the upcoming week, i intend to experiment with various embedding models and explore the \npossibility of constructing a question-and-answer system from alternative resources like json and \nexcel files. \n \ni.",
            "children": []
        },
        {
            "id": "1.17",
            "name": "week 9",
            "nodeType": "title",
            "text": "week 9",
            "page": null,
            "goal": "week 9",
            "children": []
        },
        {
            "id": "1.18",
            "name": "week 9 goals,",
            "nodeType": "paragraph",
            "text": "week 9 \ngoals, activities and outcomes \ngoals: this week, my goal is to continue exploring and experimenting with alternative models and \ntools for building metadata from internal reference documents. \nactivities: during week 9, we were waiting for the data governance team's response on the suitable \nsolution for developing metadata from internal documents. the aim is to protect sensitive data from \nunauthorized access when sharing it with third-party service providers like openai. to avoid exposing \ncritical information about the domain, we explored two possible solutions to minimize the risk of \ndata exposure, including microsoft azure openai and an in-house aws solution using an open-\nsource large language model (llm) like gpt, hosting it on sagemaker, and setting up a vector \ndatabase on either on-premises or aws server. \nwhile awaiting the data governance team's response, i also explored the usage of the llama-index \nlibrary with the huggingfaceembeddings model to analyse internal documents extracted from \nconfluence and api swagger pages. \noutcomes: i have deepened my understanding of the risks of sharing sensitive data with third-party \nservice providers and explored two potential solutions to reduce the risk of data exposure. \nadditionally, i gained valuable insights from experimenting with the llama-index library and custom \nhuggingfaceembeddings model, which i can use to compare with baseline results in the future. \nknowledge, skills and experiences \nknowledge: i gained new knowledge about data governance and security, specifically in the context \nof sharing sensitive information with third-party service providers. i learned about the potential risks \nof sharing data without a contractual agreement and the importance of minimizing the risk of data \nexposure. i also gained knowledge about the different options available to address these risks. \nskills: this week, i developed skills in experimenting with different embedding models and exploring \ntheir applications in metadata construction. specifically, i worked with the llama-index library and \nhuggingfaceembeddings model to analyse internal documents extracted from confluence and api \nswagger pages.  \nexperience:  i gained practical experience in dealing with data governance challenges and the \nimportance of effective communication between data science and governance teams. \nrewarding experiences \none of the most rewarding experiences during week 9 was gaining hands-on experience in \nconducting research and experimentation. through exploring different solutions to the problem of \nmetadata construction and data governance, i was able to gain practical knowledge that i can apply \nin future projects. additionally, i enjoyed the challenge of experimenting with different embedding \n31 \n \nmodels and exploring their potential applications. this experience helped me build confidence in my \nabilities as a data scientist and gave me a sense of satisfaction in exploring new ideas and \ntechnologies. \nchallenging experiences \nwhile our objective for this week it to discover the approach that can avoid using openai's services, i \nfound that the openai text-davinci models was still invoked during the querying process for text \ncompletion, resulting in a cost of approximately $2-3 for generating 100 column descriptions. \nadditionally, it was observed that the openai key was directly accessed from the local environment \nvariables, which could pose a potential security risk. \nupcoming tasks \nmy focus will be on cleaning the data extracted from confluence pages. this is an essential step in \nensuring the accuracy and quality of the metadata generated from these internal reference \ndocuments. additionally, i will be exploring various techniques and tools that can be used to clean \nand pre-process the data efficiently. \n \n \nj.",
            "page": null,
            "goal": "week 9 \ngoals, activities and outcomes \ngoals: this week, my goal is to continue exploring and experimenting with alternative models and \ntools for building metadata from internal reference documents. \nactivities: during week 9, we were waiting for the data governance team's response on the suitable \nsolution for developing metadata from internal documents. the aim is to protect sensitive data from \nunauthorized access when sharing it with third-party service providers like openai. to avoid exposing \ncritical information about the domain, we explored two possible solutions to minimize the risk of \ndata exposure, including microsoft azure openai and an in-house aws solution using an open-\nsource large language model (llm) like gpt, hosting it on sagemaker, and setting up a vector \ndatabase on either on-premises or aws server. \nwhile awaiting the data governance team's response, i also explored the usage of the llama-index \nlibrary with the huggingfaceembeddings model to analyse internal documents extracted from \nconfluence and api swagger pages. \noutcomes: i have deepened my understanding of the risks of sharing sensitive data with third-party \nservice providers and explored two potential solutions to reduce the risk of data exposure. \nadditionally, i gained valuable insights from experimenting with the llama-index library and custom \nhuggingfaceembeddings model, which i can use to compare with baseline results in the future. \nknowledge, skills and experiences \nknowledge: i gained new knowledge about data governance and security, specifically in the context \nof sharing sensitive information with third-party service providers. i learned about the potential risks \nof sharing data without a contractual agreement and the importance of minimizing the risk of data \nexposure. i also gained knowledge about the different options available to address these risks. \nskills: this week, i developed skills in experimenting with different embedding models and exploring \ntheir applications in metadata construction. specifically, i worked with the llama-index library and \nhuggingfaceembeddings model to analyse internal documents extracted from confluence and api \nswagger pages.  \nexperience:  i gained practical experience in dealing with data governance challenges and the \nimportance of effective communication between data science and governance teams. \nrewarding experiences \none of the most rewarding experiences during week 9 was gaining hands-on experience in \nconducting research and experimentation. through exploring different solutions to the problem of \nmetadata construction and data governance, i was able to gain practical knowledge that i can apply \nin future projects. additionally, i enjoyed the challenge of experimenting with different embedding \n31 \n \nmodels and exploring their potential applications. this experience helped me build confidence in my \nabilities as a data scientist and gave me a sense of satisfaction in exploring new ideas and \ntechnologies. \nchallenging experiences \nwhile our objective for this week it to discover the approach that can avoid using openai's services, i \nfound that the openai text-davinci models was still invoked during the querying process for text \ncompletion, resulting in a cost of approximately $2-3 for generating 100 column descriptions. \nadditionally, it was observed that the openai key was directly accessed from the local environment \nvariables, which could pose a potential security risk. \nupcoming tasks \nmy focus will be on cleaning the data extracted from confluence pages. this is an essential step in \nensuring the accuracy and quality of the metadata generated from these internal reference \ndocuments. additionally, i will be exploring various techniques and tools that can be used to clean \nand pre-process the data efficiently. \n \n \nj.",
            "children": []
        },
        {
            "id": "1.19",
            "name": "week 10",
            "nodeType": "title",
            "text": "week 10",
            "page": null,
            "goal": "week 10",
            "children": []
        },
        {
            "id": "1.20",
            "name": "week 10 goals,",
            "nodeType": "paragraph",
            "text": "week 10 \ngoals, activities and outcomes \ngoals: the primary goal for this week was to focus on cleaning the data extracted from confluence \npages. the aim was to ensure the accuracy and quality of the metadata generated from these \ninternal reference documents. additionally, the objective was to explore various techniques and \ntools that can be used to efficiently clean and pre-process the data. \nactivities: to achieve the set goals, i developed a program specifically designed for web content \nextraction and processing, with a focus on confluence pages. the program utilized the confluence \napi to fetch the content of each page and employed beautifulsoup4 for html parsing. it involved \nsteps such as extracting text content from headings and sections, removing unwanted elements, \nhandling special content like tables and lists, and utilizing spacy for natural language processing \ntasks. the activities also included implementing and testing the program, ensuring the correct \ninstallation of required python packages, and configuring the confluence api. \noutcomes: i successfully developed a robust program for extracting and processing content from \nconfluence pages, resulting in clean and structured data ready for analysis. \nknowledge, skills and experiences \nknowledge: i gained knowledge in web content extraction and processing. i learned how to utilize \nthe confluence api to fetch content and parse html using beautifulsoup4. \nskills: i improved my data pre-processing skills, including removing unwanted elements and \nconverting complex content into a more manageable format. \nexperience:  engaging in web content extraction and processing provided me with hands-on \nexperience in working with real-world data. i gained practical experience in utilizing web apis, \nmanipulating html structures, and addressing common challenges in data cleaning and pre-\n32 \n \nprocessing. this experience enhanced my problem-solving and debugging skills in a real-world \ncontext. \nrewarding experiences \nthe experience that was particularly rewarding during this report period was successfully developing \na program for web content extraction and processing from confluence pages. being able to fetch and \nparse the desired information, resulting in clean and structured data ready for analysis, provided a \nsense of accomplishment. gaining proficiency in utilizing the confluence api and html parsing, as \nwell as troubleshooting and debugging in a real-world context, further enhanced my skills and \nproblem-solving abilities. overall, the successful implementation of the program and the hands-on \nlearning gained from working with real data were highly rewarding experiences. \nchallenging experiences \nextracting content from confluence pages proved to be a challenging task during this report period. \nit required gaining knowledge of html format and becoming proficient in using beautifulsoup4. \nhowever, through persistent experimentation and learning, i successfully managed to extract the \ndata and structure it in a more organized format. this experience provided valuable insights and \nimproved my understanding of working with html and data extraction techniques. \nupcoming tasks \nmy primary tasks and duties will involve improving the cleaning process for the data extracted from \nconfluence pages. additionally, i will be actively participating in code reviews, testing, and the \ndeployment of the data pipeline. \n \nk.",
            "page": null,
            "goal": "week 10 \ngoals, activities and outcomes \ngoals: the primary goal for this week was to focus on cleaning the data extracted from confluence \npages. the aim was to ensure the accuracy and quality of the metadata generated from these \ninternal reference documents. additionally, the objective was to explore various techniques and \ntools that can be used to efficiently clean and pre-process the data. \nactivities: to achieve the set goals, i developed a program specifically designed for web content \nextraction and processing, with a focus on confluence pages. the program utilized the confluence \napi to fetch the content of each page and employed beautifulsoup4 for html parsing. it involved \nsteps such as extracting text content from headings and sections, removing unwanted elements, \nhandling special content like tables and lists, and utilizing spacy for natural language processing \ntasks. the activities also included implementing and testing the program, ensuring the correct \ninstallation of required python packages, and configuring the confluence api. \noutcomes: i successfully developed a robust program for extracting and processing content from \nconfluence pages, resulting in clean and structured data ready for analysis. \nknowledge, skills and experiences \nknowledge: i gained knowledge in web content extraction and processing. i learned how to utilize \nthe confluence api to fetch content and parse html using beautifulsoup4. \nskills: i improved my data pre-processing skills, including removing unwanted elements and \nconverting complex content into a more manageable format. \nexperience:  engaging in web content extraction and processing provided me with hands-on \nexperience in working with real-world data. i gained practical experience in utilizing web apis, \nmanipulating html structures, and addressing common challenges in data cleaning and pre-\n32 \n \nprocessing. this experience enhanced my problem-solving and debugging skills in a real-world \ncontext. \nrewarding experiences \nthe experience that was particularly rewarding during this report period was successfully developing \na program for web content extraction and processing from confluence pages. being able to fetch and \nparse the desired information, resulting in clean and structured data ready for analysis, provided a \nsense of accomplishment. gaining proficiency in utilizing the confluence api and html parsing, as \nwell as troubleshooting and debugging in a real-world context, further enhanced my skills and \nproblem-solving abilities. overall, the successful implementation of the program and the hands-on \nlearning gained from working with real data were highly rewarding experiences. \nchallenging experiences \nextracting content from confluence pages proved to be a challenging task during this report period. \nit required gaining knowledge of html format and becoming proficient in using beautifulsoup4. \nhowever, through persistent experimentation and learning, i successfully managed to extract the \ndata and structure it in a more organized format. this experience provided valuable insights and \nimproved my understanding of working with html and data extraction techniques. \nupcoming tasks \nmy primary tasks and duties will involve improving the cleaning process for the data extracted from \nconfluence pages. additionally, i will be actively participating in code reviews, testing, and the \ndeployment of the data pipeline. \n \nk.",
            "children": []
        },
        {
            "id": "1.21",
            "name": "week 11",
            "nodeType": "title",
            "text": "week 11",
            "page": null,
            "goal": "week 11",
            "children": []
        },
        {
            "id": "1.22",
            "name": "week 11 goals,",
            "nodeType": "paragraph",
            "text": "week 11 \ngoals, activities and outcomes \ngoals: during week 11, my focus was on enhancing the data cleaning process for the content \nextracted from confluence pages.  \nactivities: throughout the week, i dedicated time and effort to refine the cleaning techniques and \nensure the accuracy and quality of the extracted data. additionally, i conducted experiments with \nother large language models (llms) such as fastchat-t5 and wizardlm-7b, attempting to incorporate \nthese llms into the metadata generation workflow. \noutcomes: firstly, the data cleaning process was significantly improved, leading to enhanced \naccuracy and quality of the extracted content from confluence pages. secondly, the experiments \nwith fastchat-t5 and wizardlm-7b, although hindered by limited gpu resources, provided valuable \ninsights into the requirements and limitations of advanced language models. \nknowledge, skills and experiences \nknowledge: i acquired knowledge about different large language models (llms) such as fastchat-t5 \nand wizardlm-7b, and their potential applications in metadata generation. \n33 \n \nskills: i expanded my skills in the field of data cleaning and pre-processing. i gained a deeper \nunderstanding of the specific techniques required to clean data extracted from confluence pages, \nensuring its accuracy and reliability. \nexperience: the experience of working with different llms and attempting their integration \ndeepened my understanding of their capabilities and limitations. while the experiments were not \nsuccessful, these experiences allowed me to gain insights into the practical considerations and \nrequirements for utilizing advanced language models. \nrewarding experiences \none rewarding experience was the significant progress made in improving the data cleaning process. \nthe successful implementation of refinements and enhancements resulted in higher accuracy and \nquality of the extracted content from confluence pages. the opportunity to explore different \napproaches and techniques in data cleaning provided valuable learning experiences. \n \nchallenging experiences \nduring this report period, one particularly challenging experience was the unsuccessful attempts to \napply other large language models (llms) such as fastchat-t5 and wizardlm-7b. the lack of \navailable gpu resources posed a significant obstacle in exploring and experimenting with these \nadvanced models. this technical constraint hindered the evaluation and utilization of these models \nfor metadata generation. however, this experience provided valuable insights into the hardware \nrequirements and limitations of working with advanced language models, emphasizing the need for \nadequate resources for successful implementation. \nupcoming tasks \nmy principal tasks and duties will involve finalizing my project. this will include cleaning up my \ngithub repository by organizing and optimizing the code, documentation, and project files. \nadditionally, i will prepare for the final presentation with the university. \n \nl.",
            "page": null,
            "goal": "week 11 \ngoals, activities and outcomes \ngoals: during week 11, my focus was on enhancing the data cleaning process for the content \nextracted from confluence pages.  \nactivities: throughout the week, i dedicated time and effort to refine the cleaning techniques and \nensure the accuracy and quality of the extracted data. additionally, i conducted experiments with \nother large language models (llms) such as fastchat-t5 and wizardlm-7b, attempting to incorporate \nthese llms into the metadata generation workflow. \noutcomes: firstly, the data cleaning process was significantly improved, leading to enhanced \naccuracy and quality of the extracted content from confluence pages. secondly, the experiments \nwith fastchat-t5 and wizardlm-7b, although hindered by limited gpu resources, provided valuable \ninsights into the requirements and limitations of advanced language models. \nknowledge, skills and experiences \nknowledge: i acquired knowledge about different large language models (llms) such as fastchat-t5 \nand wizardlm-7b, and their potential applications in metadata generation. \n33 \n \nskills: i expanded my skills in the field of data cleaning and pre-processing. i gained a deeper \nunderstanding of the specific techniques required to clean data extracted from confluence pages, \nensuring its accuracy and reliability. \nexperience: the experience of working with different llms and attempting their integration \ndeepened my understanding of their capabilities and limitations. while the experiments were not \nsuccessful, these experiences allowed me to gain insights into the practical considerations and \nrequirements for utilizing advanced language models. \nrewarding experiences \none rewarding experience was the significant progress made in improving the data cleaning process. \nthe successful implementation of refinements and enhancements resulted in higher accuracy and \nquality of the extracted content from confluence pages. the opportunity to explore different \napproaches and techniques in data cleaning provided valuable learning experiences. \n \nchallenging experiences \nduring this report period, one particularly challenging experience was the unsuccessful attempts to \napply other large language models (llms) such as fastchat-t5 and wizardlm-7b. the lack of \navailable gpu resources posed a significant obstacle in exploring and experimenting with these \nadvanced models. this technical constraint hindered the evaluation and utilization of these models \nfor metadata generation. however, this experience provided valuable insights into the hardware \nrequirements and limitations of working with advanced language models, emphasizing the need for \nadequate resources for successful implementation. \nupcoming tasks \nmy principal tasks and duties will involve finalizing my project. this will include cleaning up my \ngithub repository by organizing and optimizing the code, documentation, and project files. \nadditionally, i will prepare for the final presentation with the university. \n \nl.",
            "children": []
        },
        {
            "id": "1.23",
            "name": "week 12",
            "nodeType": "title",
            "text": "week 12",
            "page": null,
            "goal": "week 12",
            "children": []
        },
        {
            "id": "1.24",
            "name": "week 12 goals,",
            "nodeType": "paragraph",
            "text": "week 12 \ngoals, activities and outcomes \ngoals: my primary goal was to finalize my project by focusing on two key areas: cleaning up my \ngithub repository and preparing for the final presentation with the university.  \nactivities: to achieve these goals, i dedicated my time and effort to cleaning up my github \nrepository. this involved organizing and optimizing the code, documentation, and project files. i \nreviewed the repository's structure, updated readme files, and ensured proper documentation for \nfuture reference. additionally, i spent time preparing for the final presentation with the university, \nrefining my presentation materials, and ensuring clear communication of the project's objectives, \nmethodologies, and outcomes. \noutcomes: as a result of these activities, i successfully finalized my project. the github repository is \nnow clean, well-organized, and optimized, making it easier for others to understand and contribute \nto the project. the final presentation materials are polished and effectively convey the key aspects \n34 \n \nand outcomes of the project. these outcomes contribute to the overall success of the project and lay \na strong foundation for future collaboration and development. \nknowledge, skills and experiences \nknowledge: i acquired valuable knowledge in team collaboration on github repositories. i learned \nthe intricacies of working together with a team, including how to effectively contribute to a shared \nproject by submitting pull requests. i gained a deeper understanding of version control principles, \nsuch as branching and merging, and how they contribute to efficient collaboration on github. \nskills: through the process of finalizing my project, i developed essential skills in organizing and \noptimizing code, documentation, and project files. i learned how to streamline the repository \nstructure, improve file naming conventions, and ensure clear and concise documentation. \nexperience: collaborating on a github repository provided valuable insights and experiences. \ncontributing to the project's development, navigating merge conflicts, and working with different \nbranches enhanced my understanding of github and fostered effective teamwork. these experiences \nwill be invaluable for future collaborations.  \nrewarding experiences \na notable experience during this week was the preparation of the final presentation for the project. i \ninvested time and effort in creating well-crafted presentation slides that effectively conveyed the \nproject objectives and outcomes. this experience enhanced my skills in preparing and delivering \ntechnical presentations, ensuring that the content was concise, visually appealing, and easy to \nunderstand for a technical audience. it highlighted the importance of clear communication and \npolished presentation materials in effectively conveying project insights and achievements. \nchallenging experiences \none particularly challenging experience during this report period was preparing the final \npresentation slides within a limited time frame of just 5 minutes. this task required condensing a \nsignificant amount of information and summarizing the key highlights from the project. it was a \nchallenge to effectively communicate the most important points and convey the project's objectives, \nmethodologies, and outcomes in a concise manner. however, through careful analysis and \nprioritization, i was able to overcome this difficulty and create slides that effectively captured the \nessence of the project. this experience provided valuable insights into the importance of concise \ncommunication and the ability to convey complex information into clear and impactful messages. \nupcoming tasks \ni will prepare for the presentation and showcase of my work in domain, as well as finalize the final \nreport for the university.",
            "page": null,
            "goal": "week 12 \ngoals, activities and outcomes \ngoals: my primary goal was to finalize my project by focusing on two key areas: cleaning up my \ngithub repository and preparing for the final presentation with the university.  \nactivities: to achieve these goals, i dedicated my time and effort to cleaning up my github \nrepository. this involved organizing and optimizing the code, documentation, and project files. i \nreviewed the repository's structure, updated readme files, and ensured proper documentation for \nfuture reference. additionally, i spent time preparing for the final presentation with the university, \nrefining my presentation materials, and ensuring clear communication of the project's objectives, \nmethodologies, and outcomes. \noutcomes: as a result of these activities, i successfully finalized my project. the github repository is \nnow clean, well-organized, and optimized, making it easier for others to understand and contribute \nto the project. the final presentation materials are polished and effectively convey the key aspects \n34 \n \nand outcomes of the project. these outcomes contribute to the overall success of the project and lay \na strong foundation for future collaboration and development. \nknowledge, skills and experiences \nknowledge: i acquired valuable knowledge in team collaboration on github repositories. i learned \nthe intricacies of working together with a team, including how to effectively contribute to a shared \nproject by submitting pull requests. i gained a deeper understanding of version control principles, \nsuch as branching and merging, and how they contribute to efficient collaboration on github. \nskills: through the process of finalizing my project, i developed essential skills in organizing and \noptimizing code, documentation, and project files. i learned how to streamline the repository \nstructure, improve file naming conventions, and ensure clear and concise documentation. \nexperience: collaborating on a github repository provided valuable insights and experiences. \ncontributing to the project's development, navigating merge conflicts, and working with different \nbranches enhanced my understanding of github and fostered effective teamwork. these experiences \nwill be invaluable for future collaborations.  \nrewarding experiences \na notable experience during this week was the preparation of the final presentation for the project. i \ninvested time and effort in creating well-crafted presentation slides that effectively conveyed the \nproject objectives and outcomes. this experience enhanced my skills in preparing and delivering \ntechnical presentations, ensuring that the content was concise, visually appealing, and easy to \nunderstand for a technical audience. it highlighted the importance of clear communication and \npolished presentation materials in effectively conveying project insights and achievements. \nchallenging experiences \none particularly challenging experience during this report period was preparing the final \npresentation slides within a limited time frame of just 5 minutes. this task required condensing a \nsignificant amount of information and summarizing the key highlights from the project. it was a \nchallenge to effectively communicate the most important points and convey the project's objectives, \nmethodologies, and outcomes in a concise manner. however, through careful analysis and \nprioritization, i was able to overcome this difficulty and create slides that effectively captured the \nessence of the project. this experience provided valuable insights into the importance of concise \ncommunication and the ability to convey complex information into clear and impactful messages. \nupcoming tasks \ni will prepare for the presentation and showcase of my work in domain, as well as finalize the final \nreport for the university.",
            "children": []
        }
    ]
}