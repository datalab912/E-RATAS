{
    "id": "1",
    "name": "Reflective journal Entities",
    "nodeType": "section",
    "text": "week 1 \ngoals, activities, and outcomes \ngoals \nthe goal of the first week was to secure an internship project by thursday, february 24th, 2022. another goal \nwas to write up an internship proposal that describes the project description, objectives, and estimated \ntimeline of the internship plan. \nactivities \ni contacted a few companies and did interviews with ones that contacted me back.   \nafter i secured a company to work in, i communicated with my supervisor regarding the details of the \ninternship project as well as on the internship proposal. \noutcomes \nafter a few interviews, i thankfully got a few offers from the companies that interviewed me. eventually, i \naccepted an offer with domain for a part-time internship program. \nnew knowledge, skills, and experiences \nknowledge \ni learned to properly communicate in interviews so that i can showcase my strengths to interviewers and \npromote myself, as well as formulate my interests. i sought advice from friends on this.  \nskills \nother than interview skills, i also learned a bit of the sql querying language, which is required later in my \ninternship but is a language i had yet to learn.   \nexperience \ngoing on interviews for internships is a good introduction to job interviews, so this experience will be very \nuseful during my future career.  \nrewarding experiences \nhaving companies take high interest in me made me feel very valued. this is because i am happy that i have \nsufficient knowledge to join the internship program in numerous companies. \nfinal report \ncomp8851 major project internship stream \n3 june 2022 \n \n13 \ndifficult experiences \ngoing through the internship interview process for the first time was quite anxiety-inducing at times. \nhowever, having to make the final decision on which project to choose was the most challenging experience.  \ntask and duties for upcoming week \nbefore my internship officially starts on march 7th, i have decided to start with independent learning before \nthe internship program starts. \nweek 2 \ngoals, activities, and outcomes \ngoals \nmy main goal is to learn sql independently prior to the official training period with domain. sql will be an \nimportant language to learn as i will use it to gather relevant data from domain’s data warehouse for my \nmain internship project. \ni also started my induction process before the official internship start time, so i made sure to communicate \nwith relevant people in domain to ensure induction works smoothly. \nactivities \ni followed online courses to understand the querying process with sql as well as further understand how \nrdbms (relational database management systems) work. i also communicated with my supervisor and the \ndata science team assistant regarding the induction process. \noutcomes \ni have understood the basics of sql querying, as well as how to query data and use the query results on \npython. the induction will start on monday, 7th of march, and i have received the equipment that will be \nused during induction. \nnew knowledge, skills, and experiences \nknowledge \ni have gained knowledge on the sql querying language as well as interacting with rdbms, which will be \nrequired for my project as domain uses rdbms for its databases. \nskills \nother than the technical skills i have listed as new knowledge, i learned how to communicate with people in \ndomain whilst getting used to the steps of office inductions and the administrative process behind working \nin a company.  \nexperience \nthis is my first time working in a company, so early steps throughout the induction or onboarding process as \nwell as administrative work were a completely new experience for me. \nrewarding experiences \nlearning sql was rewarding as i believe learning step-by-step will slowly boost my confidence and ensure \nthat i can progress on my project as expected. \ndifficult experiences \ni have been having trouble concentrating due to anxiety from things unrelated to the internship, and it has \nmade my progress with learning sql slower than expected. \ntask and duties for upcoming week \nthe training period will officially start next week, so i will make sure that i am fully prepared to start working \non the internship. \nfinal report \ncomp8851 major project internship stream \n3 june 2022 \n \n14 \nweek 3 \ngoals, activities, and outcomes \ngoals \nwithin the first week, the goal is to become more familiar with using sql on snowflake, which is the rdbms \nused by domain, and understand some databases that domain has in their warehouse. i was assigned a few \ntasks to query tables from multiple schemas. the tasks require varying levels of sql knowledge, from \nimplementing simpler queries to more complex, nested ones. \nactivities \ni ran a few queries to achieve each task i was assigned to. to achieve the expected results, i compared \nmultiple sql queries from different tables to see which was more accurate, then presented the results in a \nreport dashboard on mode. \noutcomes \ni produced a simple report dashboard that shows visit counts of filtered pages on domain, which includes \nsearch pages of existing properties for sale on suburbs in australia, available real estate agents in a certain \nregion, etc. \nnew knowledge, skills, and experiences \nknowledge \non top of deeper sql knowledge to run more complex queries, i also learned the basics of how search pages \nand their parameters work and a bit of web scraping on google. \nskills \nthe skills i have learned are primarily on more complex sql queries using functions such as if- and case-\nwhen-statements as well as nested select queries. \nexperience \ni got to explore some googlebot and organic page visit data on domain and gained additional insight on \nwhich types of pages (ex. which regions in australia) have higher visits than others. finally, i was introduced \nto members of the seo (search engine optimization) team in their weekly team meeting. i will work with the \nmembers of this team the most throughout my internship. \nrewarding experiences \ndespite making a mistake when filtering my sql queries, my supervisor and i found an interesting \nobservation from the incorrect queries i produced. this observation could possibly help distinguish hits from \ngooglebots and organic hits from human visitors. i was happy that we could find interesting things even \nfrom mistakes.  \ndifficult experiences \nworking remotely can be difficult as it generally reduces my concentration significantly, so i had to force \nmyself to focus on my tasks at times.  \ntask and duties for upcoming week \ncontinuing with the sql training period, i will work on a new table containing a rank of top search page \nresults on google from querying common keywords. additionally, i will help the seo team with creating \ndashboards to present their current experiment outcomes.  \n \n \n \nfinal report \ncomp8851 major project internship stream \n3 june 2022 \n \n15 \nweek 4 \ngoals, activities, and outcomes \ngoals \nthe goal of this week is to produce a dashboard that summarizes search page rankings of property agent \nprofiles. these rankings are compared between a control and treatment group. this dashboard will be used \nin the future to help a member of the seo team with their ongoing research.  \nactivities \ni queried from two separate schemas; one is a list consisting of the keywords to search for each agent, the \nother stores a ranking of search result pages on google for each keyword id. combining the two results with \nsql, i could extract the information i needed and collect both the daily average ranking for agent profiles on \na specific website, allhomes.com, that appear on google search.  \noutcomes \ni produced line charts summarizing the rankings of allhomes.com agent profiles on google search results \nover time. it consists of both average daily rankings and the percentages of the pages in top 10, 3, and 2 \nwithin google’s search page. \nnew knowledge, skills, and experiences \nknowledge \ni gained further knowledge on what google analytics produces, including visit frequencies on pages, from \nboth bots and organic visits. \nskills \ni learned more about the sql querying language as well as how to make dashboards on mode. this \ndashboard includes summaries and charts from the queries.  \nexperience \nalthough it was a simplistic and informal one, i managed to create and present a short report to some \nmembers of the seo team. i explained what i did and what results i obtained on the report. \ni will surely have to present my own findings for my own internship project, so this was good practice of \nwhat’s to come. \nrewarding experiences \nbeing able to not only learn sql more, but especially to assist someone in the seo team with their own \nresearch work, was very rewarding. it was nice to see that the dashboard i made will be used throughout her \nproject for future work. \ndifficult experiences \ncommunicating with other members in the seo team can be quite challenging due to both time conflict and \nmy own nervousness, which is something i’m trying to improve on. \ntask and duties for upcoming week \ni will begin my major project next week. the first goal i will focus on is to compare two time points of data; \none is the googlebot hits of hpg (house pricing guide) pages before the page update on may 11th, 2021, and \nthe other is the hits after the update has been implemented. this is the simplest analysis that i can do before \nadding in other factors that contribute to googlebot hit counts. \nadditionally, i will start researching to find the most suitable models for the studies in my major project. \n \n \nfinal report \ncomp8851 major project internship stream \n3 june 2022 \n \n16 \nweek 5 \ngoals, activities, and outcomes \ngoals \nthe goal of the week is to determine whether hpg (house pricing guide) pages have more daily googlebot \nhits after an update on may 11, 2021. this was done on a basic level at first, meaning that no other factors are \nconsidered other than whether the hits were recorded before or after the update. \nactivities \nthe first activity was to divide the data into two group; googlebot hits of hpg pagess before the update, \nwhich were recorded from march 22, 2021, to may 10, 2021, and the other after the update, which consists of \nhits from may 11, 2021, to june 29, 2021. i then investigated what distribution the total daily googlebot hits \nfor each group follows. i discovered that the googlebot hits were over-dispersed, meaning that it has a much \nhigher variance than the average value. as daily googlebot hits are count data, the most suitable distribution \nwas a negative binomial distribution.  \nusing hypothesis testing, i compared the means of total hits before and after the page updates to investigate \nwhether there is a significant difference between the groups.  \nadditionally, i further divided the data into subgroups based on what type of update was done to each page. \nthere are three types: no changes on the update, adding only content descriptions, and adding both images \nand content descriptions to the page. again, for each subgroup, i tested the difference of means before and \nafter the update. however, i could not compare the means between each subgroup, as the subgroups do not \nhave an equal number of samples. \noutcomes \nthe outcome of this week serves as a baseline for further investigations once other factors are considered. i \nproduced a simple report consisting of boxplots of the googlebot hits, as well histograms visualizing \ngooglebot hit frequencies and how well they fit the distribution plots. \nthe report also contains the hypothesis testing summaries and information regarding whether there was a \nsignificant difference in daily googlebot hits between hpgs prior and after the update on may 11, 2021. \nthere was no significant difference in googlebot hits before and after the hpg page updates. for each \nsubgroup, there was significant difference before and after the updating pages with both images and content, \nand strangely, with no changes done during the update. for the latter, this is likely due to the many outliers \nin said subgroup. \nnew knowledge, skills, and experiences \nknowledge \ni learned more on how to apply my current knowledge, including statistical analyses and more complex sql \nqueries, than gaining new knowledge itself. \nskills \ni was able to explore alternatives of statistical analyses that i have learned at university, and apply which \nmethods suit my data most. this is something i have never done myself; for my assignments at university, \nthere were clear hints as the data was intentionally selected to fit a certain type of method. with this, the data \nis rawer, so there was a lot more things to think of and consider. \nexperience \non top of applying the skills i’ve learned as i have mentioned above, i had more experience communicating \nmy current ideas with my supervisors and learning how to improve from their feedback properly. \nrewarding experiences \nbeing able to do the first task of the major project smoothly was rewarding. i was also happy that i could do it \nfaster than i had expected, especially considering i was feeling physically unwell and fatigued this week. \n \n \nfinal report \ncomp8851 major project internship stream \n3 june 2022 \n \n17 \ndifficult experiences \nthe fatigue from being unwell didn’t help, but i was losing motivation after my first goal was done and i felt \nthat it was harder for me to keep working on my project. sometimes, being stuck and not knowing the \nsolution can make the working more demotivating. in particular, i have difficulty aggregating the hit data \ninto daily counts whilst including zero values. this is because the database only records the daily counts \nwhenever at least one hit occurs that day. otherwise, the record for that day does not exist in the table, \ninstead of being recorded as zero hit counts. \ni also feel scared to ask my supervisor and members of the seo team sometimes, as i feel that the questions \nmay be too simplistic, even though it’s not always the case. \ntask and duties for upcoming week \nas briefly mentioned in the “activities for the week” section above, i should scale my analysis down to the \naverage daily googlebot hits for each page, so that i can compare hits between hpg page update types. this \nis done as per my supervisor’s suggestion. i have done part of it but have yet to produce significant results out \nof it. \nweek 6 \ngoals, activities, and outcomes \ngoals \nthe initial goal was to get the average daily counts for each hpg page and compare them before and after the \nupdate on may 11th, 2021. however, the results were inconclusive due to high variance of the average hit \ncoutns. \nthe goal of this week is to uncover possible ways to alleviate the current problem i have stated above; my \ncurrent analysis methods have not yielded a sensible conclusion due to high variance in the data. \nactivities \nthe first thing i have done was to follow through the initial plan and get an average daily count of hpg \n(house price guide) page hits by googlebots before and after an update on may 11th, 2021. however, the \nhypothesis test of means does not prove a significant difference between the counts before and after the \nupdate.   \nfurther investigation suggests that there is a high variance between counts – for example, while most pages \nare not visited at all on most days, other pages are visited up to 14 times. this has caused a discrepancy \nbetween values, which has thus decreased the accuracy of the hypothesis test. \nafter consulting with my supervisor, he derived a possible alternative from my current results. instead of \ncomparing the means, which assumes that googlebots visit the hpg pages at random intervals, he suggested \nthat the googlebots may have been scheduled to visit the pages at certain cycles. these peaks may have \ninfluenced the variance as in the initial analysis may not have captured the same number of cycles, for both \nbefore and after the page update. i then produced a line chart to identify the peaks, i.e., the times where the \nhit frequency is the highest, as well as determine how long as the peaks lasted and how far the gaps are \nbetween peaks. from this line chart, i also tried to identify differences in peaks before and after the hpg \npage update. \noutcomes \nthe initial analysis resulted in inconclusive results due to high variance, so the analysis needs to be done by \ntaking a different angle, as mentioned above.  \nafter taking consideration that googlebots may have a specific visit schedule, the line chart shows the \nexistence of cycles that seem to last for a week on average, with the peaks lasting for 1-2 days. however, the \npattern of these cycles is still unclear from the line chart and the length of the gaps between the peaks were \nshown to be quite random. \n \n \nfinal report \ncomp8851 major project internship stream \n3 june 2022 \n \n18 \nnew knowledge, skills, and experiences \nknowledge \ni have further understood the behavior of googlebots – this week’s results have shown that instead of visiting \nthe pages at random times, googlebots have certain cycles where it visits more pages on certain peak days. \nskills \ni have learned to apply my statistical knowledge on my project, including how to interpret and troubleshoot \nthe results. this includes identifying why a high variance has influenced the accuracy of the hypothesis tests, \nas well as why the variance exists in the first place. \nexperience \ni learned how to communicate with my supervisor and present my current problems to him, as well as \nlearning how to think outside the box from him and find a different side to my problem. \nrewarding experiences \nafter communicating with my supervisor about it, he suggested some alternatives that might solve the \ninconclusive results from my current analysis. i was very happy that i could come out of the current pitfall i \nwas in from my initial analysis thanks to my supervisor’s advice. it has also enabled me to find new and \nrelevant information that sporadic nature of the peaks. may affect the daily counts of hits. \ndifficult experiences \nafter finding the hurdle on the analysis, i had difficulty finding solutions to it and focused too much on the \nproblem, forcing myself trying to perfect my current solution instead of trying to find a better alternative.  \ntask and duties for upcoming week \nthe task for next week is to communicate with the head of the seo team about the peaks discovered. he will \nprovide further insights on the patterns that exist within these peaks, and what has caused the seemingly \nsporadic nature of the peaks. \nweek 7 \ngoals, activities, and outcomes \ngoals \nthe goal of this week was to communicate with the head of the seo team regarding the results of my findings \nthroughout my project so far. after receiving his suggestions about my current results and the workarounds i \ncan do to improve it, i am working on finalizing my results. \nactivities \nthe first activity i did was to have multiple meetings with both my supervisor and the head of the seo team. \nwe communicated about last week’s findings regarding the googlebot visit behavior and how it affects the \ndaily page hits. he described that the peaks that my supervisor and i discovered last week was likely due to \ncrawling waste that was fixed in early april 2021, meaning that i could not use the daily hit count data prior \nto the fix. however, identifying the peaks have helped us predict the visit pattern of googlebots, which will be \ndiscussed later in this section.  \nthe head of the seo team then provided me a timeline of possible other modifications of the server log \ncollection methods that may have affected the average daily page hit count. he also advised me to look at the \nhpg pages individually to identify hit frequency patterns. together with the cycles my supervisor and i \ndiscovered the previous week, i found a potential pattern of the googlebot visit rate freuqency. the reason \nwhy said visit cycles became more sporadic after the update on may 11th, 2021, primarily had to do with \ngooglebots visiting updated pages more frequently for a certain time span before the frequency rate slows \ndown to its usual rate over time. this happens because googlebot no longer detects newer updates and has \nno reason to crawl them. \nfinally, i discovered the effects of an experimental sitemap, where the hpg pages were added in mid-may or \njune 2021, towards the daily hit counts. the experimental sitemap was released to encourage googlebots to \nvisiting pages in the experimental sitemap, serving as a manual trigger. the sitemap caused googlebots to \nfinal report \ncomp8851 major project internship stream \n3 june 2022 \n \n19 \nvisit hpg pages that have not been visited in months. however, the updates seemed to be what’s causing the \nhit frequency rate to temporarily become higher. i made this observation by comparing an hpg page that \nwas updated and one that was not using graphs. \noutcomes \nfrom the meetings i did this week and the visualization graphs i created, i found that the experiment \nsitemap triggered the initial googlebot page visit on may 11th onwards, but the updates made the googlebots \nvisit certain pages more frequently. \nnew knowledge, skills, and experiences \nknowledge \nfrom my meetings with the head of the seo team, i learned a lot about the behavior of googlebots, i also \nlearned how the server log data from domain’s server was collected to obtained googlebot visit hit counts at \na deeper level. \nskills \ni learned to apply the new information i was given from the head of the seo team towards adjusting my \nproject details. \nexperience \ni managed to communicate my findings as well as possible ideas i could work on towards the head of the seo \nteam. i also tried formulating my own ideas for my project, such as about how i will analyze googlebot hit \nfrequency rates and learned to express these ideas to other people. \nrewarding experiences \nthe most rewarding experience was finding the reason to the hit count peaks my supervisor and i discovered \nthe week before. i felt satisfied by how the questions from last week could be answered by learning more \nabout it and investigating on the observation further.   \ndifficult experiences \nthe most challenging aspect of this week, as well as the internship project as a whole, is that i can only work \non a smaller dataset for my project. according to the head of the seo team, the small sample size was the \nlikely culprit for the high variance, which led to the problems that arose from the hypothesis testing. \nthe smaller dataset has forced me to continuously find new ideas with my supervisor to answer my research \nquestion.    \ntask and duties for upcoming week \ni am scheduled to have completed my main project after my mid-semester break, in three weeks. throughout \nthis time frame, i will try wrap up on my project. this includes investigating the changes in hit page \nfrequency rates and investigating the effects of removing outliers on the data if necessary. \nweek 8 \ngoals, activities, and outcomes \ngoals \nthe goal of the week is to explore ways to compare hpg page hits between page update types, other than \nusing average daily hits. i have used the distinct pages visited and the average page frequency rates. with \nthese metrics, i aimed to compare different months in 2021 to identify any differences between them. \nactivities \ni produced two bar plots that summarize the number and proportion of distinct pages visited by googlebots \nfrom may to december 2021, starting from the update on may 11th, 2021. \nas for the page hit frequency rate, i had to first identify the last time a page was visited since may 11th, 2021. \nthis was done on sql using the lag function. i then calculated the distance in days between a visit of a page \nand its most recent previous visit and stored the distance in a new column. if there was no previous visit \nfinal report \ncomp8851 major project internship stream \n3 june 2022 \n \n20 \nprior to may 11th, 2021 (i.e. the page was visited the first time since may 11th, 2021), the distance is listed as \nnull. the expected outcome is that pages visited less frequently (lower frequency rate) will have a higher \naverage distance (in days) to its last visit.  \ni then summarized the distances between a visit of a page and its previous visit as monthly averages, from \nmay 11th, 2021 to december 2021. the averages were visualized as bar plots, whereas the distributions of the \ndistances between hits were visualized with boxplots. \nto remove outliers and reduce variability, i then attempted to separate pages visited more frequently (i.e. \nwith higher frequencies by month and shorter distances) to ones visited less frequently, then making \nseparate boxplots for each. \noutcomes \nthe number and proportion of distinct pages were shown to inflate in june 2021, but decreased in july 2021 \nonwards, while going back up at the end of 2021. the initial increase is expected due to the manual trigger of \ngooglebot visits, which happened sometime in may-june 2021. however, the proportion decayed after july \n2021, and this occurred across all page update types. therefore, analyzing the page hit counts using the \nproportion of distinct pages did not show significant differences between each update type. \nas for the frequency rates, the variability between distances were too high, leading to many outliers. i \nattempted to alleviate this issue by separating pages visited less than 10 times from may 11th to december \n31st, 2021, to ones visited more than 10 times within that time. however, the variability was still too high for \nboth less and more frequently visited pages. therefore, it is difficult to draw a conclusion from the distances. \nnew knowledge, skills, and experiences \nknowledge \ni learned that sometimes, some datasets and research questions does not lead to significant differences with \nany form of visualization or statistical analysis. should i continue working with this data, i might find \npatterns that do not mean anything and give the wrong conclusion due to confirmation bias. \nskills \nalthough it was not used in the results, i tried exploring more nonparametric statistical analysis alternatives \nto anova, as the data did not follow a normal distribution. \nexperience \ni attempted to explore other angles of the dataset i am analyzing in my project to answer my research \nquestion. though in this case it still yielded inconclusive results, exploring different angles for analysis may \nhelp with future work. \nrewarding experiences \ndespite the inconclusive results, i am satisfied with the work throughout my first project. i am also grateful \nthat i was able to get help from my supervisor and the heads of the data science and seo teams. \ndifficult experiences \ni personally found it disappointing that i failed to get anything significant from the first project, but i should \nacknowledge that inconclusive results happen much more often in real-life scenarios than i’d find in \nuniversity assignments. \ntask and duties for upcoming week \ngiven that no analysis i have done yielded significant results, i will wrap up my first project and conclude \nthat the results were inconclusive due to small sample sizes and high variability in page visits. from next \nweek, i will start working on my second project, which is to create an etl process for page visits. \nadditionally, i will start writing up on my final report. \n \n \nfinal report \ncomp8851 major project internship stream \n3 june 2022 \n \n21 \nweek 9 \ngoals, activities, and outcomes \ngoals \nthe goal of the week is to wrap up my first project. i will do this by summarizing my work so far and \npresenting the summary to my supervisors, as well as start drafting my final report. \nactivities \ni first read through my previous reports to try to refresh myself with what i have done. i then categorized all \nmy work into a list of approaches i have done in chronological order. for each approach, i summarized what \ni did in dot points, whilst writing full paragraphs for some of them. \nadditionally, i have also a graph to summarize my results visually. to create some of the graphs, i had to \nclean up my code that produces the graph. i also had to fix some kinks and mistakes i previously made to \nmake the graphs look better and make more sense. \noutcomes \ni have created a short word document with the findings i have discovered within my first project. this \nincludes one graph to support each approach i have done. the document ends with a list of conclusions and \nlimitations i have encountered that prevented me from finding a conclusive result. \nnew knowledge, skills, and experiences \nknowledge \nfrom the project, i learned about the limitations of server logs in web crawling analytics from the seo team \nhead. server logs are only useful to investigate page discoverability, but it only shows a small portion of the \ngooglebot behavior. other data may be interesting to look at for future work. \nskills \ni was able to summarize my findings using my skills at both writing descriptive explanations, creating \nsupporting graphs, and making sensible conclusions. \nexperience \nthe new experience i learned this week, and overall, is to write up a report from my work on the first project. \ni learned how to write a concise report that summarizes what i have done. \nrewarding experiences \ndespite the inconclusive results, looking back, i notice that i have done a lot of things to try finding viable \nresults from my dataset. the dataset i worked on was particularly difficult due to the variability and other \nfactors that affecting the crawling, so being able to try tackling each issue was quite satisfying. \ndifficult experiences \nthe most difficult experience is to summarize all the different approaches i have done in my project with \nwords. it was difficult to make a cohesive story out of it such that the flow of the steps i have done makes \nsense. \ntask and duties for upcoming week \ndue to a change in plans, i will start working on my second etl-related project from next tuesday onwards, \ninstead of staring it this week. additionally, i will have a meeting to talk about the conclusions of my first \nproject with my supervisor and the head of the seo team. \nweek 10 \ngoals, activities, and outcomes \ngoals \nsince i started working on the second project this week, the goal primarily revolves around the new project. \nthe second project deviates a bit from what was mentioned last week. the goal is to create a python script \nfinal report \ncomp8851 major project internship stream \n3 june 2022 \n \n22 \nthat scrapes a google search result page and gather related keywords within the page’s “feature snippet”, \n“people also ask”, and “related searches” sections. these keywords are outputted given an initial keyword \nprovided by a user. \nactivities \ni was first briefed on the second project on wednesday, may 11th, 2022. this involved a meeting with my \nsupervisor and the head of the seo team. the seo team head described what specifications the seo team \nneeded for the script. the seo team originally had written a script that scrapes google search result pages of \nkeywords being fed by the user and collect relevant keywords within such pages.  \nmy task was to clean the existing script to contain more well-defined functions and to optimize it by reducing \nunnecessary and repeated commands. after the initial meeting, i had another separate discussion with my \nsupervisor to brainstorm how i should restructure the existing script to fulfill the requirements listed above.    \noutcomes \ni have cleaned and tested the original script to make it more optimized and easier to read. i also added new \nfeatures to it so that the output of keywords is saved as a csv file within a tidier directory structure. \nnew knowledge, skills, and experiences \nknowledge \ni learned about the applications of web scraping with the beautifulsoup package, in the context of search \nengine optimization. i have previously done simpler web scraping for personal projects but learning the \nversatility of its usage in more work-oriented projects was particularly interesting. \nskills \ni used new python packages that i used to clean the original web scraping script. i also reused ones that i \nhave not used in ages; the second project therefore was a good refresher for me of things i previously learned. \nthis project is also a good way for me to apply such packages further than what i was previously taught at \nuniversity. \nexperience \nthis is my first time writing a package, which is the final outcome of the second project. this involves \ntranslating what a team in a company requests for into lines of code, which is definitely valuable experience \nfor the future. \nrewarding experiences \nthe most rewarding experience was seeing the extent of what i can apply from previous knowledge at \nuniversity. even though it wasn’t the most complex script to write, especially since i mostly just expanded \nfrom an existing script, i was still able to use what i’ve learned to the script. \ndifficult experiences \nit can be difficult to write a script that will eventually be used by the company. the difficulty comes from \nmaking a script that will be practical and not break easily. additionally, i also need to learn and relearn how i \nuse relevant packages on python. \ntask and duties for upcoming week \nif there are no more things that i need to fix in the script i’ve written, which is currently more focused on \ndata collection, i will expand the script to produce simple visualized summaries on excel. this can include \nthe keyword frequency and what real estate-related topics that google users are the most interested in. \ni also have a few more meetings with my supervisor regarding further details of the data collection aspect of \nthe script. \n \n \n \nfinal report \ncomp8851 major project internship stream \n3 june 2022 \n \n23 \nweek 11 \ngoals, activities, and outcomes \ngoals \nthe goal of the week is to fix existing bugs from the script that causes the process to break. on top of that, i \nwas also expected to save the keywords collected from the script in an excel report and summarize the results \nautomatically. \nactivities \ni saved the search result pages that caused errors on the script. i troubleshooted the problems by reading \nthrough the html file and find which section caused the problem. i then fixed the issue accordingly on the \nscript. different errors occasionally occur depending on the page scraped as each search result page has a \ndifferent structure, so i tried to generalize the code such that it can work with multiple page types.    \nunfortunately, i did not manage to work on the script that does the excel report automation that much other \nthan to make a simple frequency table. this happened as i got the flu from wednesday, which was hindering \nme from working at my fullest. \noutcomes \ni have fixed all the bugs i have found so far with the current script, generalizing it such that it does not cause \nerrors the next time google updates its pages’ structures. \nnew knowledge, skills, and experiences \nknowledge \ni learned about the nature of google search results and how versatile it is, making it hard to scrape related \nkeywords in the pages without maintaining the web scraping script you have. \nskills \ni developed skills on troubleshooting scripts that use the beautifulsoup package and understanding the \nstructure of html webpages such that beautifulsoup scrapes the correct information. \nexperience \ni learned to troubleshoot scripts to generalize it such that it runs for a wider variety of data being sent to the \nscript. \nrewarding experiences \nthe most rewarding experience is being able to troubleshoot the errors caused by the script from reading \nthrough a page’s html structure, even though i had little knowledge on webpages and using beautifulsoup. \ndifficult experiences \ngoogle search page results are constantly updated to avoid third parties from crawling and scraping the site \ntoo much. this causes the script’s capacities to become unstable, so the script needs to be maintained every \nso often so that it can handle the newest updates from google. therefore, it can be hard to fix the current \nbugs such that the script doesn’t have to be updated as often later. \ntask and duties for upcoming week \nthe task for next week is to communicate with some colleagues in the seo team about what they expect in \nthe excel report summary and fulfill the requirements they need in the summary. they may request putting \nin a word cloud, queries of frequencies of specific types of questions (5w + 1h questions) in the keyword \nsuggestions, etc., in the excel report. \nweek 12 \ngoals, activities, and outcomes \ngoals \nthe goal of this week is to further expand on the excel report summarizing existing keywords in google \nfinal report \ncomp8851 major project internship stream \n3 june 2022 \n \n24 \nsearch pages. additionally, i aim to preserve the stability of the current script to prevent further breaks due \nto the unstable nature of google pages. \nactivities \nthroughout expanding the excel report for summaries, i have used python packages such as openpyxl to \nintegrate the existing script with the produced excel output. i summarized the keyword terms as frequency \ntables as well as appended word clouds for common word combinations into the excel report. \ni found more errors with the current script that is caused by the unique html structures of each search page \nresult. these errors can produce unexpected results or even break the script entirely. i attempted to \ngeneralize the fixes for these errors as much as possible such that the script can be applied in any variant in \nhtml structure for each page.   \noutcomes \nthe core part of the excel report is complete, where it has all the main summaries expected in the second \nproject’s outcomes. \nnew knowledge, skills, and experiences \nknowledge \ni discovered common related terms of a specific topic (initial keyword in the script) and learned how results \nof the script can be expanded towards different teams in domain not limited to the data and seo team. for \nexample, the sales and content-writing team will use these report summaries to write news and guides on \ndomain’s website. \nskills \ni have learned to create word clouds and integrate python script results into excel reports. \nexperience \nwhile this is not mentioned anywhere else in this weekly report, i finally met up with my supervisor and \ncolleagues at both the data and seo team this week. i was so happy to finally have that opportunity and see \nthem in person. they have been very supportive, providing both technical and life advice throughout my \ninternship, and i will cherish my connections with them for a long time. \nrewarding experiences \nthe most rewarding experience was being able to see a working product for my second project. the first \nproject, while is satisfactory in a different sense, did not exactly produce conclusive results. the second \nproject, meanwhile, is more aligned with its expected outcomes, and is a much more straightforward path \nthan the first one. \ndifficult experiences \nthe difficult experience primarily lies in juggling between my internship responsibility and my other \nuniversity responsibilities. this, admittedly, has caused me to work less on my internship in favor of my \nuniversity work. \ntask and duties for upcoming week \ndue to commitments with other units this semester and the internship wrapping up next week, i work with \ndomain less this and next week. however, if i have the time, i will communicate with the seo team \nregarding further features they might want in the excel report and pass on my current project back to my \nsupervisor. these features might require more sophisticated nlp algorithms such as topic modelling. \n \n \n",
    "page": null,
    "goal": "Reflective journal Entities",
    "children": [
        {
            "id": "1.1",
            "name": "week 1",
            "nodeType": "title",
            "text": "week 1",
            "page": null,
            "goal": "week 1",
            "children": []
        },
        {
            "id": "1.2",
            "name": "week 1 goals,",
            "nodeType": "paragraph",
            "text": "week 1 \ngoals, activities, and outcomes \ngoals \nthe goal of the first week was to secure an internship project by thursday, february 24th, 2022. another goal \nwas to write up an internship proposal that describes the project description, objectives, and estimated \ntimeline of the internship plan. \nactivities \ni contacted a few companies and did interviews with ones that contacted me back.   \nafter i secured a company to work in, i communicated with my supervisor regarding the details of the \ninternship project as well as on the internship proposal. \noutcomes \nafter a few interviews, i thankfully got a few offers from the companies that interviewed me. eventually, i \naccepted an offer with domain for a part-time internship program. \nnew knowledge, skills, and experiences \nknowledge \ni learned to properly communicate in interviews so that i can showcase my strengths to interviewers and \npromote myself, as well as formulate my interests. i sought advice from friends on this.  \nskills \nother than interview skills, i also learned a bit of the sql querying language, which is required later in my \ninternship but is a language i had yet to learn.   \nexperience \ngoing on interviews for internships is a good introduction to job interviews, so this experience will be very \nuseful during my future career.  \nrewarding experiences \nhaving companies take high interest in me made me feel very valued. this is because i am happy that i have \nsufficient knowledge to join the internship program in numerous companies. \nfinal report \ncomp8851 major project internship stream \n3 june 2022 \n \n13 \ndifficult experiences \ngoing through the internship interview process for the first time was quite anxiety-inducing at times. \nhowever, having to make the final decision on which project to choose was the most challenging experience.  \ntask and duties for upcoming week \nbefore my internship officially starts on march 7th, i have decided to start with independent learning before \nthe internship program starts.",
            "page": null,
            "goal": "week 1 \ngoals, activities, and outcomes \ngoals \nthe goal of the first week was to secure an internship project by thursday, february 24th, 2022. another goal \nwas to write up an internship proposal that describes the project description, objectives, and estimated \ntimeline of the internship plan. \nactivities \ni contacted a few companies and did interviews with ones that contacted me back.   \nafter i secured a company to work in, i communicated with my supervisor regarding the details of the \ninternship project as well as on the internship proposal. \noutcomes \nafter a few interviews, i thankfully got a few offers from the companies that interviewed me. eventually, i \naccepted an offer with domain for a part-time internship program. \nnew knowledge, skills, and experiences \nknowledge \ni learned to properly communicate in interviews so that i can showcase my strengths to interviewers and \npromote myself, as well as formulate my interests. i sought advice from friends on this.  \nskills \nother than interview skills, i also learned a bit of the sql querying language, which is required later in my \ninternship but is a language i had yet to learn.   \nexperience \ngoing on interviews for internships is a good introduction to job interviews, so this experience will be very \nuseful during my future career.  \nrewarding experiences \nhaving companies take high interest in me made me feel very valued. this is because i am happy that i have \nsufficient knowledge to join the internship program in numerous companies. \nfinal report \ncomp8851 major project internship stream \n3 june 2022 \n \n13 \ndifficult experiences \ngoing through the internship interview process for the first time was quite anxiety-inducing at times. \nhowever, having to make the final decision on which project to choose was the most challenging experience.  \ntask and duties for upcoming week \nbefore my internship officially starts on march 7th, i have decided to start with independent learning before \nthe internship program starts.",
            "children": []
        },
        {
            "id": "1.3",
            "name": "week 2",
            "nodeType": "title",
            "text": "week 2",
            "page": null,
            "goal": "week 2",
            "children": []
        },
        {
            "id": "1.4",
            "name": "week 2 goals,",
            "nodeType": "paragraph",
            "text": "week 2 \ngoals, activities, and outcomes \ngoals \nmy main goal is to learn sql independently prior to the official training period with domain. sql will be an \nimportant language to learn as i will use it to gather relevant data from domain’s data warehouse for my \nmain internship project. \ni also started my induction process before the official internship start time, so i made sure to communicate \nwith relevant people in domain to ensure induction works smoothly. \nactivities \ni followed online courses to understand the querying process with sql as well as further understand how \nrdbms (relational database management systems) work. i also communicated with my supervisor and the \ndata science team assistant regarding the induction process. \noutcomes \ni have understood the basics of sql querying, as well as how to query data and use the query results on \npython. the induction will start on monday, 7th of march, and i have received the equipment that will be \nused during induction. \nnew knowledge, skills, and experiences \nknowledge \ni have gained knowledge on the sql querying language as well as interacting with rdbms, which will be \nrequired for my project as domain uses rdbms for its databases. \nskills \nother than the technical skills i have listed as new knowledge, i learned how to communicate with people in \ndomain whilst getting used to the steps of office inductions and the administrative process behind working \nin a company.  \nexperience \nthis is my first time working in a company, so early steps throughout the induction or onboarding process as \nwell as administrative work were a completely new experience for me. \nrewarding experiences \nlearning sql was rewarding as i believe learning step-by-step will slowly boost my confidence and ensure \nthat i can progress on my project as expected. \ndifficult experiences \ni have been having trouble concentrating due to anxiety from things unrelated to the internship, and it has \nmade my progress with learning sql slower than expected. \ntask and duties for upcoming week \nthe training period will officially start next week, so i will make sure that i am fully prepared to start working \non the internship. \nfinal report \ncomp8851 major project internship stream \n3 june 2022 \n \n14",
            "page": null,
            "goal": "week 2 \ngoals, activities, and outcomes \ngoals \nmy main goal is to learn sql independently prior to the official training period with domain. sql will be an \nimportant language to learn as i will use it to gather relevant data from domain’s data warehouse for my \nmain internship project. \ni also started my induction process before the official internship start time, so i made sure to communicate \nwith relevant people in domain to ensure induction works smoothly. \nactivities \ni followed online courses to understand the querying process with sql as well as further understand how \nrdbms (relational database management systems) work. i also communicated with my supervisor and the \ndata science team assistant regarding the induction process. \noutcomes \ni have understood the basics of sql querying, as well as how to query data and use the query results on \npython. the induction will start on monday, 7th of march, and i have received the equipment that will be \nused during induction. \nnew knowledge, skills, and experiences \nknowledge \ni have gained knowledge on the sql querying language as well as interacting with rdbms, which will be \nrequired for my project as domain uses rdbms for its databases. \nskills \nother than the technical skills i have listed as new knowledge, i learned how to communicate with people in \ndomain whilst getting used to the steps of office inductions and the administrative process behind working \nin a company.  \nexperience \nthis is my first time working in a company, so early steps throughout the induction or onboarding process as \nwell as administrative work were a completely new experience for me. \nrewarding experiences \nlearning sql was rewarding as i believe learning step-by-step will slowly boost my confidence and ensure \nthat i can progress on my project as expected. \ndifficult experiences \ni have been having trouble concentrating due to anxiety from things unrelated to the internship, and it has \nmade my progress with learning sql slower than expected. \ntask and duties for upcoming week \nthe training period will officially start next week, so i will make sure that i am fully prepared to start working \non the internship. \nfinal report \ncomp8851 major project internship stream \n3 june 2022 \n \n14",
            "children": []
        },
        {
            "id": "1.5",
            "name": "week 3",
            "nodeType": "title",
            "text": "week 3",
            "page": null,
            "goal": "week 3",
            "children": []
        },
        {
            "id": "1.6",
            "name": "week 3 goals,",
            "nodeType": "paragraph",
            "text": "week 3 \ngoals, activities, and outcomes \ngoals \nwithin the first week, the goal is to become more familiar with using sql on snowflake, which is the rdbms \nused by domain, and understand some databases that domain has in their warehouse. i was assigned a few \ntasks to query tables from multiple schemas. the tasks require varying levels of sql knowledge, from \nimplementing simpler queries to more complex, nested ones. \nactivities \ni ran a few queries to achieve each task i was assigned to. to achieve the expected results, i compared \nmultiple sql queries from different tables to see which was more accurate, then presented the results in a \nreport dashboard on mode. \noutcomes \ni produced a simple report dashboard that shows visit counts of filtered pages on domain, which includes \nsearch pages of existing properties for sale on suburbs in australia, available real estate agents in a certain \nregion, etc. \nnew knowledge, skills, and experiences \nknowledge \non top of deeper sql knowledge to run more complex queries, i also learned the basics of how search pages \nand their parameters work and a bit of web scraping on google. \nskills \nthe skills i have learned are primarily on more complex sql queries using functions such as if- and case-\nwhen-statements as well as nested select queries. \nexperience \ni got to explore some googlebot and organic page visit data on domain and gained additional insight on \nwhich types of pages (ex. which regions in australia) have higher visits than others. finally, i was introduced \nto members of the seo (search engine optimization) team in their weekly team meeting. i will work with the \nmembers of this team the most throughout my internship. \nrewarding experiences \ndespite making a mistake when filtering my sql queries, my supervisor and i found an interesting \nobservation from the incorrect queries i produced. this observation could possibly help distinguish hits from \ngooglebots and organic hits from human visitors. i was happy that we could find interesting things even \nfrom mistakes.  \ndifficult experiences \nworking remotely can be difficult as it generally reduces my concentration significantly, so i had to force \nmyself to focus on my tasks at times.  \ntask and duties for upcoming week \ncontinuing with the sql training period, i will work on a new table containing a rank of top search page \nresults on google from querying common keywords. additionally, i will help the seo team with creating \ndashboards to present their current experiment outcomes.  \n \n \n \nfinal report \ncomp8851 major project internship stream \n3 june 2022 \n \n15",
            "page": null,
            "goal": "week 3 \ngoals, activities, and outcomes \ngoals \nwithin the first week, the goal is to become more familiar with using sql on snowflake, which is the rdbms \nused by domain, and understand some databases that domain has in their warehouse. i was assigned a few \ntasks to query tables from multiple schemas. the tasks require varying levels of sql knowledge, from \nimplementing simpler queries to more complex, nested ones. \nactivities \ni ran a few queries to achieve each task i was assigned to. to achieve the expected results, i compared \nmultiple sql queries from different tables to see which was more accurate, then presented the results in a \nreport dashboard on mode. \noutcomes \ni produced a simple report dashboard that shows visit counts of filtered pages on domain, which includes \nsearch pages of existing properties for sale on suburbs in australia, available real estate agents in a certain \nregion, etc. \nnew knowledge, skills, and experiences \nknowledge \non top of deeper sql knowledge to run more complex queries, i also learned the basics of how search pages \nand their parameters work and a bit of web scraping on google. \nskills \nthe skills i have learned are primarily on more complex sql queries using functions such as if- and case-\nwhen-statements as well as nested select queries. \nexperience \ni got to explore some googlebot and organic page visit data on domain and gained additional insight on \nwhich types of pages (ex. which regions in australia) have higher visits than others. finally, i was introduced \nto members of the seo (search engine optimization) team in their weekly team meeting. i will work with the \nmembers of this team the most throughout my internship. \nrewarding experiences \ndespite making a mistake when filtering my sql queries, my supervisor and i found an interesting \nobservation from the incorrect queries i produced. this observation could possibly help distinguish hits from \ngooglebots and organic hits from human visitors. i was happy that we could find interesting things even \nfrom mistakes.  \ndifficult experiences \nworking remotely can be difficult as it generally reduces my concentration significantly, so i had to force \nmyself to focus on my tasks at times.  \ntask and duties for upcoming week \ncontinuing with the sql training period, i will work on a new table containing a rank of top search page \nresults on google from querying common keywords. additionally, i will help the seo team with creating \ndashboards to present their current experiment outcomes.  \n \n \n \nfinal report \ncomp8851 major project internship stream \n3 june 2022 \n \n15",
            "children": []
        },
        {
            "id": "1.7",
            "name": "week 4",
            "nodeType": "title",
            "text": "week 4",
            "page": null,
            "goal": "week 4",
            "children": []
        },
        {
            "id": "1.8",
            "name": "week 4 goals,",
            "nodeType": "paragraph",
            "text": "week 4 \ngoals, activities, and outcomes \ngoals \nthe goal of this week is to produce a dashboard that summarizes search page rankings of property agent \nprofiles. these rankings are compared between a control and treatment group. this dashboard will be used \nin the future to help a member of the seo team with their ongoing research.  \nactivities \ni queried from two separate schemas; one is a list consisting of the keywords to search for each agent, the \nother stores a ranking of search result pages on google for each keyword id. combining the two results with \nsql, i could extract the information i needed and collect both the daily average ranking for agent profiles on \na specific website, allhomes.com, that appear on google search.  \noutcomes \ni produced line charts summarizing the rankings of allhomes.com agent profiles on google search results \nover time. it consists of both average daily rankings and the percentages of the pages in top 10, 3, and 2 \nwithin google’s search page. \nnew knowledge, skills, and experiences \nknowledge \ni gained further knowledge on what google analytics produces, including visit frequencies on pages, from \nboth bots and organic visits. \nskills \ni learned more about the sql querying language as well as how to make dashboards on mode. this \ndashboard includes summaries and charts from the queries.  \nexperience \nalthough it was a simplistic and informal one, i managed to create and present a short report to some \nmembers of the seo team. i explained what i did and what results i obtained on the report. \ni will surely have to present my own findings for my own internship project, so this was good practice of \nwhat’s to come. \nrewarding experiences \nbeing able to not only learn sql more, but especially to assist someone in the seo team with their own \nresearch work, was very rewarding. it was nice to see that the dashboard i made will be used throughout her \nproject for future work. \ndifficult experiences \ncommunicating with other members in the seo team can be quite challenging due to both time conflict and \nmy own nervousness, which is something i’m trying to improve on. \ntask and duties for upcoming week \ni will begin my major project next week. the first goal i will focus on is to compare two time points of data; \none is the googlebot hits of hpg (house pricing guide) pages before the page update on may 11th, 2021, and \nthe other is the hits after the update has been implemented. this is the simplest analysis that i can do before \nadding in other factors that contribute to googlebot hit counts. \nadditionally, i will start researching to find the most suitable models for the studies in my major project. \n \n \nfinal report \ncomp8851 major project internship stream \n3 june 2022 \n \n16",
            "page": null,
            "goal": "week 4 \ngoals, activities, and outcomes \ngoals \nthe goal of this week is to produce a dashboard that summarizes search page rankings of property agent \nprofiles. these rankings are compared between a control and treatment group. this dashboard will be used \nin the future to help a member of the seo team with their ongoing research.  \nactivities \ni queried from two separate schemas; one is a list consisting of the keywords to search for each agent, the \nother stores a ranking of search result pages on google for each keyword id. combining the two results with \nsql, i could extract the information i needed and collect both the daily average ranking for agent profiles on \na specific website, allhomes.com, that appear on google search.  \noutcomes \ni produced line charts summarizing the rankings of allhomes.com agent profiles on google search results \nover time. it consists of both average daily rankings and the percentages of the pages in top 10, 3, and 2 \nwithin google’s search page. \nnew knowledge, skills, and experiences \nknowledge \ni gained further knowledge on what google analytics produces, including visit frequencies on pages, from \nboth bots and organic visits. \nskills \ni learned more about the sql querying language as well as how to make dashboards on mode. this \ndashboard includes summaries and charts from the queries.  \nexperience \nalthough it was a simplistic and informal one, i managed to create and present a short report to some \nmembers of the seo team. i explained what i did and what results i obtained on the report. \ni will surely have to present my own findings for my own internship project, so this was good practice of \nwhat’s to come. \nrewarding experiences \nbeing able to not only learn sql more, but especially to assist someone in the seo team with their own \nresearch work, was very rewarding. it was nice to see that the dashboard i made will be used throughout her \nproject for future work. \ndifficult experiences \ncommunicating with other members in the seo team can be quite challenging due to both time conflict and \nmy own nervousness, which is something i’m trying to improve on. \ntask and duties for upcoming week \ni will begin my major project next week. the first goal i will focus on is to compare two time points of data; \none is the googlebot hits of hpg (house pricing guide) pages before the page update on may 11th, 2021, and \nthe other is the hits after the update has been implemented. this is the simplest analysis that i can do before \nadding in other factors that contribute to googlebot hit counts. \nadditionally, i will start researching to find the most suitable models for the studies in my major project. \n \n \nfinal report \ncomp8851 major project internship stream \n3 june 2022 \n \n16",
            "children": []
        },
        {
            "id": "1.9",
            "name": "week 5",
            "nodeType": "title",
            "text": "week 5",
            "page": null,
            "goal": "week 5",
            "children": []
        },
        {
            "id": "1.10",
            "name": "week 5 goals,",
            "nodeType": "paragraph",
            "text": "week 5 \ngoals, activities, and outcomes \ngoals \nthe goal of the week is to determine whether hpg (house pricing guide) pages have more daily googlebot \nhits after an update on may 11, 2021. this was done on a basic level at first, meaning that no other factors are \nconsidered other than whether the hits were recorded before or after the update. \nactivities \nthe first activity was to divide the data into two group; googlebot hits of hpg pagess before the update, \nwhich were recorded from march 22, 2021, to may 10, 2021, and the other after the update, which consists of \nhits from may 11, 2021, to june 29, 2021. i then investigated what distribution the total daily googlebot hits \nfor each group follows. i discovered that the googlebot hits were over-dispersed, meaning that it has a much \nhigher variance than the average value. as daily googlebot hits are count data, the most suitable distribution \nwas a negative binomial distribution.  \nusing hypothesis testing, i compared the means of total hits before and after the page updates to investigate \nwhether there is a significant difference between the groups.  \nadditionally, i further divided the data into subgroups based on what type of update was done to each page. \nthere are three types: no changes on the update, adding only content descriptions, and adding both images \nand content descriptions to the page. again, for each subgroup, i tested the difference of means before and \nafter the update. however, i could not compare the means between each subgroup, as the subgroups do not \nhave an equal number of samples. \noutcomes \nthe outcome of this week serves as a baseline for further investigations once other factors are considered. i \nproduced a simple report consisting of boxplots of the googlebot hits, as well histograms visualizing \ngooglebot hit frequencies and how well they fit the distribution plots. \nthe report also contains the hypothesis testing summaries and information regarding whether there was a \nsignificant difference in daily googlebot hits between hpgs prior and after the update on may 11, 2021. \nthere was no significant difference in googlebot hits before and after the hpg page updates. for each \nsubgroup, there was significant difference before and after the updating pages with both images and content, \nand strangely, with no changes done during the update. for the latter, this is likely due to the many outliers \nin said subgroup. \nnew knowledge, skills, and experiences \nknowledge \ni learned more on how to apply my current knowledge, including statistical analyses and more complex sql \nqueries, than gaining new knowledge itself. \nskills \ni was able to explore alternatives of statistical analyses that i have learned at university, and apply which \nmethods suit my data most. this is something i have never done myself; for my assignments at university, \nthere were clear hints as the data was intentionally selected to fit a certain type of method. with this, the data \nis rawer, so there was a lot more things to think of and consider. \nexperience \non top of applying the skills i’ve learned as i have mentioned above, i had more experience communicating \nmy current ideas with my supervisors and learning how to improve from their feedback properly. \nrewarding experiences \nbeing able to do the first task of the major project smoothly was rewarding. i was also happy that i could do it \nfaster than i had expected, especially considering i was feeling physically unwell and fatigued this week. \n \n \nfinal report \ncomp8851 major project internship stream \n3 june 2022 \n \n17 \ndifficult experiences \nthe fatigue from being unwell didn’t help, but i was losing motivation after my first goal was done and i felt \nthat it was harder for me to keep working on my project. sometimes, being stuck and not knowing the \nsolution can make the working more demotivating. in particular, i have difficulty aggregating the hit data \ninto daily counts whilst including zero values. this is because the database only records the daily counts \nwhenever at least one hit occurs that day. otherwise, the record for that day does not exist in the table, \ninstead of being recorded as zero hit counts. \ni also feel scared to ask my supervisor and members of the seo team sometimes, as i feel that the questions \nmay be too simplistic, even though it’s not always the case. \ntask and duties for upcoming week \nas briefly mentioned in the “activities for the week” section above, i should scale my analysis down to the \naverage daily googlebot hits for each page, so that i can compare hits between hpg page update types. this \nis done as per my supervisor’s suggestion. i have done part of it but have yet to produce significant results out \nof it.",
            "page": null,
            "goal": "week 5 \ngoals, activities, and outcomes \ngoals \nthe goal of the week is to determine whether hpg (house pricing guide) pages have more daily googlebot \nhits after an update on may 11, 2021. this was done on a basic level at first, meaning that no other factors are \nconsidered other than whether the hits were recorded before or after the update. \nactivities \nthe first activity was to divide the data into two group; googlebot hits of hpg pagess before the update, \nwhich were recorded from march 22, 2021, to may 10, 2021, and the other after the update, which consists of \nhits from may 11, 2021, to june 29, 2021. i then investigated what distribution the total daily googlebot hits \nfor each group follows. i discovered that the googlebot hits were over-dispersed, meaning that it has a much \nhigher variance than the average value. as daily googlebot hits are count data, the most suitable distribution \nwas a negative binomial distribution.  \nusing hypothesis testing, i compared the means of total hits before and after the page updates to investigate \nwhether there is a significant difference between the groups.  \nadditionally, i further divided the data into subgroups based on what type of update was done to each page. \nthere are three types: no changes on the update, adding only content descriptions, and adding both images \nand content descriptions to the page. again, for each subgroup, i tested the difference of means before and \nafter the update. however, i could not compare the means between each subgroup, as the subgroups do not \nhave an equal number of samples. \noutcomes \nthe outcome of this week serves as a baseline for further investigations once other factors are considered. i \nproduced a simple report consisting of boxplots of the googlebot hits, as well histograms visualizing \ngooglebot hit frequencies and how well they fit the distribution plots. \nthe report also contains the hypothesis testing summaries and information regarding whether there was a \nsignificant difference in daily googlebot hits between hpgs prior and after the update on may 11, 2021. \nthere was no significant difference in googlebot hits before and after the hpg page updates. for each \nsubgroup, there was significant difference before and after the updating pages with both images and content, \nand strangely, with no changes done during the update. for the latter, this is likely due to the many outliers \nin said subgroup. \nnew knowledge, skills, and experiences \nknowledge \ni learned more on how to apply my current knowledge, including statistical analyses and more complex sql \nqueries, than gaining new knowledge itself. \nskills \ni was able to explore alternatives of statistical analyses that i have learned at university, and apply which \nmethods suit my data most. this is something i have never done myself; for my assignments at university, \nthere were clear hints as the data was intentionally selected to fit a certain type of method. with this, the data \nis rawer, so there was a lot more things to think of and consider. \nexperience \non top of applying the skills i’ve learned as i have mentioned above, i had more experience communicating \nmy current ideas with my supervisors and learning how to improve from their feedback properly. \nrewarding experiences \nbeing able to do the first task of the major project smoothly was rewarding. i was also happy that i could do it \nfaster than i had expected, especially considering i was feeling physically unwell and fatigued this week. \n \n \nfinal report \ncomp8851 major project internship stream \n3 june 2022 \n \n17 \ndifficult experiences \nthe fatigue from being unwell didn’t help, but i was losing motivation after my first goal was done and i felt \nthat it was harder for me to keep working on my project. sometimes, being stuck and not knowing the \nsolution can make the working more demotivating. in particular, i have difficulty aggregating the hit data \ninto daily counts whilst including zero values. this is because the database only records the daily counts \nwhenever at least one hit occurs that day. otherwise, the record for that day does not exist in the table, \ninstead of being recorded as zero hit counts. \ni also feel scared to ask my supervisor and members of the seo team sometimes, as i feel that the questions \nmay be too simplistic, even though it’s not always the case. \ntask and duties for upcoming week \nas briefly mentioned in the “activities for the week” section above, i should scale my analysis down to the \naverage daily googlebot hits for each page, so that i can compare hits between hpg page update types. this \nis done as per my supervisor’s suggestion. i have done part of it but have yet to produce significant results out \nof it.",
            "children": []
        },
        {
            "id": "1.11",
            "name": "week 6",
            "nodeType": "title",
            "text": "week 6",
            "page": null,
            "goal": "week 6",
            "children": []
        },
        {
            "id": "1.12",
            "name": "week 6 goals,",
            "nodeType": "paragraph",
            "text": "week 6 \ngoals, activities, and outcomes \ngoals \nthe initial goal was to get the average daily counts for each hpg page and compare them before and after the \nupdate on may 11th, 2021. however, the results were inconclusive due to high variance of the average hit \ncoutns. \nthe goal of this week is to uncover possible ways to alleviate the current problem i have stated above; my \ncurrent analysis methods have not yielded a sensible conclusion due to high variance in the data. \nactivities \nthe first thing i have done was to follow through the initial plan and get an average daily count of hpg \n(house price guide) page hits by googlebots before and after an update on may 11th, 2021. however, the \nhypothesis test of means does not prove a significant difference between the counts before and after the \nupdate.   \nfurther investigation suggests that there is a high variance between counts – for example, while most pages \nare not visited at all on most days, other pages are visited up to 14 times. this has caused a discrepancy \nbetween values, which has thus decreased the accuracy of the hypothesis test. \nafter consulting with my supervisor, he derived a possible alternative from my current results. instead of \ncomparing the means, which assumes that googlebots visit the hpg pages at random intervals, he suggested \nthat the googlebots may have been scheduled to visit the pages at certain cycles. these peaks may have \ninfluenced the variance as in the initial analysis may not have captured the same number of cycles, for both \nbefore and after the page update. i then produced a line chart to identify the peaks, i.e., the times where the \nhit frequency is the highest, as well as determine how long as the peaks lasted and how far the gaps are \nbetween peaks. from this line chart, i also tried to identify differences in peaks before and after the hpg \npage update. \noutcomes \nthe initial analysis resulted in inconclusive results due to high variance, so the analysis needs to be done by \ntaking a different angle, as mentioned above.  \nafter taking consideration that googlebots may have a specific visit schedule, the line chart shows the \nexistence of cycles that seem to last for a week on average, with the peaks lasting for 1-2 days. however, the \npattern of these cycles is still unclear from the line chart and the length of the gaps between the peaks were \nshown to be quite random. \n \n \nfinal report \ncomp8851 major project internship stream \n3 june 2022 \n \n18 \nnew knowledge, skills, and experiences \nknowledge \ni have further understood the behavior of googlebots – this week’s results have shown that instead of visiting \nthe pages at random times, googlebots have certain cycles where it visits more pages on certain peak days. \nskills \ni have learned to apply my statistical knowledge on my project, including how to interpret and troubleshoot \nthe results. this includes identifying why a high variance has influenced the accuracy of the hypothesis tests, \nas well as why the variance exists in the first place. \nexperience \ni learned how to communicate with my supervisor and present my current problems to him, as well as \nlearning how to think outside the box from him and find a different side to my problem. \nrewarding experiences \nafter communicating with my supervisor about it, he suggested some alternatives that might solve the \ninconclusive results from my current analysis. i was very happy that i could come out of the current pitfall i \nwas in from my initial analysis thanks to my supervisor’s advice. it has also enabled me to find new and \nrelevant information that sporadic nature of the peaks. may affect the daily counts of hits. \ndifficult experiences \nafter finding the hurdle on the analysis, i had difficulty finding solutions to it and focused too much on the \nproblem, forcing myself trying to perfect my current solution instead of trying to find a better alternative.  \ntask and duties for upcoming week \nthe task for next week is to communicate with the head of the seo team about the peaks discovered. he will \nprovide further insights on the patterns that exist within these peaks, and what has caused the seemingly \nsporadic nature of the peaks.",
            "page": null,
            "goal": "week 6 \ngoals, activities, and outcomes \ngoals \nthe initial goal was to get the average daily counts for each hpg page and compare them before and after the \nupdate on may 11th, 2021. however, the results were inconclusive due to high variance of the average hit \ncoutns. \nthe goal of this week is to uncover possible ways to alleviate the current problem i have stated above; my \ncurrent analysis methods have not yielded a sensible conclusion due to high variance in the data. \nactivities \nthe first thing i have done was to follow through the initial plan and get an average daily count of hpg \n(house price guide) page hits by googlebots before and after an update on may 11th, 2021. however, the \nhypothesis test of means does not prove a significant difference between the counts before and after the \nupdate.   \nfurther investigation suggests that there is a high variance between counts – for example, while most pages \nare not visited at all on most days, other pages are visited up to 14 times. this has caused a discrepancy \nbetween values, which has thus decreased the accuracy of the hypothesis test. \nafter consulting with my supervisor, he derived a possible alternative from my current results. instead of \ncomparing the means, which assumes that googlebots visit the hpg pages at random intervals, he suggested \nthat the googlebots may have been scheduled to visit the pages at certain cycles. these peaks may have \ninfluenced the variance as in the initial analysis may not have captured the same number of cycles, for both \nbefore and after the page update. i then produced a line chart to identify the peaks, i.e., the times where the \nhit frequency is the highest, as well as determine how long as the peaks lasted and how far the gaps are \nbetween peaks. from this line chart, i also tried to identify differences in peaks before and after the hpg \npage update. \noutcomes \nthe initial analysis resulted in inconclusive results due to high variance, so the analysis needs to be done by \ntaking a different angle, as mentioned above.  \nafter taking consideration that googlebots may have a specific visit schedule, the line chart shows the \nexistence of cycles that seem to last for a week on average, with the peaks lasting for 1-2 days. however, the \npattern of these cycles is still unclear from the line chart and the length of the gaps between the peaks were \nshown to be quite random. \n \n \nfinal report \ncomp8851 major project internship stream \n3 june 2022 \n \n18 \nnew knowledge, skills, and experiences \nknowledge \ni have further understood the behavior of googlebots – this week’s results have shown that instead of visiting \nthe pages at random times, googlebots have certain cycles where it visits more pages on certain peak days. \nskills \ni have learned to apply my statistical knowledge on my project, including how to interpret and troubleshoot \nthe results. this includes identifying why a high variance has influenced the accuracy of the hypothesis tests, \nas well as why the variance exists in the first place. \nexperience \ni learned how to communicate with my supervisor and present my current problems to him, as well as \nlearning how to think outside the box from him and find a different side to my problem. \nrewarding experiences \nafter communicating with my supervisor about it, he suggested some alternatives that might solve the \ninconclusive results from my current analysis. i was very happy that i could come out of the current pitfall i \nwas in from my initial analysis thanks to my supervisor’s advice. it has also enabled me to find new and \nrelevant information that sporadic nature of the peaks. may affect the daily counts of hits. \ndifficult experiences \nafter finding the hurdle on the analysis, i had difficulty finding solutions to it and focused too much on the \nproblem, forcing myself trying to perfect my current solution instead of trying to find a better alternative.  \ntask and duties for upcoming week \nthe task for next week is to communicate with the head of the seo team about the peaks discovered. he will \nprovide further insights on the patterns that exist within these peaks, and what has caused the seemingly \nsporadic nature of the peaks.",
            "children": []
        },
        {
            "id": "1.13",
            "name": "week 7",
            "nodeType": "title",
            "text": "week 7",
            "page": null,
            "goal": "week 7",
            "children": []
        },
        {
            "id": "1.14",
            "name": "week 7 goals,",
            "nodeType": "paragraph",
            "text": "week 7 \ngoals, activities, and outcomes \ngoals \nthe goal of this week was to communicate with the head of the seo team regarding the results of my findings \nthroughout my project so far. after receiving his suggestions about my current results and the workarounds i \ncan do to improve it, i am working on finalizing my results. \nactivities \nthe first activity i did was to have multiple meetings with both my supervisor and the head of the seo team. \nwe communicated about last week’s findings regarding the googlebot visit behavior and how it affects the \ndaily page hits. he described that the peaks that my supervisor and i discovered last week was likely due to \ncrawling waste that was fixed in early april 2021, meaning that i could not use the daily hit count data prior \nto the fix. however, identifying the peaks have helped us predict the visit pattern of googlebots, which will be \ndiscussed later in this section.  \nthe head of the seo team then provided me a timeline of possible other modifications of the server log \ncollection methods that may have affected the average daily page hit count. he also advised me to look at the \nhpg pages individually to identify hit frequency patterns. together with the cycles my supervisor and i \ndiscovered the previous week, i found a potential pattern of the googlebot visit rate freuqency. the reason \nwhy said visit cycles became more sporadic after the update on may 11th, 2021, primarily had to do with \ngooglebots visiting updated pages more frequently for a certain time span before the frequency rate slows \ndown to its usual rate over time. this happens because googlebot no longer detects newer updates and has \nno reason to crawl them. \nfinally, i discovered the effects of an experimental sitemap, where the hpg pages were added in mid-may or \njune 2021, towards the daily hit counts. the experimental sitemap was released to encourage googlebots to \nvisiting pages in the experimental sitemap, serving as a manual trigger. the sitemap caused googlebots to \nfinal report \ncomp8851 major project internship stream \n3 june 2022 \n \n19 \nvisit hpg pages that have not been visited in months. however, the updates seemed to be what’s causing the \nhit frequency rate to temporarily become higher. i made this observation by comparing an hpg page that \nwas updated and one that was not using graphs. \noutcomes \nfrom the meetings i did this week and the visualization graphs i created, i found that the experiment \nsitemap triggered the initial googlebot page visit on may 11th onwards, but the updates made the googlebots \nvisit certain pages more frequently. \nnew knowledge, skills, and experiences \nknowledge \nfrom my meetings with the head of the seo team, i learned a lot about the behavior of googlebots, i also \nlearned how the server log data from domain’s server was collected to obtained googlebot visit hit counts at \na deeper level. \nskills \ni learned to apply the new information i was given from the head of the seo team towards adjusting my \nproject details. \nexperience \ni managed to communicate my findings as well as possible ideas i could work on towards the head of the seo \nteam. i also tried formulating my own ideas for my project, such as about how i will analyze googlebot hit \nfrequency rates and learned to express these ideas to other people. \nrewarding experiences \nthe most rewarding experience was finding the reason to the hit count peaks my supervisor and i discovered \nthe week before. i felt satisfied by how the questions from last week could be answered by learning more \nabout it and investigating on the observation further.   \ndifficult experiences \nthe most challenging aspect of this week, as well as the internship project as a whole, is that i can only work \non a smaller dataset for my project. according to the head of the seo team, the small sample size was the \nlikely culprit for the high variance, which led to the problems that arose from the hypothesis testing. \nthe smaller dataset has forced me to continuously find new ideas with my supervisor to answer my research \nquestion.    \ntask and duties for upcoming week \ni am scheduled to have completed my main project after my mid-semester break, in three weeks. throughout \nthis time frame, i will try wrap up on my project. this includes investigating the changes in hit page \nfrequency rates and investigating the effects of removing outliers on the data if necessary.",
            "page": null,
            "goal": "week 7 \ngoals, activities, and outcomes \ngoals \nthe goal of this week was to communicate with the head of the seo team regarding the results of my findings \nthroughout my project so far. after receiving his suggestions about my current results and the workarounds i \ncan do to improve it, i am working on finalizing my results. \nactivities \nthe first activity i did was to have multiple meetings with both my supervisor and the head of the seo team. \nwe communicated about last week’s findings regarding the googlebot visit behavior and how it affects the \ndaily page hits. he described that the peaks that my supervisor and i discovered last week was likely due to \ncrawling waste that was fixed in early april 2021, meaning that i could not use the daily hit count data prior \nto the fix. however, identifying the peaks have helped us predict the visit pattern of googlebots, which will be \ndiscussed later in this section.  \nthe head of the seo team then provided me a timeline of possible other modifications of the server log \ncollection methods that may have affected the average daily page hit count. he also advised me to look at the \nhpg pages individually to identify hit frequency patterns. together with the cycles my supervisor and i \ndiscovered the previous week, i found a potential pattern of the googlebot visit rate freuqency. the reason \nwhy said visit cycles became more sporadic after the update on may 11th, 2021, primarily had to do with \ngooglebots visiting updated pages more frequently for a certain time span before the frequency rate slows \ndown to its usual rate over time. this happens because googlebot no longer detects newer updates and has \nno reason to crawl them. \nfinally, i discovered the effects of an experimental sitemap, where the hpg pages were added in mid-may or \njune 2021, towards the daily hit counts. the experimental sitemap was released to encourage googlebots to \nvisiting pages in the experimental sitemap, serving as a manual trigger. the sitemap caused googlebots to \nfinal report \ncomp8851 major project internship stream \n3 june 2022 \n \n19 \nvisit hpg pages that have not been visited in months. however, the updates seemed to be what’s causing the \nhit frequency rate to temporarily become higher. i made this observation by comparing an hpg page that \nwas updated and one that was not using graphs. \noutcomes \nfrom the meetings i did this week and the visualization graphs i created, i found that the experiment \nsitemap triggered the initial googlebot page visit on may 11th onwards, but the updates made the googlebots \nvisit certain pages more frequently. \nnew knowledge, skills, and experiences \nknowledge \nfrom my meetings with the head of the seo team, i learned a lot about the behavior of googlebots, i also \nlearned how the server log data from domain’s server was collected to obtained googlebot visit hit counts at \na deeper level. \nskills \ni learned to apply the new information i was given from the head of the seo team towards adjusting my \nproject details. \nexperience \ni managed to communicate my findings as well as possible ideas i could work on towards the head of the seo \nteam. i also tried formulating my own ideas for my project, such as about how i will analyze googlebot hit \nfrequency rates and learned to express these ideas to other people. \nrewarding experiences \nthe most rewarding experience was finding the reason to the hit count peaks my supervisor and i discovered \nthe week before. i felt satisfied by how the questions from last week could be answered by learning more \nabout it and investigating on the observation further.   \ndifficult experiences \nthe most challenging aspect of this week, as well as the internship project as a whole, is that i can only work \non a smaller dataset for my project. according to the head of the seo team, the small sample size was the \nlikely culprit for the high variance, which led to the problems that arose from the hypothesis testing. \nthe smaller dataset has forced me to continuously find new ideas with my supervisor to answer my research \nquestion.    \ntask and duties for upcoming week \ni am scheduled to have completed my main project after my mid-semester break, in three weeks. throughout \nthis time frame, i will try wrap up on my project. this includes investigating the changes in hit page \nfrequency rates and investigating the effects of removing outliers on the data if necessary.",
            "children": []
        },
        {
            "id": "1.15",
            "name": "week 8",
            "nodeType": "title",
            "text": "week 8",
            "page": null,
            "goal": "week 8",
            "children": []
        },
        {
            "id": "1.16",
            "name": "week 8 goals,",
            "nodeType": "paragraph",
            "text": "week 8 \ngoals, activities, and outcomes \ngoals \nthe goal of the week is to explore ways to compare hpg page hits between page update types, other than \nusing average daily hits. i have used the distinct pages visited and the average page frequency rates. with \nthese metrics, i aimed to compare different months in 2021 to identify any differences between them. \nactivities \ni produced two bar plots that summarize the number and proportion of distinct pages visited by googlebots \nfrom may to december 2021, starting from the update on may 11th, 2021. \nas for the page hit frequency rate, i had to first identify the last time a page was visited since may 11th, 2021. \nthis was done on sql using the lag function. i then calculated the distance in days between a visit of a page \nand its most recent previous visit and stored the distance in a new column. if there was no previous visit \nfinal report \ncomp8851 major project internship stream \n3 june 2022 \n \n20 \nprior to may 11th, 2021 (i.e. the page was visited the first time since may 11th, 2021), the distance is listed as \nnull. the expected outcome is that pages visited less frequently (lower frequency rate) will have a higher \naverage distance (in days) to its last visit.  \ni then summarized the distances between a visit of a page and its previous visit as monthly averages, from \nmay 11th, 2021 to december 2021. the averages were visualized as bar plots, whereas the distributions of the \ndistances between hits were visualized with boxplots. \nto remove outliers and reduce variability, i then attempted to separate pages visited more frequently (i.e. \nwith higher frequencies by month and shorter distances) to ones visited less frequently, then making \nseparate boxplots for each. \noutcomes \nthe number and proportion of distinct pages were shown to inflate in june 2021, but decreased in july 2021 \nonwards, while going back up at the end of 2021. the initial increase is expected due to the manual trigger of \ngooglebot visits, which happened sometime in may-june 2021. however, the proportion decayed after july \n2021, and this occurred across all page update types. therefore, analyzing the page hit counts using the \nproportion of distinct pages did not show significant differences between each update type. \nas for the frequency rates, the variability between distances were too high, leading to many outliers. i \nattempted to alleviate this issue by separating pages visited less than 10 times from may 11th to december \n31st, 2021, to ones visited more than 10 times within that time. however, the variability was still too high for \nboth less and more frequently visited pages. therefore, it is difficult to draw a conclusion from the distances. \nnew knowledge, skills, and experiences \nknowledge \ni learned that sometimes, some datasets and research questions does not lead to significant differences with \nany form of visualization or statistical analysis. should i continue working with this data, i might find \npatterns that do not mean anything and give the wrong conclusion due to confirmation bias. \nskills \nalthough it was not used in the results, i tried exploring more nonparametric statistical analysis alternatives \nto anova, as the data did not follow a normal distribution. \nexperience \ni attempted to explore other angles of the dataset i am analyzing in my project to answer my research \nquestion. though in this case it still yielded inconclusive results, exploring different angles for analysis may \nhelp with future work. \nrewarding experiences \ndespite the inconclusive results, i am satisfied with the work throughout my first project. i am also grateful \nthat i was able to get help from my supervisor and the heads of the data science and seo teams. \ndifficult experiences \ni personally found it disappointing that i failed to get anything significant from the first project, but i should \nacknowledge that inconclusive results happen much more often in real-life scenarios than i’d find in \nuniversity assignments. \ntask and duties for upcoming week \ngiven that no analysis i have done yielded significant results, i will wrap up my first project and conclude \nthat the results were inconclusive due to small sample sizes and high variability in page visits. from next \nweek, i will start working on my second project, which is to create an etl process for page visits. \nadditionally, i will start writing up on my final report. \n \n \nfinal report \ncomp8851 major project internship stream \n3 june 2022 \n \n21",
            "page": null,
            "goal": "week 8 \ngoals, activities, and outcomes \ngoals \nthe goal of the week is to explore ways to compare hpg page hits between page update types, other than \nusing average daily hits. i have used the distinct pages visited and the average page frequency rates. with \nthese metrics, i aimed to compare different months in 2021 to identify any differences between them. \nactivities \ni produced two bar plots that summarize the number and proportion of distinct pages visited by googlebots \nfrom may to december 2021, starting from the update on may 11th, 2021. \nas for the page hit frequency rate, i had to first identify the last time a page was visited since may 11th, 2021. \nthis was done on sql using the lag function. i then calculated the distance in days between a visit of a page \nand its most recent previous visit and stored the distance in a new column. if there was no previous visit \nfinal report \ncomp8851 major project internship stream \n3 june 2022 \n \n20 \nprior to may 11th, 2021 (i.e. the page was visited the first time since may 11th, 2021), the distance is listed as \nnull. the expected outcome is that pages visited less frequently (lower frequency rate) will have a higher \naverage distance (in days) to its last visit.  \ni then summarized the distances between a visit of a page and its previous visit as monthly averages, from \nmay 11th, 2021 to december 2021. the averages were visualized as bar plots, whereas the distributions of the \ndistances between hits were visualized with boxplots. \nto remove outliers and reduce variability, i then attempted to separate pages visited more frequently (i.e. \nwith higher frequencies by month and shorter distances) to ones visited less frequently, then making \nseparate boxplots for each. \noutcomes \nthe number and proportion of distinct pages were shown to inflate in june 2021, but decreased in july 2021 \nonwards, while going back up at the end of 2021. the initial increase is expected due to the manual trigger of \ngooglebot visits, which happened sometime in may-june 2021. however, the proportion decayed after july \n2021, and this occurred across all page update types. therefore, analyzing the page hit counts using the \nproportion of distinct pages did not show significant differences between each update type. \nas for the frequency rates, the variability between distances were too high, leading to many outliers. i \nattempted to alleviate this issue by separating pages visited less than 10 times from may 11th to december \n31st, 2021, to ones visited more than 10 times within that time. however, the variability was still too high for \nboth less and more frequently visited pages. therefore, it is difficult to draw a conclusion from the distances. \nnew knowledge, skills, and experiences \nknowledge \ni learned that sometimes, some datasets and research questions does not lead to significant differences with \nany form of visualization or statistical analysis. should i continue working with this data, i might find \npatterns that do not mean anything and give the wrong conclusion due to confirmation bias. \nskills \nalthough it was not used in the results, i tried exploring more nonparametric statistical analysis alternatives \nto anova, as the data did not follow a normal distribution. \nexperience \ni attempted to explore other angles of the dataset i am analyzing in my project to answer my research \nquestion. though in this case it still yielded inconclusive results, exploring different angles for analysis may \nhelp with future work. \nrewarding experiences \ndespite the inconclusive results, i am satisfied with the work throughout my first project. i am also grateful \nthat i was able to get help from my supervisor and the heads of the data science and seo teams. \ndifficult experiences \ni personally found it disappointing that i failed to get anything significant from the first project, but i should \nacknowledge that inconclusive results happen much more often in real-life scenarios than i’d find in \nuniversity assignments. \ntask and duties for upcoming week \ngiven that no analysis i have done yielded significant results, i will wrap up my first project and conclude \nthat the results were inconclusive due to small sample sizes and high variability in page visits. from next \nweek, i will start working on my second project, which is to create an etl process for page visits. \nadditionally, i will start writing up on my final report. \n \n \nfinal report \ncomp8851 major project internship stream \n3 june 2022 \n \n21",
            "children": []
        },
        {
            "id": "1.17",
            "name": "week 9",
            "nodeType": "title",
            "text": "week 9",
            "page": null,
            "goal": "week 9",
            "children": []
        },
        {
            "id": "1.18",
            "name": "week 9 goals,",
            "nodeType": "paragraph",
            "text": "week 9 \ngoals, activities, and outcomes \ngoals \nthe goal of the week is to wrap up my first project. i will do this by summarizing my work so far and \npresenting the summary to my supervisors, as well as start drafting my final report. \nactivities \ni first read through my previous reports to try to refresh myself with what i have done. i then categorized all \nmy work into a list of approaches i have done in chronological order. for each approach, i summarized what \ni did in dot points, whilst writing full paragraphs for some of them. \nadditionally, i have also a graph to summarize my results visually. to create some of the graphs, i had to \nclean up my code that produces the graph. i also had to fix some kinks and mistakes i previously made to \nmake the graphs look better and make more sense. \noutcomes \ni have created a short word document with the findings i have discovered within my first project. this \nincludes one graph to support each approach i have done. the document ends with a list of conclusions and \nlimitations i have encountered that prevented me from finding a conclusive result. \nnew knowledge, skills, and experiences \nknowledge \nfrom the project, i learned about the limitations of server logs in web crawling analytics from the seo team \nhead. server logs are only useful to investigate page discoverability, but it only shows a small portion of the \ngooglebot behavior. other data may be interesting to look at for future work. \nskills \ni was able to summarize my findings using my skills at both writing descriptive explanations, creating \nsupporting graphs, and making sensible conclusions. \nexperience \nthe new experience i learned this week, and overall, is to write up a report from my work on the first project. \ni learned how to write a concise report that summarizes what i have done. \nrewarding experiences \ndespite the inconclusive results, looking back, i notice that i have done a lot of things to try finding viable \nresults from my dataset. the dataset i worked on was particularly difficult due to the variability and other \nfactors that affecting the crawling, so being able to try tackling each issue was quite satisfying. \ndifficult experiences \nthe most difficult experience is to summarize all the different approaches i have done in my project with \nwords. it was difficult to make a cohesive story out of it such that the flow of the steps i have done makes \nsense. \ntask and duties for upcoming week \ndue to a change in plans, i will start working on my second etl-related project from next tuesday onwards, \ninstead of staring it this week. additionally, i will have a meeting to talk about the conclusions of my first \nproject with my supervisor and the head of the seo team.",
            "page": null,
            "goal": "week 9 \ngoals, activities, and outcomes \ngoals \nthe goal of the week is to wrap up my first project. i will do this by summarizing my work so far and \npresenting the summary to my supervisors, as well as start drafting my final report. \nactivities \ni first read through my previous reports to try to refresh myself with what i have done. i then categorized all \nmy work into a list of approaches i have done in chronological order. for each approach, i summarized what \ni did in dot points, whilst writing full paragraphs for some of them. \nadditionally, i have also a graph to summarize my results visually. to create some of the graphs, i had to \nclean up my code that produces the graph. i also had to fix some kinks and mistakes i previously made to \nmake the graphs look better and make more sense. \noutcomes \ni have created a short word document with the findings i have discovered within my first project. this \nincludes one graph to support each approach i have done. the document ends with a list of conclusions and \nlimitations i have encountered that prevented me from finding a conclusive result. \nnew knowledge, skills, and experiences \nknowledge \nfrom the project, i learned about the limitations of server logs in web crawling analytics from the seo team \nhead. server logs are only useful to investigate page discoverability, but it only shows a small portion of the \ngooglebot behavior. other data may be interesting to look at for future work. \nskills \ni was able to summarize my findings using my skills at both writing descriptive explanations, creating \nsupporting graphs, and making sensible conclusions. \nexperience \nthe new experience i learned this week, and overall, is to write up a report from my work on the first project. \ni learned how to write a concise report that summarizes what i have done. \nrewarding experiences \ndespite the inconclusive results, looking back, i notice that i have done a lot of things to try finding viable \nresults from my dataset. the dataset i worked on was particularly difficult due to the variability and other \nfactors that affecting the crawling, so being able to try tackling each issue was quite satisfying. \ndifficult experiences \nthe most difficult experience is to summarize all the different approaches i have done in my project with \nwords. it was difficult to make a cohesive story out of it such that the flow of the steps i have done makes \nsense. \ntask and duties for upcoming week \ndue to a change in plans, i will start working on my second etl-related project from next tuesday onwards, \ninstead of staring it this week. additionally, i will have a meeting to talk about the conclusions of my first \nproject with my supervisor and the head of the seo team.",
            "children": []
        },
        {
            "id": "1.19",
            "name": "week 10",
            "nodeType": "title",
            "text": "week 10",
            "page": null,
            "goal": "week 10",
            "children": []
        },
        {
            "id": "1.20",
            "name": "week 10 goals,",
            "nodeType": "paragraph",
            "text": "week 10 \ngoals, activities, and outcomes \ngoals \nsince i started working on the second project this week, the goal primarily revolves around the new project. \nthe second project deviates a bit from what was mentioned last week. the goal is to create a python script \nfinal report \ncomp8851 major project internship stream \n3 june 2022 \n \n22 \nthat scrapes a google search result page and gather related keywords within the page’s “feature snippet”, \n“people also ask”, and “related searches” sections. these keywords are outputted given an initial keyword \nprovided by a user. \nactivities \ni was first briefed on the second project on wednesday, may 11th, 2022. this involved a meeting with my \nsupervisor and the head of the seo team. the seo team head described what specifications the seo team \nneeded for the script. the seo team originally had written a script that scrapes google search result pages of \nkeywords being fed by the user and collect relevant keywords within such pages.  \nmy task was to clean the existing script to contain more well-defined functions and to optimize it by reducing \nunnecessary and repeated commands. after the initial meeting, i had another separate discussion with my \nsupervisor to brainstorm how i should restructure the existing script to fulfill the requirements listed above.    \noutcomes \ni have cleaned and tested the original script to make it more optimized and easier to read. i also added new \nfeatures to it so that the output of keywords is saved as a csv file within a tidier directory structure. \nnew knowledge, skills, and experiences \nknowledge \ni learned about the applications of web scraping with the beautifulsoup package, in the context of search \nengine optimization. i have previously done simpler web scraping for personal projects but learning the \nversatility of its usage in more work-oriented projects was particularly interesting. \nskills \ni used new python packages that i used to clean the original web scraping script. i also reused ones that i \nhave not used in ages; the second project therefore was a good refresher for me of things i previously learned. \nthis project is also a good way for me to apply such packages further than what i was previously taught at \nuniversity. \nexperience \nthis is my first time writing a package, which is the final outcome of the second project. this involves \ntranslating what a team in a company requests for into lines of code, which is definitely valuable experience \nfor the future. \nrewarding experiences \nthe most rewarding experience was seeing the extent of what i can apply from previous knowledge at \nuniversity. even though it wasn’t the most complex script to write, especially since i mostly just expanded \nfrom an existing script, i was still able to use what i’ve learned to the script. \ndifficult experiences \nit can be difficult to write a script that will eventually be used by the company. the difficulty comes from \nmaking a script that will be practical and not break easily. additionally, i also need to learn and relearn how i \nuse relevant packages on python. \ntask and duties for upcoming week \nif there are no more things that i need to fix in the script i’ve written, which is currently more focused on \ndata collection, i will expand the script to produce simple visualized summaries on excel. this can include \nthe keyword frequency and what real estate-related topics that google users are the most interested in. \ni also have a few more meetings with my supervisor regarding further details of the data collection aspect of \nthe script. \n \n \n \nfinal report \ncomp8851 major project internship stream \n3 june 2022 \n \n23",
            "page": null,
            "goal": "week 10 \ngoals, activities, and outcomes \ngoals \nsince i started working on the second project this week, the goal primarily revolves around the new project. \nthe second project deviates a bit from what was mentioned last week. the goal is to create a python script \nfinal report \ncomp8851 major project internship stream \n3 june 2022 \n \n22 \nthat scrapes a google search result page and gather related keywords within the page’s “feature snippet”, \n“people also ask”, and “related searches” sections. these keywords are outputted given an initial keyword \nprovided by a user. \nactivities \ni was first briefed on the second project on wednesday, may 11th, 2022. this involved a meeting with my \nsupervisor and the head of the seo team. the seo team head described what specifications the seo team \nneeded for the script. the seo team originally had written a script that scrapes google search result pages of \nkeywords being fed by the user and collect relevant keywords within such pages.  \nmy task was to clean the existing script to contain more well-defined functions and to optimize it by reducing \nunnecessary and repeated commands. after the initial meeting, i had another separate discussion with my \nsupervisor to brainstorm how i should restructure the existing script to fulfill the requirements listed above.    \noutcomes \ni have cleaned and tested the original script to make it more optimized and easier to read. i also added new \nfeatures to it so that the output of keywords is saved as a csv file within a tidier directory structure. \nnew knowledge, skills, and experiences \nknowledge \ni learned about the applications of web scraping with the beautifulsoup package, in the context of search \nengine optimization. i have previously done simpler web scraping for personal projects but learning the \nversatility of its usage in more work-oriented projects was particularly interesting. \nskills \ni used new python packages that i used to clean the original web scraping script. i also reused ones that i \nhave not used in ages; the second project therefore was a good refresher for me of things i previously learned. \nthis project is also a good way for me to apply such packages further than what i was previously taught at \nuniversity. \nexperience \nthis is my first time writing a package, which is the final outcome of the second project. this involves \ntranslating what a team in a company requests for into lines of code, which is definitely valuable experience \nfor the future. \nrewarding experiences \nthe most rewarding experience was seeing the extent of what i can apply from previous knowledge at \nuniversity. even though it wasn’t the most complex script to write, especially since i mostly just expanded \nfrom an existing script, i was still able to use what i’ve learned to the script. \ndifficult experiences \nit can be difficult to write a script that will eventually be used by the company. the difficulty comes from \nmaking a script that will be practical and not break easily. additionally, i also need to learn and relearn how i \nuse relevant packages on python. \ntask and duties for upcoming week \nif there are no more things that i need to fix in the script i’ve written, which is currently more focused on \ndata collection, i will expand the script to produce simple visualized summaries on excel. this can include \nthe keyword frequency and what real estate-related topics that google users are the most interested in. \ni also have a few more meetings with my supervisor regarding further details of the data collection aspect of \nthe script. \n \n \n \nfinal report \ncomp8851 major project internship stream \n3 june 2022 \n \n23",
            "children": []
        },
        {
            "id": "1.21",
            "name": "week 11",
            "nodeType": "title",
            "text": "week 11",
            "page": null,
            "goal": "week 11",
            "children": []
        },
        {
            "id": "1.22",
            "name": "week 11 goals,",
            "nodeType": "paragraph",
            "text": "week 11 \ngoals, activities, and outcomes \ngoals \nthe goal of the week is to fix existing bugs from the script that causes the process to break. on top of that, i \nwas also expected to save the keywords collected from the script in an excel report and summarize the results \nautomatically. \nactivities \ni saved the search result pages that caused errors on the script. i troubleshooted the problems by reading \nthrough the html file and find which section caused the problem. i then fixed the issue accordingly on the \nscript. different errors occasionally occur depending on the page scraped as each search result page has a \ndifferent structure, so i tried to generalize the code such that it can work with multiple page types.    \nunfortunately, i did not manage to work on the script that does the excel report automation that much other \nthan to make a simple frequency table. this happened as i got the flu from wednesday, which was hindering \nme from working at my fullest. \noutcomes \ni have fixed all the bugs i have found so far with the current script, generalizing it such that it does not cause \nerrors the next time google updates its pages’ structures. \nnew knowledge, skills, and experiences \nknowledge \ni learned about the nature of google search results and how versatile it is, making it hard to scrape related \nkeywords in the pages without maintaining the web scraping script you have. \nskills \ni developed skills on troubleshooting scripts that use the beautifulsoup package and understanding the \nstructure of html webpages such that beautifulsoup scrapes the correct information. \nexperience \ni learned to troubleshoot scripts to generalize it such that it runs for a wider variety of data being sent to the \nscript. \nrewarding experiences \nthe most rewarding experience is being able to troubleshoot the errors caused by the script from reading \nthrough a page’s html structure, even though i had little knowledge on webpages and using beautifulsoup. \ndifficult experiences \ngoogle search page results are constantly updated to avoid third parties from crawling and scraping the site \ntoo much. this causes the script’s capacities to become unstable, so the script needs to be maintained every \nso often so that it can handle the newest updates from google. therefore, it can be hard to fix the current \nbugs such that the script doesn’t have to be updated as often later. \ntask and duties for upcoming week \nthe task for next week is to communicate with some colleagues in the seo team about what they expect in \nthe excel report summary and fulfill the requirements they need in the summary. they may request putting \nin a word cloud, queries of frequencies of specific types of questions (5w + 1h questions) in the keyword \nsuggestions, etc., in the excel report.",
            "page": null,
            "goal": "week 11 \ngoals, activities, and outcomes \ngoals \nthe goal of the week is to fix existing bugs from the script that causes the process to break. on top of that, i \nwas also expected to save the keywords collected from the script in an excel report and summarize the results \nautomatically. \nactivities \ni saved the search result pages that caused errors on the script. i troubleshooted the problems by reading \nthrough the html file and find which section caused the problem. i then fixed the issue accordingly on the \nscript. different errors occasionally occur depending on the page scraped as each search result page has a \ndifferent structure, so i tried to generalize the code such that it can work with multiple page types.    \nunfortunately, i did not manage to work on the script that does the excel report automation that much other \nthan to make a simple frequency table. this happened as i got the flu from wednesday, which was hindering \nme from working at my fullest. \noutcomes \ni have fixed all the bugs i have found so far with the current script, generalizing it such that it does not cause \nerrors the next time google updates its pages’ structures. \nnew knowledge, skills, and experiences \nknowledge \ni learned about the nature of google search results and how versatile it is, making it hard to scrape related \nkeywords in the pages without maintaining the web scraping script you have. \nskills \ni developed skills on troubleshooting scripts that use the beautifulsoup package and understanding the \nstructure of html webpages such that beautifulsoup scrapes the correct information. \nexperience \ni learned to troubleshoot scripts to generalize it such that it runs for a wider variety of data being sent to the \nscript. \nrewarding experiences \nthe most rewarding experience is being able to troubleshoot the errors caused by the script from reading \nthrough a page’s html structure, even though i had little knowledge on webpages and using beautifulsoup. \ndifficult experiences \ngoogle search page results are constantly updated to avoid third parties from crawling and scraping the site \ntoo much. this causes the script’s capacities to become unstable, so the script needs to be maintained every \nso often so that it can handle the newest updates from google. therefore, it can be hard to fix the current \nbugs such that the script doesn’t have to be updated as often later. \ntask and duties for upcoming week \nthe task for next week is to communicate with some colleagues in the seo team about what they expect in \nthe excel report summary and fulfill the requirements they need in the summary. they may request putting \nin a word cloud, queries of frequencies of specific types of questions (5w + 1h questions) in the keyword \nsuggestions, etc., in the excel report.",
            "children": []
        },
        {
            "id": "1.23",
            "name": "week 12",
            "nodeType": "title",
            "text": "week 12",
            "page": null,
            "goal": "week 12",
            "children": []
        },
        {
            "id": "1.24",
            "name": "week 12 goals,",
            "nodeType": "paragraph",
            "text": "week 12 \ngoals, activities, and outcomes \ngoals \nthe goal of this week is to further expand on the excel report summarizing existing keywords in google \nfinal report \ncomp8851 major project internship stream \n3 june 2022 \n \n24 \nsearch pages. additionally, i aim to preserve the stability of the current script to prevent further breaks due \nto the unstable nature of google pages. \nactivities \nthroughout expanding the excel report for summaries, i have used python packages such as openpyxl to \nintegrate the existing script with the produced excel output. i summarized the keyword terms as frequency \ntables as well as appended word clouds for common word combinations into the excel report. \ni found more errors with the current script that is caused by the unique html structures of each search page \nresult. these errors can produce unexpected results or even break the script entirely. i attempted to \ngeneralize the fixes for these errors as much as possible such that the script can be applied in any variant in \nhtml structure for each page.   \noutcomes \nthe core part of the excel report is complete, where it has all the main summaries expected in the second \nproject’s outcomes. \nnew knowledge, skills, and experiences \nknowledge \ni discovered common related terms of a specific topic (initial keyword in the script) and learned how results \nof the script can be expanded towards different teams in domain not limited to the data and seo team. for \nexample, the sales and content-writing team will use these report summaries to write news and guides on \ndomain’s website. \nskills \ni have learned to create word clouds and integrate python script results into excel reports. \nexperience \nwhile this is not mentioned anywhere else in this weekly report, i finally met up with my supervisor and \ncolleagues at both the data and seo team this week. i was so happy to finally have that opportunity and see \nthem in person. they have been very supportive, providing both technical and life advice throughout my \ninternship, and i will cherish my connections with them for a long time. \nrewarding experiences \nthe most rewarding experience was being able to see a working product for my second project. the first \nproject, while is satisfactory in a different sense, did not exactly produce conclusive results. the second \nproject, meanwhile, is more aligned with its expected outcomes, and is a much more straightforward path \nthan the first one. \ndifficult experiences \nthe difficult experience primarily lies in juggling between my internship responsibility and my other \nuniversity responsibilities. this, admittedly, has caused me to work less on my internship in favor of my \nuniversity work. \ntask and duties for upcoming week \ndue to commitments with other units this semester and the internship wrapping up next week, i work with \ndomain less this and next week. however, if i have the time, i will communicate with the seo team \nregarding further features they might want in the excel report and pass on my current project back to my \nsupervisor. these features might require more sophisticated nlp algorithms such as topic modelling.",
            "page": null,
            "goal": "week 12 \ngoals, activities, and outcomes \ngoals \nthe goal of this week is to further expand on the excel report summarizing existing keywords in google \nfinal report \ncomp8851 major project internship stream \n3 june 2022 \n \n24 \nsearch pages. additionally, i aim to preserve the stability of the current script to prevent further breaks due \nto the unstable nature of google pages. \nactivities \nthroughout expanding the excel report for summaries, i have used python packages such as openpyxl to \nintegrate the existing script with the produced excel output. i summarized the keyword terms as frequency \ntables as well as appended word clouds for common word combinations into the excel report. \ni found more errors with the current script that is caused by the unique html structures of each search page \nresult. these errors can produce unexpected results or even break the script entirely. i attempted to \ngeneralize the fixes for these errors as much as possible such that the script can be applied in any variant in \nhtml structure for each page.   \noutcomes \nthe core part of the excel report is complete, where it has all the main summaries expected in the second \nproject’s outcomes. \nnew knowledge, skills, and experiences \nknowledge \ni discovered common related terms of a specific topic (initial keyword in the script) and learned how results \nof the script can be expanded towards different teams in domain not limited to the data and seo team. for \nexample, the sales and content-writing team will use these report summaries to write news and guides on \ndomain’s website. \nskills \ni have learned to create word clouds and integrate python script results into excel reports. \nexperience \nwhile this is not mentioned anywhere else in this weekly report, i finally met up with my supervisor and \ncolleagues at both the data and seo team this week. i was so happy to finally have that opportunity and see \nthem in person. they have been very supportive, providing both technical and life advice throughout my \ninternship, and i will cherish my connections with them for a long time. \nrewarding experiences \nthe most rewarding experience was being able to see a working product for my second project. the first \nproject, while is satisfactory in a different sense, did not exactly produce conclusive results. the second \nproject, meanwhile, is more aligned with its expected outcomes, and is a much more straightforward path \nthan the first one. \ndifficult experiences \nthe difficult experience primarily lies in juggling between my internship responsibility and my other \nuniversity responsibilities. this, admittedly, has caused me to work less on my internship in favor of my \nuniversity work. \ntask and duties for upcoming week \ndue to commitments with other units this semester and the internship wrapping up next week, i work with \ndomain less this and next week. however, if i have the time, i will communicate with the seo team \nregarding further features they might want in the excel report and pass on my current project back to my \nsupervisor. these features might require more sophisticated nlp algorithms such as topic modelling.",
            "children": []
        }
    ]
}