{
    "id": "1",
    "name": "Reflective journal Entities",
    "nodeType": "section",
    "text": "week 1:  \nthe goals of the first week are to get started with onboarding and get to know about the task and \ndeadlines. the activities of the first week involve attending a kick-off workshop hosted by the company \nceo. mike simpson the ceo & co-founder introduced the machine learning teams and explained the \nworkflow inside the organization. got assigned a list of tasks to perform during the internship period, and \nwas able to ask and clarify the point of contact and more about the assigned task in person. \n \nfigure 3 roadmap for the company to achieve by june \nthe organisation introduced 6 streams where all the selected interns will be working this includes \nprocesses like data collection, capture, classification of data, authentication, extraction, and quality \nassurance and reporting. design, building, improving text format check, and designing, building, and \nimproving unique security feature check were the two important tasks assigned to me during the meeting. \nthese were part of the id document authenticity check. \nthe task design, building, and improving text format check includes the following functions to perform \n• \ngather text format metadata for all documents, define the logic of matching data against the \ndefined metadata. \nweek 2: \ngoals of week 2 involves getting access to most of the logins, which the company uses, like access to \nemails, teams, etc., and planning the 1:1 meeting. activities involve getting in touch with the security \nteams for access. outcomes: received the credentials for the work platforms and got in touch with the \nsupervisor and gave the internship availability dates every tuesday, wednesday, and thursday from 9:00 \nam to 6:00 pm, and scheduled weekly meetings for reporting with the supervisor.  \nevery thursday 30 minutes meeting is arranged between the technical product owner(sai kiran ravula) \nand the ceo (mike simpson) to discuss the progress made in the particular week and to discuss the \nroadmaps for the upcoming week including giving suggestions and comments on the performance done \nso far.  \nweek 2 the work mostly deals with researching the non-id document extraction. week 2 mostly worked \non finding different text patterns and finding how to extract the data from the text or metadata it has. \nhas to go through numerous real-time projects and research papers and implementations to find out the \ndifferent ways to extract data from the non-id particularly since nothing has a unique structure. used \ngoogle scholar for the research papers. \nresearching part was rewarding, never found it easy but the process was insightful about the latest or \npossible implementation of the task. this also helped me to think and process differently than the \ntraditional approach. as part of the onboarding did sign off on policy and standards or truuth organisation, \ndid setup jira confluence for the documentation part, followed the company guidelines to encrypt the \nsystem, and also went through security awareness training modules as part of locii information security \nmanagement system  security training & iso27k awareness training for interns 2020 - 2021 - locii isms.  \nthe security awareness module mostly deals with how to secure the data inside the organization and how \nto deal with vulnerability and threats and a detailed introduction about the cyber attacks through spear \nphishing emails, baiting, scareware, and pretexting. a few of the take away from the four module training \nare listed below. \n• \ncyber security in the workplace is everyone’s responsibility staffs should ensure that \n• \ndo not access locii’s  data from public wi-fi internet  \n• \nproper security measures are in place when handling confidential and high confidential data \n• \nwhen working in a shared workspace, privacy filters and screen locks are on for the endpoint \ndevices used. \n• \nare in a protected closed environment when discussing confidential ideas in a shared working \nspace   \n• \nthe url links are legitimate and not malicious before clicking it  \n• \nantivirus and patching are always on in their locii devices and byod \n• \n \nat some point i felt communicating with the team was difficult, especially when no response to the mail \nsent regarding the task to proceed next. the access set up with the security consultant was smooth. \ngetting to know more about what you do and trying to implement is a good way to handle the tough \nsituation, i spent extra hours after work researching the topic and going through the multiple research \npaper. maybe frequent follow-ups and guidelines to proceed further, i find it difficult to understand how \nto proceed further. \n \nweek 3: \nthe goals of week 3 involve checking the authenticity of the documents. here is the brief about it (an \nauthenticity check is needed to determine whether the document submitted by the user is an original \ndocument or a b&w photocopy, particularly in situations where authenticity is critical such as legal or \nfinancial transactions or verifying government-issued identification documents.) \nas part of this process, it's important to have a clear idea of the data before processing with and checking \nauthenticity using python script. on weekly calls, ceo mentioned he wanted the geographical \ndistribution of the data the organization has in a visualization format. the process involved fetching the \ndocument and passing the documents through python open cv api created in azure. then analysis the \ngeographical diversity of the documents. \n \nbelow is the step-by-step method followed to achieve the same. \n \n• \nthe dataset from the two data sources is carefully screened and merged into one source with 469 \ntotal passports. \n• \nwrote a python script to fetch the api and gather the score & country code of the passport. the \nreference sample is attached below. \n• \nthis process ran through all the individual passports in the dataset and got this output.csv file. \n \n \nfigure 4 output format received in the csv \n• \nran the visualization code for the bar plot, and geo plot to understand the distribution of the \npassport dataset. \ndata source \nfor the analysis of the geography distribution of the passport dataset, the organization initially collected \nthe dataset from the id bible, where manually downloaded the file from the a-z countries, and obtained \n269 passports. another dataset has been provided with a collection of 200 different passports. later \ncombined these two sets of data sources into one to proceed further. \n \n \ntotal dataset of passport: 469 \ntable 1 table showing the dataset which visualization performed \ndataset \ncount \nid bible \n269 \ndataset #1 \n200 \ntotal \n469 \nanalysis \nthe entire process is finished on 469 dataset images and received an output of 431. the remaining 38 \npassport file couldn't fetch the result either of the apis not able to fetch the data. most of the \npassports on the dataset are from australia(133), new zealand (12), the uk (8), philippines(7). most of \nthe diverse passports are covered as part of the dataset but are limited to 1 or <6. \n \nfigure 5: geo plot representing the wide coverage of passport \n \n \ngetting to know more about the data and being clear about the task i’m supposed to do helped to deal \nbetter with the challenge.  reaching out to the supervisor after not being able to get the solution and \nlooking for help better. improved skills in python and visualization, received proper guidance on extracting \nthe text from the document from the supervisor assigned. followed a proper communication channel for \nthe same.  \n \nweek 4: \nthe goals of the week are to implement a visual feature check on the document database and generate \na matching score and generate a hypothesis on the mrz positive and negative testing + worked on the \nclassification errors apcer and bpcer. activities involve writing python scripts to check the visual feature \nof sample documents and user-submitted documents and generate a matching score.  \n \nbpcer (bona fide presentation classification error rate), is the percentage ratio at which bona fide \nexamples are misidentified as presentation attack examples. a higher value indicates higher user friction.  \napcer attack presentation classification error rate, the percentage ratio at which presentation attack \nexamples are misidentified as a bona fide example. a higher value indicates higher security vulnerability.) \n \nrun the testing on both positive and negative by generating negative samples, the negative sample is \ncreated by forging the authentic file like changing the value in the mrz field. negative testing has been \nperformed on the 200 authentic datasets from the org. a sample of 70 document images has been taken \nand changed the data based on the date of expiry, passport number, and date of issue.  \n \n \ntable 2 table showing how the document has been falsified for neg testing \nmrz dataset changes \ncount \nchange in passport no \n16 \nchange in dob \n24 \nchange in doe \n30 \ntotal \n70 \n \n \noutcomes: negative testing has been performed on the 200 authentic datasets from the org. a sample of \n70 document images has been taken and changed the data based on the date of expiry, passport number, \nand date of issue. the output of the above run has been attached below, where 66/70 of the false \ndocument api run was able to recognize its fake document.  \n \n \nfigure 6 further analysis of failed documents to understand why they failed. \nfor the positive testing considered the 200 authentic passports dataset and ran the script on the same. \nthe result obtained is 30/200 files turned out to be not authentic. 19/200 files turned to be mrz score of \n0 instead of 1 and the remaining 11 files didn't produce any score. few of the files produced blank results \nthis has been analyzed in the below approach. \n \nthen the final stage is to classify errors and get the error rate for apcer and bpcer. the error rate for \nbpcer: is 15.00%, and the error rate for apcer: is 5.71% \n \n \nfigure 7: figure showing bpcer error rates \n \nfigure 8: figure showing apcer error rate \n \nfrom the activity from this week got to know why sample set analysis is better than processing the entire \ndataset in one go. learned to use a python script to call the azure api. learned about the data complexity \nand chances of getting biased based on the data. a summary of this week's activity is listed below. \n• \nwrote a python script to fetch the api and gather the visual features of the document and \ngenerate a matching score. \n• \nwrote python script to redirect the ocr extracted text in the form of json code. \n• \nfew documents didn’t have any output, getting to a hypothesis and detailed analysis of that \nparticular process is a rewarding experience. this way gather knowledge to analyze the data \ndifferently.  \n• \nthe coding part was a bit challenging as never worked with real-time fetching of api and doing \nthe visual analysis of the document. \na few things noticed is organizationrequirement keeps changing sometimes, having a platform like an \nexcel report sheet to track the roadmap and timeline would be easy to understand the progress and track \nthe productivity as well. \nweek 5: \nevery identification document contains distinct and unchanging features or objects that are consistent in \nsize and location within the document. these objects are utilized in determining the document's \nclassification. this week the goal is to build a visual feature check. this can achieve by specifying the \nprecise location of each object, and by examining the location of each object in a sample image, we \ndetermine whether the document is correctly positioned or not and find the matching percentage of the \ntemplate. \n \nactivities include checking the validity of the documents with a respective template which also provides \nthe matching percentage. the objectives have been categorized as the priority below. categorize the \nsample into respective document types folder. a sample of max 20 or less as per the requirement. run \nthe python program to find the respective match per the folders. analyze the match percentage and \nfurther investigate failed documents which will help with the following. \n \n• \nto see the positive and negative score distributions for each classification code. \n• \nto compare the results of the 3 different curation approaches. \n• \nto test the models on unseen positive and negative samples and calculate the far and frr. \n• \nto store the logistic regression parameters.  \n \nknowledge:  \nthe visual feature check is a sub-check within our authenticity api version 2 that tests whether the client-\nsubmitted id document image has similar visual features to the authentic template we have saved for \nthat classification code. visual characteristics like banners, logos, emblems, and any special symbols \npresent on the document are considered visual features. \n \n• \n learned about the data complexity and chances of getting biased based on the data.  \n• \nwrote a python script to fetch the api and gather the visual features of the document and \ngenerate a matching score. \n• \nwrote python script to get a matching percentage and further used logistic regression model for \nthe outputs. \n• \nthe data curation part was a tricky scenario: \n• \nthis is the scenario where we don’t have sufficient positive samples to perform the data gathering. \n• \nfor example, we don’t have 200 documents of the nsw_full_dl to run the positive analysis. in \nthis case, we would like to do the following data curation approach: \n \nthe coding part was a bit challenging as never worked with real-time fetching of api and doing the visual \nanalysis of the document. \n \nnote: this week has taken leave from work on thursday due to the back pain issue i faced, and my \nproductivity was less compared to the other weeks so far due to the sick. the organization's work culture \nis amazing without a prior email communication regarding the sick leave they approved the leave and \nmade sure i’m taking rest by giving off to the task assigned. most of the document tasks have been \ncovered this week comparing to the coding. \n \n \n \nweek 6: \nthe task of week 6 involves doing the pending task from the last week of visual feature check and the \nbelow ones. \n• \ncollect quality samples for a wide range of global identity documents and create a template for \neach document class. our aim for the poc is to extend the visual feature check to all identity \ndocuments that have been templated (au + nz dls). a stretch goal is to extend the poc to global \npassports. \n• \ncategorize the samples into respective document types folders. a sample of max 50 or less as per \nthe requirement. \n• \nuse sift to calculate the number of matching features for each sample image vs the chosen \ntemplate for the document class \n \nactivities include running the generic testing. for a positive set, actual documents run against the actual \ntemplates, and for negative (generic negative documents) documents which are not australian \ndocuments like debit cards, debit cards. here selecting max of 200 files for each category. image enhance \nwill be run before proceeding with every test for the same and running the logistic regression. \n \nsample dataset - ideal  \ntable 3 shows the ideal dataset to process \ndataset quality \ncount \n \ndependant \nvariable \nindependent variable. \naus_nsw_full_drivers_licence \n200 \n1 \nmatching feature template with \naus_nsw_full_drivers_licence_\nv \nother samples from the data source \nused are \n(aus_act_full_drivers_licence_\nv1) \n200 \n0 \nunmatching features \n \nnote: the dataset has mixed samples of other types of documents, so we need a manual approach to \nselect 200 license front page aus_nsw_full_drivers_licence_v1. data curation in the specific \nsession is shown below. \npositive testing \nfor the positive testing got the similarity score ranged from 0 to 31.79 and analyzed the distribution of \nthe similarity using a histogram. 27 records are falling under a similarity score of 5. \n \nfigure 9 shows the similarity score obtained for positive files. \nnegative testing \nfor the positive testing got the similarity score ranged from 0 to 4.62 and analyzed the distribution of \nthe similarity using a histogram.  \n \nfigure 10 on the left is the postive distribution & on the right negative distribution. \nfrom the hypothesis tests it has been found out the similarity score could improve by enhancing the \nimage, using the image enhancer script for the same, and running the same process of negative and \npositive testing to understand further differences in the similarity score obtained. similarly below is the \nhypothesis test to run to improve the output. \n \n• \nthe image enhancement model has improved the image quality it enhanced the blurred images \nwhich eventually improve the similarity scores and thereby better results. \n \nexperience: learned about the data complexity and chances of getting biased based on the data. the \ndocumentation part was rewarding during the time, documentation makes it easier to maintain the code \nover time. as code evolves and changes, well-documented code is more likely to be reusable. the coding \npart was a bit challenging as never worked with real-time processing and image enhancement of 1000 \ndocuments in a go. the organization team was supportive of providing a token to the azure api every time \nit expires which is an integral part of the image enhancement. \n \n \nweek 7: \ngoals involve converting the number of matching features into a match percentage that takes account of \nvariations in the max visual features for each document class and using logistic regression further.  analyze \nthe match percentage and further investigate failed documents. continue last week's work on generic \nmodels as there was some skewness observed.  \nactivities:  selecting max of 200 positive and negative files for each category. image enhance will be run \nbefore proceeding with every test for the same and running the logistic regression. then write code to \nfind visual feature matches for random files based on their similarity score. \ntable 4 dataset for hypothesis test 2 \ndataset quality \ncount \n \ndependant \nvariable \nindependent variable. \naus_nsw_full_drivers_licence_\nv1 (positive & negative combined) \n400 \n0,1 \nsimilarity scores \n \nfrom the analysis got logistic regression score of 0.98 typically refers to the accuracy or the performance \nof the logistic regression model on the evaluation dataset or the test dataset. the score of 0.98 indicates \nthat the model can correctly classify 98% of the observations in the test dataset. \noverall, a logistic regression score of 0.98 suggests that the model is performing well and is a strong \npredictor of the binary outcome variable being modeled.  \n for the below observation took 10 positives from aus_nsw_full_drivers_licence_v1 and negative \nsample files randomly from the dataset aus_nt_drivers_licence_v1, access_card_kiwietc. \ntable 5 calculating visual feature match based on the similarity score obtained. \nfile name \nsimilarity score \nvisual feature match \n04698be2-6b54-4ae9-82f0-02785c5a3230-1.jpg \n12.13872832 \n0.9998575724388266 \n4617c1bb-55ee-47ff-a522-8610de9c566c-1.jpg \n4.624277457 \n0.6760779822878397 \n4829fe2e-529c-4a35-9412-2cd14b4fdc27-1.jpg \n2.023121387 \n0.11152914079973549 \n \n \n \nhypothesis test iii \n \n• \nrun the generic testing. for a positive set, actual documents run against the actual templates, \nand for negative (generic negative documents) documents are not australian documents.  \nnote:\n \na \n \ndetailed \n \nexplanation \n \nof \n \nhow \n \nthe \n \nvisual \n \nfeature \n \nmatch \n \nis \n \ncalculated \n \nand \n \na \n \nlogistic \n \nregression\nproblem statement is explained detailed in next section.\n• \ncategorize the samples into respective document types folders. a sample of max 50 or less as per \nthe requirement later run the negative testing with non-id documents and perform the logistic \nregression on the same. \n \ntable 6 positive dataset for hypothesis test iii \ndata quality \ndata quantity \ndependent \nvariable \nindependent variable \nnon-id documents such as \nvisiting cards, debit, and \ncredit cards. \n1223 \n0 \nsimilarity scores \n \ntable 7 table 6 positive dataset for hypothesis test iii \ndata quality \ndata quantity \ndependent \nvariable \nindependent variable \nauthentic documents + \nnon-id documents such as \nvisiting cards, debit, and \ncredit cards. \n2532 \n1 \nsimilarity scores \n \n \nfigure 11 on left negative testing and on the right positive testing \nanalysis: it has been observed that there is right skewness in the positive data, it has to be further analyzed \nwhat causing the skewness. for this selected the positive files with a similarity score of less than 5 and \nnegative files with a similarity score greater than 5 from the dataset and did distribution plot for the same. \nthe future roadmap includes rectifying the skewness by improving the brightness of images through a \ndeep-learning model and performing the above steps including logistic regression. this process was the \nmost difficult so far in the internship period, i have come up with a couple of solutions like normalisation \non images, and log transformation on images to fix the skewness, which eventually has not given any \nexponential difference in the skewness, i have also received support to try out different solutions to the \nproblem from the team. \n \nweek 8: \ngoals involve converting the number of matching features into a match percentage that takes account \nof variations goals:  \n• \nimprove the skewness of the positive dataset image processed. \n• \nhypothesis 1: increase the image’s overall brightness by a specific percentage and see an \nimprovement in the document scores. \n \nactivities:   \nactivities include trying out multiple deep-learning image enhancement papers, selecting the best suits \nfrom them according to our requirements, and executing the same on the positive dataset. \nthe detailed objectives are: \n• \nfind out the best paper and try to replicate the codebase.  \n• \ntweak and apply the codebase to the organization’s existing positive dataset.  \n• \nimprove the accuracy score on the positive dataset and check the changes in skewness. \n \nfor this approach selected, 20 images from the daily processed document files with an image confidence \nscore of less than <0.7. wrote a python script to improve the existing image brightness by 30%. the \nfunction returns the original image if the image is already bright enough (determined by checking its \nmean pixel value).  \nimage performance metrics \nuse the metrics as signal-to-noise ratio (psnr), and mean squared error (mse). these metrics can be \nused to compare two images and evaluate their visual quality or fidelity similarity.  \nusing mse metrics on the above \nmse calculation assumes that the original image is the ground truth and the enhanced image is the \napproximation. they used psnr metrics as well. it compares the original image to the modified image \nand calculates how much noise has been introduced in the modified image. higher psnr values indicate \nthat the modified image is closer to the original image in terms of visual quality. \n \noutcomes: out of 20 documents, 11 document scores have been improved, two scores noted the same \nas before, and slight changes in the score of the other 7 papers were noted. \n \nfigure 12 output of the file after running the image enhancement \n \nfigure 13 change in the distribution of the data after enhancement \noverall: mean absolute error: 0.13245 \ncode reads the data into a panda’s data frame, applies the mae formula to each row of the data frame, \nand calculates the average of the mae values to get the final error rate. a lower mae indicates better \naccuracy in the predictions. \ngoing through multiple papers and replicating the code was challenging, especially when trying to \nreplicate the old paper, where the author has used tensorflow 2 as those functions still need to be \nsupported.  \na few of the images provided excellent values after image enhancement and running the ocr checks; \nunderstanding what caused this issue was also tiering way. \n \n \nweek 9 \nthe goals involve implementing different deep-learning model papers to solve the brightness issues. \n \nthe detailed objectives were: \n• \nfind out the best paper and try to replicate the codebase.  \n• \ntweak and apply the codebase to the organization’s existing positive dataset.  \n \n \npaper 1: denoising diffusion for low-light image enhancement \n \nthe paper proposes a new approach for the post-processing of low-light images, called the low-light \npost-processing diffusion model (lpdm). low-light image enhancement techniques often introduce \nvarious image degradations, such as noise and color bias, which post-processing denoisers address. \nhowever, these denoisers often produce smoothed results lacking detail. \nissues: \nthe model replication was successful and was able to generate the denoised images; the issue \nhighlighted below made not to proceed with the paper. \n• \nwe replicated the code base and applied the same to the positive dataset but eventually figured \nout the pre-trained model size is more than 1 gb, which would be a problem executing the \nprod system.  \n \npaper 2: local color distributions prior to image enhancement \n \nthis paper proposes a novel method called lcd-net, which consists of three main steps. in the first step, \nwe extract the lcds of the input image using a clustering-based algorithm. in the second step, we use a \nsegmentation network to locate the over- and underexposed regions based on their lcds. finally, in the \nthird step, we use a color enhancement network to enhance these regions. \n \npaper 3: neural curve layers for global image enhancement \n \nthe article further demonstrates the effectiveness of curl by combining this global image \ntransformation block with a pixel-level (local) image multi-scale encoder-decoder backbone network. \nthe experiments show that curl produces state-of-the-art image quality compared to recently \nproposed deep learning approaches in both objective and perceptual metrics, setting new state-of-the-\nart performance on multiple public datasets. \n \ntesting \n• \ntaken daily analysis report, which has documented confidence score reported, and selected 50 \nrows of data. \n• \nsorted the 50 data sources with a confidence score < 0.7 and checked the change in confidence \nscore by performing the step below: hypothesis testing. \n• \nsorted the 50 data sources with a confidence score < 0.8 and checked the change in confidence \nscore by performing the step below: regression testing. \n• \npass the document through an image enhancement process using a deep learning model.  \n• \nrun the document through ocr api on the above 2 sorted documents based on the confidence \nscore. \nhypothesis test 1 \nperforming image enhancement on the sorted image files with a document confidence score of less than \n0.7. from the below histogram analysis of the image before and after deep learning enhancement, it is \nevident that there has been a change in the scores, but concerns the ideal score has been given to 8 \ndocuments. \n \nfigure 14 on the left, before image enhancement. on the right, after image enhancement. \nregression testing -  \nperforming image enhancement on the sorted image files with a document confidence score of greater \nthan 0.8. from the below histogram analysis of the image before and after deep learning enhancement. \nwe need further analysis as it includes the idea score of 1 and observed some change in scores. \n \nfigure 15 on the left before image enhancement on the right after image enhancement \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nweek 10: \nlast week tried a few papers on deep-learning image enhancement and found the curl model performs \nbetter, ideated code from the paper and implement it according to current requirements then finally \nexecute the same on the positive dataset. \nthe detailed objectives are: \n• \nfind out the best paper and try to replicate the codebase.  \n• \ntweak and apply the codebase to the existing positive dataset the organization has.  \n• \nimprove the accuracy score on the positive dataset and check the changes in skewness. \n \nactivities:   \nfor this approach selected 20 failed ocr images from one drive and wrote a code that results in an excel \nsheet with respective filenames and brightness and overall brightness threshold value. \nto process the brightness threshold and values, i used the python pillow library, which reads the image \nand calculates the respective brightness index, whereas the openpyxl library helps to put these values in \nexcel format at the end. \nthe code calculates the brightness index for each image using the imagestat module from the pillow \nlibrary. the brightness index is a single value that represents the image's overall brightness. the average \nbrightness threshold value is calculated by taking the mean of the brightness values for all the images. \nthis value can be used as a threshold to determine which images have adequate brightness levels and \nwhich images may require adjustment. \nimage brightness: \nthe average brightness threshold for 20 images processed is 82.216 \nimage performance calculations: \nmse calculation assumes that the original image is the ground truth and the enhanced image is the \napproximation. also, the mse metric has its limitations, and its values may not always reflect the \nperceptual quality of the images. therefore, using multiple metrics and human evaluations to measure \nimage performance accurately is recommended. \nso using psnr metrics as well. it compares the original image to the modified image and calculates how \nmuch noise has been introduced in the modified image. higher psnr values indicate that the modified \nimage is closer to the original image in terms of visual quality, while lower psnr values indicate that the \nmodified image has more noise and is farther from the original image value ranging from 0 - 90db. a higher \nmse score indicates that there is more distortion or noise in the modified image. \n \nfigure 16 overview of performance metrics \n \n \nfigure 23 sample image showing an increase in brightness \ncode reads the data into a pandas data frame, applies the mae formula to each row of the data frame, \nand calculates the average of the mae values to get the final error rate. a lower mae indicates better \naccuracy in the predictions. \ncomparison of ocr performance before and after image brightness enhancement. \n \nfigure 17 distribution of the data \nweek 11: \n \nthe goal is to find the optimal target brightness could be improved by selecting with more reasoning. \nfor this reason, finding an optimal target brightness for image enhancement is important.  \n \n• \ndefine the optimal target brightness. \n• \ndetermine there is a difference in targets of different target brightness. \n \nactivities:  below is the target brightness with the respective starting brightness, the respective column \nrepresents the error rate percentage comparing the image before pre-processing and post-processing. \nthe target brightness is calculated with the formula as shown below. \ne.g., if the image brightness index is 127, the function subtracts the image brightness from the \nbrightness index divided by the brightness value. \n(127 - brightness value)/(brightness value)) \nwhere brightness index is the average image brightness which got by calculating the mean from the pil \npython package ‘imagestat’ library. the above formula determines how much brightness needs to be \nincreased to reach the desired level of 127 target value.  \nonce the pixel value load through the (load) function and the size function helps to calculate the width \nand length of the picture and using for loops it iterates over each pixel, hence new value for the rgb is \ncalculated by multiplying the original value.  \n \nhypothesis: \n• \nerror rates improve across all brackets of starting brightness for target = 130, 140, 100 \n• \nfound out 127 is not the optimal target brightness. \n• \nfor some brackets of starting brightness applying the target brightness results in higher ocr \nerrors. eg: 110. 127 \n• \nthe best way to define is to implement target brightness according to the starting brightness.  \n• \nuse target brightness 140 if starting brightness is 25 -40 and 150 if 50-75. \n• \nfor the higher starting brightness from 75- 125 use 110 as target brightness. \nin this way, a jump from 4% to 10.5% in error rate improvement has been noticed. \nissue analysis: \nin the above analysis, it's evident that enhancing image brightness increases the overall percentage of the \nerror rate improvement but the sample dataset is not viable to proceed. so need to further proceed with \nthe bigger dataset. in each starting image brightness category we make sure there is 10 variable in total \nto proceed this way it gives some confidence in the analysis of the data. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nweek 12: \n \nthe goal is to redo the last week's work as the sample dataset required for the initial analysis of finding \nthe optimal brightness, it has been found that the optimal target brightness could be improved by \nselecting with more reasoning.  \n \n• \ndefine the optimal target brightness. \n• \nuse a sample set of 50 with different starting brightness from 30 to 150. \n• \nperform brightness enhancement on pitch-dark images to find out if the black image happened \ndue to a lens/phone issue while the user taking the document photo.  \n \nactivities:  for the pitch-black image it has been found that it is not possible to increase the brightness \nvalue of a completely black (0 value) image. since there is no data to amplify or increase. brightness \nadjustments can be made to images that have existing pixel values representing different levels of \nbrightness.  \n \n \nfigure 18 analysing of the pitch black document images. \n \nthere was 2 image in each dataset with a brightness index value starting at 35 before enhancement \nincreasing the target brightness to 127 resulted in the not expected result. thus concluded that the \ndocument was not scanned properly by the user in the beginning and there have some other issues like \nlens issues and phone software-related issues in dealing with a dataset of pitch-black images received. \n \nanother task is to increase the target brightness with the respective starting brightness, the respective \ncolumn represents the error rate percentage comparing the image before pre-processing and post-\nprocessing. the target brightness is calculated with the formula as shown below. \ne.g., if the image brightness index is 127, the function subtracts the image brightness from the brightness \nindex divided by the brightness value. \n(127 - brightness value)/(brightness value)) \nwhere brightness index is the average image brightness which got by calculating the mean from the pil \npython package ‘imagestat’ library. the above formula determines how much brightness needs to be \nincreased to reach the desired level of 127 target value.  \n \nfigure 19 final percentage improvement. \n \n \n \nfigure 20 calculating the error rates. \n \nhypothesis: \n• \nerror rates improve majorly on optimal brightness 127 \n• \nthe weighted total average of 127 sees major changes.  \nin this way, a jump from 6 to 14% error rate improvement has been noticed.  \n \n \n \n \nweek 13: \n \nthe goal of this week is to rectify the manual error in the error rate analysis and generate the hypothesis \nfrom the inference from the table. each of the below-starting brightness error rates from 25-50,50-75,75-\n100,100-125 and above is calculated the below way. \n \n \n \nfigure 21 overall percentage improvement. \nanalysis: from the analysis it has been found that enhancing an image beyond 100 doesn’t add more value \nand sometimes the error rate improvement declines to an extent. so further apply the enhancement to \nthe image with brightness less than <50 as an index. \nfurther from the task i have also helped the organization a glimpse into the analysis of the market \nperformance by fetching the data curated from the search console and providing insights on the market \ntrends and keywords to focus to build brand awareness and further lead generation.  \n \nfigure 22 keyword analysis of the company. \nas this was the last week of the internship there was a review/ feedback session organized by the company \nsupervisor where he has given insights about the performance of the organization so far in detail and \nsuggestions to go ahead and mentioned to summarise the overall work done on the confluence \ndocumentation page of the organization. \n \n",
    "page": null,
    "goal": "Reflective journal Entities",
    "children": [
        {
            "id": "1.1",
            "name": "week 1",
            "nodeType": "title",
            "text": "week 1",
            "page": null,
            "goal": "week 1",
            "children": []
        },
        {
            "id": "1.2",
            "name": "week 1: the",
            "nodeType": "paragraph",
            "text": "week 1:  \nthe goals of the first week are to get started with onboarding and get to know about the task and \ndeadlines. the activities of the first week involve attending a kick-off workshop hosted by the company \nceo. mike simpson the ceo & co-founder introduced the machine learning teams and explained the \nworkflow inside the organization. got assigned a list of tasks to perform during the internship period, and \nwas able to ask and clarify the point of contact and more about the assigned task in person. \n \nfigure 3 roadmap for the company to achieve by june \nthe organisation introduced 6 streams where all the selected interns will be working this includes \nprocesses like data collection, capture, classification of data, authentication, extraction, and quality \nassurance and reporting. design, building, improving text format check, and designing, building, and \nimproving unique security feature check were the two important tasks assigned to me during the meeting. \nthese were part of the id document authenticity check. \nthe task design, building, and improving text format check includes the following functions to perform \n• \ngather text format metadata for all documents, define the logic of matching data against the \ndefined metadata.",
            "page": null,
            "goal": "week 1:  \nthe goals of the first week are to get started with onboarding and get to know about the task and \ndeadlines. the activities of the first week involve attending a kick-off workshop hosted by the company \nceo. mike simpson the ceo & co-founder introduced the machine learning teams and explained the \nworkflow inside the organization. got assigned a list of tasks to perform during the internship period, and \nwas able to ask and clarify the point of contact and more about the assigned task in person. \n \nfigure 3 roadmap for the company to achieve by june \nthe organisation introduced 6 streams where all the selected interns will be working this includes \nprocesses like data collection, capture, classification of data, authentication, extraction, and quality \nassurance and reporting. design, building, improving text format check, and designing, building, and \nimproving unique security feature check were the two important tasks assigned to me during the meeting. \nthese were part of the id document authenticity check. \nthe task design, building, and improving text format check includes the following functions to perform \n• \ngather text format metadata for all documents, define the logic of matching data against the \ndefined metadata.",
            "children": []
        },
        {
            "id": "1.3",
            "name": "week 2",
            "nodeType": "title",
            "text": "week 2",
            "page": null,
            "goal": "week 2",
            "children": []
        },
        {
            "id": "1.4",
            "name": "week 2: goals",
            "nodeType": "paragraph",
            "text": "week 2: \ngoals of week 2 involves getting access to most of the logins, which the company uses, like access to \nemails, teams, etc., and planning the 1:1 meeting. activities involve getting in touch with the security \nteams for access. outcomes: received the credentials for the work platforms and got in touch with the \nsupervisor and gave the internship availability dates every tuesday, wednesday, and thursday from 9:00 \nam to 6:00 pm, and scheduled weekly meetings for reporting with the supervisor.  \nevery thursday 30 minutes meeting is arranged between the technical product owner(sai kiran ravula) \nand the ceo (mike simpson) to discuss the progress made in the particular week and to discuss the \nroadmaps for the upcoming week including giving suggestions and comments on the performance done \nso far.  \nweek 2 the work mostly deals with researching the non-id document extraction. week 2 mostly worked \non finding different text patterns and finding how to extract the data from the text or metadata it has. \nhas to go through numerous real-time projects and research papers and implementations to find out the \ndifferent ways to extract data from the non-id particularly since nothing has a unique structure. used \ngoogle scholar for the research papers. \nresearching part was rewarding, never found it easy but the process was insightful about the latest or \npossible implementation of the task. this also helped me to think and process differently than the \ntraditional approach. as part of the onboarding did sign off on policy and standards or truuth organisation, \ndid setup jira confluence for the documentation part, followed the company guidelines to encrypt the \nsystem, and also went through security awareness training modules as part of locii information security \nmanagement system  security training & iso27k awareness training for interns 2020 - 2021 - locii isms.  \nthe security awareness module mostly deals with how to secure the data inside the organization and how \nto deal with vulnerability and threats and a detailed introduction about the cyber attacks through spear \nphishing emails, baiting, scareware, and pretexting. a few of the take away from the four module training \nare listed below. \n• \ncyber security in the workplace is everyone’s responsibility staffs should ensure that \n• \ndo not access locii’s  data from public wi-fi internet  \n• \nproper security measures are in place when handling confidential and high confidential data \n• \nwhen working in a shared workspace, privacy filters and screen locks are on for the endpoint \ndevices used. \n• \nare in a protected closed environment when discussing confidential ideas in a shared working \nspace   \n• \nthe url links are legitimate and not malicious before clicking it  \n• \nantivirus and patching are always on in their locii devices and byod \n• \n \nat some point i felt communicating with the team was difficult, especially when no response to the mail \nsent regarding the task to proceed next. the access set up with the security consultant was smooth. \ngetting to know more about what you do and trying to implement is a good way to handle the tough \nsituation, i spent extra hours after work researching the topic and going through the multiple research \npaper. maybe frequent follow-ups and guidelines to proceed further, i find it difficult to understand how \nto proceed further.",
            "page": null,
            "goal": "week 2: \ngoals of week 2 involves getting access to most of the logins, which the company uses, like access to \nemails, teams, etc., and planning the 1:1 meeting. activities involve getting in touch with the security \nteams for access. outcomes: received the credentials for the work platforms and got in touch with the \nsupervisor and gave the internship availability dates every tuesday, wednesday, and thursday from 9:00 \nam to 6:00 pm, and scheduled weekly meetings for reporting with the supervisor.  \nevery thursday 30 minutes meeting is arranged between the technical product owner(sai kiran ravula) \nand the ceo (mike simpson) to discuss the progress made in the particular week and to discuss the \nroadmaps for the upcoming week including giving suggestions and comments on the performance done \nso far.  \nweek 2 the work mostly deals with researching the non-id document extraction. week 2 mostly worked \non finding different text patterns and finding how to extract the data from the text or metadata it has. \nhas to go through numerous real-time projects and research papers and implementations to find out the \ndifferent ways to extract data from the non-id particularly since nothing has a unique structure. used \ngoogle scholar for the research papers. \nresearching part was rewarding, never found it easy but the process was insightful about the latest or \npossible implementation of the task. this also helped me to think and process differently than the \ntraditional approach. as part of the onboarding did sign off on policy and standards or truuth organisation, \ndid setup jira confluence for the documentation part, followed the company guidelines to encrypt the \nsystem, and also went through security awareness training modules as part of locii information security \nmanagement system  security training & iso27k awareness training for interns 2020 - 2021 - locii isms.  \nthe security awareness module mostly deals with how to secure the data inside the organization and how \nto deal with vulnerability and threats and a detailed introduction about the cyber attacks through spear \nphishing emails, baiting, scareware, and pretexting. a few of the take away from the four module training \nare listed below. \n• \ncyber security in the workplace is everyone’s responsibility staffs should ensure that \n• \ndo not access locii’s  data from public wi-fi internet  \n• \nproper security measures are in place when handling confidential and high confidential data \n• \nwhen working in a shared workspace, privacy filters and screen locks are on for the endpoint \ndevices used. \n• \nare in a protected closed environment when discussing confidential ideas in a shared working \nspace   \n• \nthe url links are legitimate and not malicious before clicking it  \n• \nantivirus and patching are always on in their locii devices and byod \n• \n \nat some point i felt communicating with the team was difficult, especially when no response to the mail \nsent regarding the task to proceed next. the access set up with the security consultant was smooth. \ngetting to know more about what you do and trying to implement is a good way to handle the tough \nsituation, i spent extra hours after work researching the topic and going through the multiple research \npaper. maybe frequent follow-ups and guidelines to proceed further, i find it difficult to understand how \nto proceed further.",
            "children": []
        },
        {
            "id": "1.5",
            "name": "week 3",
            "nodeType": "title",
            "text": "week 3",
            "page": null,
            "goal": "week 3",
            "children": []
        },
        {
            "id": "1.6",
            "name": "week 3: the",
            "nodeType": "paragraph",
            "text": "week 3: \nthe goals of week 3 involve checking the authenticity of the documents. here is the brief about it (an \nauthenticity check is needed to determine whether the document submitted by the user is an original \ndocument or a b&w photocopy, particularly in situations where authenticity is critical such as legal or \nfinancial transactions or verifying government-issued identification documents.) \nas part of this process, it's important to have a clear idea of the data before processing with and checking \nauthenticity using python script. on weekly calls, ceo mentioned he wanted the geographical \ndistribution of the data the organization has in a visualization format. the process involved fetching the \ndocument and passing the documents through python open cv api created in azure. then analysis the \ngeographical diversity of the documents. \n \nbelow is the step-by-step method followed to achieve the same. \n \n• \nthe dataset from the two data sources is carefully screened and merged into one source with 469 \ntotal passports. \n• \nwrote a python script to fetch the api and gather the score & country code of the passport. the \nreference sample is attached below. \n• \nthis process ran through all the individual passports in the dataset and got this output.csv file. \n \n \nfigure 4 output format received in the csv \n• \nran the visualization code for the bar plot, and geo plot to understand the distribution of the \npassport dataset. \ndata source \nfor the analysis of the geography distribution of the passport dataset, the organization initially collected \nthe dataset from the id bible, where manually downloaded the file from the a-z countries, and obtained \n269 passports. another dataset has been provided with a collection of 200 different passports. later \ncombined these two sets of data sources into one to proceed further. \n \n \ntotal dataset of passport: 469 \ntable 1 table showing the dataset which visualization performed \ndataset \ncount \nid bible \n269 \ndataset #1 \n200 \ntotal \n469 \nanalysis \nthe entire process is finished on 469 dataset images and received an output of 431. the remaining 38 \npassport file couldn't fetch the result either of the apis not able to fetch the data. most of the \npassports on the dataset are from australia(133), new zealand (12), the uk (8), philippines(7). most of \nthe diverse passports are covered as part of the dataset but are limited to 1 or <6. \n \nfigure 5: geo plot representing the wide coverage of passport \n \n \ngetting to know more about the data and being clear about the task i’m supposed to do helped to deal \nbetter with the challenge.  reaching out to the supervisor after not being able to get the solution and \nlooking for help better. improved skills in python and visualization, received proper guidance on extracting \nthe text from the document from the supervisor assigned. followed a proper communication channel for \nthe same.",
            "page": null,
            "goal": "week 3: \nthe goals of week 3 involve checking the authenticity of the documents. here is the brief about it (an \nauthenticity check is needed to determine whether the document submitted by the user is an original \ndocument or a b&w photocopy, particularly in situations where authenticity is critical such as legal or \nfinancial transactions or verifying government-issued identification documents.) \nas part of this process, it's important to have a clear idea of the data before processing with and checking \nauthenticity using python script. on weekly calls, ceo mentioned he wanted the geographical \ndistribution of the data the organization has in a visualization format. the process involved fetching the \ndocument and passing the documents through python open cv api created in azure. then analysis the \ngeographical diversity of the documents. \n \nbelow is the step-by-step method followed to achieve the same. \n \n• \nthe dataset from the two data sources is carefully screened and merged into one source with 469 \ntotal passports. \n• \nwrote a python script to fetch the api and gather the score & country code of the passport. the \nreference sample is attached below. \n• \nthis process ran through all the individual passports in the dataset and got this output.csv file. \n \n \nfigure 4 output format received in the csv \n• \nran the visualization code for the bar plot, and geo plot to understand the distribution of the \npassport dataset. \ndata source \nfor the analysis of the geography distribution of the passport dataset, the organization initially collected \nthe dataset from the id bible, where manually downloaded the file from the a-z countries, and obtained \n269 passports. another dataset has been provided with a collection of 200 different passports. later \ncombined these two sets of data sources into one to proceed further. \n \n \ntotal dataset of passport: 469 \ntable 1 table showing the dataset which visualization performed \ndataset \ncount \nid bible \n269 \ndataset #1 \n200 \ntotal \n469 \nanalysis \nthe entire process is finished on 469 dataset images and received an output of 431. the remaining 38 \npassport file couldn't fetch the result either of the apis not able to fetch the data. most of the \npassports on the dataset are from australia(133), new zealand (12), the uk (8), philippines(7). most of \nthe diverse passports are covered as part of the dataset but are limited to 1 or <6. \n \nfigure 5: geo plot representing the wide coverage of passport \n \n \ngetting to know more about the data and being clear about the task i’m supposed to do helped to deal \nbetter with the challenge.  reaching out to the supervisor after not being able to get the solution and \nlooking for help better. improved skills in python and visualization, received proper guidance on extracting \nthe text from the document from the supervisor assigned. followed a proper communication channel for \nthe same.",
            "children": []
        },
        {
            "id": "1.7",
            "name": "week 4",
            "nodeType": "title",
            "text": "week 4",
            "page": null,
            "goal": "week 4",
            "children": []
        },
        {
            "id": "1.8",
            "name": "week 4: the",
            "nodeType": "paragraph",
            "text": "week 4: \nthe goals of the week are to implement a visual feature check on the document database and generate \na matching score and generate a hypothesis on the mrz positive and negative testing + worked on the \nclassification errors apcer and bpcer. activities involve writing python scripts to check the visual feature \nof sample documents and user-submitted documents and generate a matching score.  \n \nbpcer (bona fide presentation classification error rate), is the percentage ratio at which bona fide \nexamples are misidentified as presentation attack examples. a higher value indicates higher user friction.  \napcer attack presentation classification error rate, the percentage ratio at which presentation attack \nexamples are misidentified as a bona fide example. a higher value indicates higher security vulnerability.) \n \nrun the testing on both positive and negative by generating negative samples, the negative sample is \ncreated by forging the authentic file like changing the value in the mrz field. negative testing has been \nperformed on the 200 authentic datasets from the org. a sample of 70 document images has been taken \nand changed the data based on the date of expiry, passport number, and date of issue.  \n \n \ntable 2 table showing how the document has been falsified for neg testing \nmrz dataset changes \ncount \nchange in passport no \n16 \nchange in dob \n24 \nchange in doe \n30 \ntotal \n70 \n \n \noutcomes: negative testing has been performed on the 200 authentic datasets from the org. a sample of \n70 document images has been taken and changed the data based on the date of expiry, passport number, \nand date of issue. the output of the above run has been attached below, where 66/70 of the false \ndocument api run was able to recognize its fake document.  \n \n \nfigure 6 further analysis of failed documents to understand why they failed. \nfor the positive testing considered the 200 authentic passports dataset and ran the script on the same. \nthe result obtained is 30/200 files turned out to be not authentic. 19/200 files turned to be mrz score of \n0 instead of 1 and the remaining 11 files didn't produce any score. few of the files produced blank results \nthis has been analyzed in the below approach. \n \nthen the final stage is to classify errors and get the error rate for apcer and bpcer. the error rate for \nbpcer: is 15.00%, and the error rate for apcer: is 5.71% \n \n \nfigure 7: figure showing bpcer error rates \n \nfigure 8: figure showing apcer error rate \n \nfrom the activity from this week got to know why sample set analysis is better than processing the entire \ndataset in one go. learned to use a python script to call the azure api. learned about the data complexity \nand chances of getting biased based on the data. a summary of this week's activity is listed below. \n• \nwrote a python script to fetch the api and gather the visual features of the document and \ngenerate a matching score. \n• \nwrote python script to redirect the ocr extracted text in the form of json code. \n• \nfew documents didn’t have any output, getting to a hypothesis and detailed analysis of that \nparticular process is a rewarding experience. this way gather knowledge to analyze the data \ndifferently.  \n• \nthe coding part was a bit challenging as never worked with real-time fetching of api and doing \nthe visual analysis of the document. \na few things noticed is organizationrequirement keeps changing sometimes, having a platform like an \nexcel report sheet to track the roadmap and timeline would be easy to understand the progress and track \nthe productivity as well.",
            "page": null,
            "goal": "week 4: \nthe goals of the week are to implement a visual feature check on the document database and generate \na matching score and generate a hypothesis on the mrz positive and negative testing + worked on the \nclassification errors apcer and bpcer. activities involve writing python scripts to check the visual feature \nof sample documents and user-submitted documents and generate a matching score.  \n \nbpcer (bona fide presentation classification error rate), is the percentage ratio at which bona fide \nexamples are misidentified as presentation attack examples. a higher value indicates higher user friction.  \napcer attack presentation classification error rate, the percentage ratio at which presentation attack \nexamples are misidentified as a bona fide example. a higher value indicates higher security vulnerability.) \n \nrun the testing on both positive and negative by generating negative samples, the negative sample is \ncreated by forging the authentic file like changing the value in the mrz field. negative testing has been \nperformed on the 200 authentic datasets from the org. a sample of 70 document images has been taken \nand changed the data based on the date of expiry, passport number, and date of issue.  \n \n \ntable 2 table showing how the document has been falsified for neg testing \nmrz dataset changes \ncount \nchange in passport no \n16 \nchange in dob \n24 \nchange in doe \n30 \ntotal \n70 \n \n \noutcomes: negative testing has been performed on the 200 authentic datasets from the org. a sample of \n70 document images has been taken and changed the data based on the date of expiry, passport number, \nand date of issue. the output of the above run has been attached below, where 66/70 of the false \ndocument api run was able to recognize its fake document.  \n \n \nfigure 6 further analysis of failed documents to understand why they failed. \nfor the positive testing considered the 200 authentic passports dataset and ran the script on the same. \nthe result obtained is 30/200 files turned out to be not authentic. 19/200 files turned to be mrz score of \n0 instead of 1 and the remaining 11 files didn't produce any score. few of the files produced blank results \nthis has been analyzed in the below approach. \n \nthen the final stage is to classify errors and get the error rate for apcer and bpcer. the error rate for \nbpcer: is 15.00%, and the error rate for apcer: is 5.71% \n \n \nfigure 7: figure showing bpcer error rates \n \nfigure 8: figure showing apcer error rate \n \nfrom the activity from this week got to know why sample set analysis is better than processing the entire \ndataset in one go. learned to use a python script to call the azure api. learned about the data complexity \nand chances of getting biased based on the data. a summary of this week's activity is listed below. \n• \nwrote a python script to fetch the api and gather the visual features of the document and \ngenerate a matching score. \n• \nwrote python script to redirect the ocr extracted text in the form of json code. \n• \nfew documents didn’t have any output, getting to a hypothesis and detailed analysis of that \nparticular process is a rewarding experience. this way gather knowledge to analyze the data \ndifferently.  \n• \nthe coding part was a bit challenging as never worked with real-time fetching of api and doing \nthe visual analysis of the document. \na few things noticed is organizationrequirement keeps changing sometimes, having a platform like an \nexcel report sheet to track the roadmap and timeline would be easy to understand the progress and track \nthe productivity as well.",
            "children": []
        },
        {
            "id": "1.9",
            "name": "week 5",
            "nodeType": "title",
            "text": "week 5",
            "page": null,
            "goal": "week 5",
            "children": []
        },
        {
            "id": "1.10",
            "name": "week 5: every",
            "nodeType": "paragraph",
            "text": "week 5: \nevery identification document contains distinct and unchanging features or objects that are consistent in \nsize and location within the document. these objects are utilized in determining the document's \nclassification. this week the goal is to build a visual feature check. this can achieve by specifying the \nprecise location of each object, and by examining the location of each object in a sample image, we \ndetermine whether the document is correctly positioned or not and find the matching percentage of the \ntemplate. \n \nactivities include checking the validity of the documents with a respective template which also provides \nthe matching percentage. the objectives have been categorized as the priority below. categorize the \nsample into respective document types folder. a sample of max 20 or less as per the requirement. run \nthe python program to find the respective match per the folders. analyze the match percentage and \nfurther investigate failed documents which will help with the following. \n \n• \nto see the positive and negative score distributions for each classification code. \n• \nto compare the results of the 3 different curation approaches. \n• \nto test the models on unseen positive and negative samples and calculate the far and frr. \n• \nto store the logistic regression parameters.  \n \nknowledge:  \nthe visual feature check is a sub-check within our authenticity api version 2 that tests whether the client-\nsubmitted id document image has similar visual features to the authentic template we have saved for \nthat classification code. visual characteristics like banners, logos, emblems, and any special symbols \npresent on the document are considered visual features. \n \n• \n learned about the data complexity and chances of getting biased based on the data.  \n• \nwrote a python script to fetch the api and gather the visual features of the document and \ngenerate a matching score. \n• \nwrote python script to get a matching percentage and further used logistic regression model for \nthe outputs. \n• \nthe data curation part was a tricky scenario: \n• \nthis is the scenario where we don’t have sufficient positive samples to perform the data gathering. \n• \nfor example, we don’t have 200 documents of the nsw_full_dl to run the positive analysis. in \nthis case, we would like to do the following data curation approach: \n \nthe coding part was a bit challenging as never worked with real-time fetching of api and doing the visual \nanalysis of the document. \n \nnote: this week has taken leave from work on thursday due to the back pain issue i faced, and my \nproductivity was less compared to the other weeks so far due to the sick. the organization's work culture \nis amazing without a prior email communication regarding the sick leave they approved the leave and \nmade sure i’m taking rest by giving off to the task assigned. most of the document tasks have been \ncovered this week comparing to the coding.",
            "page": null,
            "goal": "week 5: \nevery identification document contains distinct and unchanging features or objects that are consistent in \nsize and location within the document. these objects are utilized in determining the document's \nclassification. this week the goal is to build a visual feature check. this can achieve by specifying the \nprecise location of each object, and by examining the location of each object in a sample image, we \ndetermine whether the document is correctly positioned or not and find the matching percentage of the \ntemplate. \n \nactivities include checking the validity of the documents with a respective template which also provides \nthe matching percentage. the objectives have been categorized as the priority below. categorize the \nsample into respective document types folder. a sample of max 20 or less as per the requirement. run \nthe python program to find the respective match per the folders. analyze the match percentage and \nfurther investigate failed documents which will help with the following. \n \n• \nto see the positive and negative score distributions for each classification code. \n• \nto compare the results of the 3 different curation approaches. \n• \nto test the models on unseen positive and negative samples and calculate the far and frr. \n• \nto store the logistic regression parameters.  \n \nknowledge:  \nthe visual feature check is a sub-check within our authenticity api version 2 that tests whether the client-\nsubmitted id document image has similar visual features to the authentic template we have saved for \nthat classification code. visual characteristics like banners, logos, emblems, and any special symbols \npresent on the document are considered visual features. \n \n• \n learned about the data complexity and chances of getting biased based on the data.  \n• \nwrote a python script to fetch the api and gather the visual features of the document and \ngenerate a matching score. \n• \nwrote python script to get a matching percentage and further used logistic regression model for \nthe outputs. \n• \nthe data curation part was a tricky scenario: \n• \nthis is the scenario where we don’t have sufficient positive samples to perform the data gathering. \n• \nfor example, we don’t have 200 documents of the nsw_full_dl to run the positive analysis. in \nthis case, we would like to do the following data curation approach: \n \nthe coding part was a bit challenging as never worked with real-time fetching of api and doing the visual \nanalysis of the document. \n \nnote: this week has taken leave from work on thursday due to the back pain issue i faced, and my \nproductivity was less compared to the other weeks so far due to the sick. the organization's work culture \nis amazing without a prior email communication regarding the sick leave they approved the leave and \nmade sure i’m taking rest by giving off to the task assigned. most of the document tasks have been \ncovered this week comparing to the coding.",
            "children": []
        },
        {
            "id": "1.11",
            "name": "week 6",
            "nodeType": "title",
            "text": "week 6",
            "page": null,
            "goal": "week 6",
            "children": []
        },
        {
            "id": "1.12",
            "name": "week 6: the",
            "nodeType": "paragraph",
            "text": "week 6: \nthe task of week 6 involves doing the pending task from the last week of visual feature check and the \nbelow ones. \n• \ncollect quality samples for a wide range of global identity documents and create a template for \neach document class. our aim for the poc is to extend the visual feature check to all identity \ndocuments that have been templated (au + nz dls). a stretch goal is to extend the poc to global \npassports. \n• \ncategorize the samples into respective document types folders. a sample of max 50 or less as per \nthe requirement. \n• \nuse sift to calculate the number of matching features for each sample image vs the chosen \ntemplate for the document class \n \nactivities include running the generic testing. for a positive set, actual documents run against the actual \ntemplates, and for negative (generic negative documents) documents which are not australian \ndocuments like debit cards, debit cards. here selecting max of 200 files for each category. image enhance \nwill be run before proceeding with every test for the same and running the logistic regression. \n \nsample dataset - ideal  \ntable 3 shows the ideal dataset to process \ndataset quality \ncount \n \ndependant \nvariable \nindependent variable. \naus_nsw_full_drivers_licence \n200 \n1 \nmatching feature template with \naus_nsw_full_drivers_licence_\nv \nother samples from the data source \nused are \n(aus_act_full_drivers_licence_\nv1) \n200 \n0 \nunmatching features \n \nnote: the dataset has mixed samples of other types of documents, so we need a manual approach to \nselect 200 license front page aus_nsw_full_drivers_licence_v1. data curation in the specific \nsession is shown below. \npositive testing \nfor the positive testing got the similarity score ranged from 0 to 31.79 and analyzed the distribution of \nthe similarity using a histogram. 27 records are falling under a similarity score of 5. \n \nfigure 9 shows the similarity score obtained for positive files. \nnegative testing \nfor the positive testing got the similarity score ranged from 0 to 4.62 and analyzed the distribution of \nthe similarity using a histogram.  \n \nfigure 10 on the left is the postive distribution & on the right negative distribution. \nfrom the hypothesis tests it has been found out the similarity score could improve by enhancing the \nimage, using the image enhancer script for the same, and running the same process of negative and \npositive testing to understand further differences in the similarity score obtained. similarly below is the \nhypothesis test to run to improve the output. \n \n• \nthe image enhancement model has improved the image quality it enhanced the blurred images \nwhich eventually improve the similarity scores and thereby better results. \n \nexperience: learned about the data complexity and chances of getting biased based on the data. the \ndocumentation part was rewarding during the time, documentation makes it easier to maintain the code \nover time. as code evolves and changes, well-documented code is more likely to be reusable. the coding \npart was a bit challenging as never worked with real-time processing and image enhancement of 1000 \ndocuments in a go. the organization team was supportive of providing a token to the azure api every time \nit expires which is an integral part of the image enhancement.",
            "page": null,
            "goal": "week 6: \nthe task of week 6 involves doing the pending task from the last week of visual feature check and the \nbelow ones. \n• \ncollect quality samples for a wide range of global identity documents and create a template for \neach document class. our aim for the poc is to extend the visual feature check to all identity \ndocuments that have been templated (au + nz dls). a stretch goal is to extend the poc to global \npassports. \n• \ncategorize the samples into respective document types folders. a sample of max 50 or less as per \nthe requirement. \n• \nuse sift to calculate the number of matching features for each sample image vs the chosen \ntemplate for the document class \n \nactivities include running the generic testing. for a positive set, actual documents run against the actual \ntemplates, and for negative (generic negative documents) documents which are not australian \ndocuments like debit cards, debit cards. here selecting max of 200 files for each category. image enhance \nwill be run before proceeding with every test for the same and running the logistic regression. \n \nsample dataset - ideal  \ntable 3 shows the ideal dataset to process \ndataset quality \ncount \n \ndependant \nvariable \nindependent variable. \naus_nsw_full_drivers_licence \n200 \n1 \nmatching feature template with \naus_nsw_full_drivers_licence_\nv \nother samples from the data source \nused are \n(aus_act_full_drivers_licence_\nv1) \n200 \n0 \nunmatching features \n \nnote: the dataset has mixed samples of other types of documents, so we need a manual approach to \nselect 200 license front page aus_nsw_full_drivers_licence_v1. data curation in the specific \nsession is shown below. \npositive testing \nfor the positive testing got the similarity score ranged from 0 to 31.79 and analyzed the distribution of \nthe similarity using a histogram. 27 records are falling under a similarity score of 5. \n \nfigure 9 shows the similarity score obtained for positive files. \nnegative testing \nfor the positive testing got the similarity score ranged from 0 to 4.62 and analyzed the distribution of \nthe similarity using a histogram.  \n \nfigure 10 on the left is the postive distribution & on the right negative distribution. \nfrom the hypothesis tests it has been found out the similarity score could improve by enhancing the \nimage, using the image enhancer script for the same, and running the same process of negative and \npositive testing to understand further differences in the similarity score obtained. similarly below is the \nhypothesis test to run to improve the output. \n \n• \nthe image enhancement model has improved the image quality it enhanced the blurred images \nwhich eventually improve the similarity scores and thereby better results. \n \nexperience: learned about the data complexity and chances of getting biased based on the data. the \ndocumentation part was rewarding during the time, documentation makes it easier to maintain the code \nover time. as code evolves and changes, well-documented code is more likely to be reusable. the coding \npart was a bit challenging as never worked with real-time processing and image enhancement of 1000 \ndocuments in a go. the organization team was supportive of providing a token to the azure api every time \nit expires which is an integral part of the image enhancement.",
            "children": []
        },
        {
            "id": "1.13",
            "name": "week 7",
            "nodeType": "title",
            "text": "week 7",
            "page": null,
            "goal": "week 7",
            "children": []
        },
        {
            "id": "1.14",
            "name": "week 7: goals",
            "nodeType": "paragraph",
            "text": "week 7: \ngoals involve converting the number of matching features into a match percentage that takes account of \nvariations in the max visual features for each document class and using logistic regression further.  analyze \nthe match percentage and further investigate failed documents. continue last week's work on generic \nmodels as there was some skewness observed.  \nactivities:  selecting max of 200 positive and negative files for each category. image enhance will be run \nbefore proceeding with every test for the same and running the logistic regression. then write code to \nfind visual feature matches for random files based on their similarity score. \ntable 4 dataset for hypothesis test 2 \ndataset quality \ncount \n \ndependant \nvariable \nindependent variable. \naus_nsw_full_drivers_licence_\nv1 (positive & negative combined) \n400 \n0,1 \nsimilarity scores \n \nfrom the analysis got logistic regression score of 0.98 typically refers to the accuracy or the performance \nof the logistic regression model on the evaluation dataset or the test dataset. the score of 0.98 indicates \nthat the model can correctly classify 98% of the observations in the test dataset. \noverall, a logistic regression score of 0.98 suggests that the model is performing well and is a strong \npredictor of the binary outcome variable being modeled.  \n for the below observation took 10 positives from aus_nsw_full_drivers_licence_v1 and negative \nsample files randomly from the dataset aus_nt_drivers_licence_v1, access_card_kiwietc. \ntable 5 calculating visual feature match based on the similarity score obtained. \nfile name \nsimilarity score \nvisual feature match \n04698be2-6b54-4ae9-82f0-02785c5a3230-1.jpg \n12.13872832 \n0.9998575724388266 \n4617c1bb-55ee-47ff-a522-8610de9c566c-1.jpg \n4.624277457 \n0.6760779822878397 \n4829fe2e-529c-4a35-9412-2cd14b4fdc27-1.jpg \n2.023121387 \n0.11152914079973549 \n \n \n \nhypothesis test iii \n \n• \nrun the generic testing. for a positive set, actual documents run against the actual templates, \nand for negative (generic negative documents) documents are not australian documents.  \nnote:\n \na \n \ndetailed \n \nexplanation \n \nof \n \nhow \n \nthe \n \nvisual \n \nfeature \n \nmatch \n \nis \n \ncalculated \n \nand \n \na \n \nlogistic \n \nregression\nproblem statement is explained detailed in next section.\n• \ncategorize the samples into respective document types folders. a sample of max 50 or less as per \nthe requirement later run the negative testing with non-id documents and perform the logistic \nregression on the same. \n \ntable 6 positive dataset for hypothesis test iii \ndata quality \ndata quantity \ndependent \nvariable \nindependent variable \nnon-id documents such as \nvisiting cards, debit, and \ncredit cards. \n1223 \n0 \nsimilarity scores \n \ntable 7 table 6 positive dataset for hypothesis test iii \ndata quality \ndata quantity \ndependent \nvariable \nindependent variable \nauthentic documents + \nnon-id documents such as \nvisiting cards, debit, and \ncredit cards. \n2532 \n1 \nsimilarity scores \n \n \nfigure 11 on left negative testing and on the right positive testing \nanalysis: it has been observed that there is right skewness in the positive data, it has to be further analyzed \nwhat causing the skewness. for this selected the positive files with a similarity score of less than 5 and \nnegative files with a similarity score greater than 5 from the dataset and did distribution plot for the same. \nthe future roadmap includes rectifying the skewness by improving the brightness of images through a \ndeep-learning model and performing the above steps including logistic regression. this process was the \nmost difficult so far in the internship period, i have come up with a couple of solutions like normalisation \non images, and log transformation on images to fix the skewness, which eventually has not given any \nexponential difference in the skewness, i have also received support to try out different solutions to the \nproblem from the team.",
            "page": null,
            "goal": "week 7: \ngoals involve converting the number of matching features into a match percentage that takes account of \nvariations in the max visual features for each document class and using logistic regression further.  analyze \nthe match percentage and further investigate failed documents. continue last week's work on generic \nmodels as there was some skewness observed.  \nactivities:  selecting max of 200 positive and negative files for each category. image enhance will be run \nbefore proceeding with every test for the same and running the logistic regression. then write code to \nfind visual feature matches for random files based on their similarity score. \ntable 4 dataset for hypothesis test 2 \ndataset quality \ncount \n \ndependant \nvariable \nindependent variable. \naus_nsw_full_drivers_licence_\nv1 (positive & negative combined) \n400 \n0,1 \nsimilarity scores \n \nfrom the analysis got logistic regression score of 0.98 typically refers to the accuracy or the performance \nof the logistic regression model on the evaluation dataset or the test dataset. the score of 0.98 indicates \nthat the model can correctly classify 98% of the observations in the test dataset. \noverall, a logistic regression score of 0.98 suggests that the model is performing well and is a strong \npredictor of the binary outcome variable being modeled.  \n for the below observation took 10 positives from aus_nsw_full_drivers_licence_v1 and negative \nsample files randomly from the dataset aus_nt_drivers_licence_v1, access_card_kiwietc. \ntable 5 calculating visual feature match based on the similarity score obtained. \nfile name \nsimilarity score \nvisual feature match \n04698be2-6b54-4ae9-82f0-02785c5a3230-1.jpg \n12.13872832 \n0.9998575724388266 \n4617c1bb-55ee-47ff-a522-8610de9c566c-1.jpg \n4.624277457 \n0.6760779822878397 \n4829fe2e-529c-4a35-9412-2cd14b4fdc27-1.jpg \n2.023121387 \n0.11152914079973549 \n \n \n \nhypothesis test iii \n \n• \nrun the generic testing. for a positive set, actual documents run against the actual templates, \nand for negative (generic negative documents) documents are not australian documents.  \nnote:\n \na \n \ndetailed \n \nexplanation \n \nof \n \nhow \n \nthe \n \nvisual \n \nfeature \n \nmatch \n \nis \n \ncalculated \n \nand \n \na \n \nlogistic \n \nregression\nproblem statement is explained detailed in next section.\n• \ncategorize the samples into respective document types folders. a sample of max 50 or less as per \nthe requirement later run the negative testing with non-id documents and perform the logistic \nregression on the same. \n \ntable 6 positive dataset for hypothesis test iii \ndata quality \ndata quantity \ndependent \nvariable \nindependent variable \nnon-id documents such as \nvisiting cards, debit, and \ncredit cards. \n1223 \n0 \nsimilarity scores \n \ntable 7 table 6 positive dataset for hypothesis test iii \ndata quality \ndata quantity \ndependent \nvariable \nindependent variable \nauthentic documents + \nnon-id documents such as \nvisiting cards, debit, and \ncredit cards. \n2532 \n1 \nsimilarity scores \n \n \nfigure 11 on left negative testing and on the right positive testing \nanalysis: it has been observed that there is right skewness in the positive data, it has to be further analyzed \nwhat causing the skewness. for this selected the positive files with a similarity score of less than 5 and \nnegative files with a similarity score greater than 5 from the dataset and did distribution plot for the same. \nthe future roadmap includes rectifying the skewness by improving the brightness of images through a \ndeep-learning model and performing the above steps including logistic regression. this process was the \nmost difficult so far in the internship period, i have come up with a couple of solutions like normalisation \non images, and log transformation on images to fix the skewness, which eventually has not given any \nexponential difference in the skewness, i have also received support to try out different solutions to the \nproblem from the team.",
            "children": []
        },
        {
            "id": "1.15",
            "name": "week 8",
            "nodeType": "title",
            "text": "week 8",
            "page": null,
            "goal": "week 8",
            "children": []
        },
        {
            "id": "1.16",
            "name": "week 8: goals",
            "nodeType": "paragraph",
            "text": "week 8: \ngoals involve converting the number of matching features into a match percentage that takes account \nof variations goals:  \n• \nimprove the skewness of the positive dataset image processed. \n• \nhypothesis 1: increase the image’s overall brightness by a specific percentage and see an \nimprovement in the document scores. \n \nactivities:   \nactivities include trying out multiple deep-learning image enhancement papers, selecting the best suits \nfrom them according to our requirements, and executing the same on the positive dataset. \nthe detailed objectives are: \n• \nfind out the best paper and try to replicate the codebase.  \n• \ntweak and apply the codebase to the organization’s existing positive dataset.  \n• \nimprove the accuracy score on the positive dataset and check the changes in skewness. \n \nfor this approach selected, 20 images from the daily processed document files with an image confidence \nscore of less than <0.7. wrote a python script to improve the existing image brightness by 30%. the \nfunction returns the original image if the image is already bright enough (determined by checking its \nmean pixel value).  \nimage performance metrics \nuse the metrics as signal-to-noise ratio (psnr), and mean squared error (mse). these metrics can be \nused to compare two images and evaluate their visual quality or fidelity similarity.  \nusing mse metrics on the above \nmse calculation assumes that the original image is the ground truth and the enhanced image is the \napproximation. they used psnr metrics as well. it compares the original image to the modified image \nand calculates how much noise has been introduced in the modified image. higher psnr values indicate \nthat the modified image is closer to the original image in terms of visual quality. \n \noutcomes: out of 20 documents, 11 document scores have been improved, two scores noted the same \nas before, and slight changes in the score of the other 7 papers were noted. \n \nfigure 12 output of the file after running the image enhancement \n \nfigure 13 change in the distribution of the data after enhancement \noverall: mean absolute error: 0.13245 \ncode reads the data into a panda’s data frame, applies the mae formula to each row of the data frame, \nand calculates the average of the mae values to get the final error rate. a lower mae indicates better \naccuracy in the predictions. \ngoing through multiple papers and replicating the code was challenging, especially when trying to \nreplicate the old paper, where the author has used tensorflow 2 as those functions still need to be \nsupported.  \na few of the images provided excellent values after image enhancement and running the ocr checks; \nunderstanding what caused this issue was also tiering way.",
            "page": null,
            "goal": "week 8: \ngoals involve converting the number of matching features into a match percentage that takes account \nof variations goals:  \n• \nimprove the skewness of the positive dataset image processed. \n• \nhypothesis 1: increase the image’s overall brightness by a specific percentage and see an \nimprovement in the document scores. \n \nactivities:   \nactivities include trying out multiple deep-learning image enhancement papers, selecting the best suits \nfrom them according to our requirements, and executing the same on the positive dataset. \nthe detailed objectives are: \n• \nfind out the best paper and try to replicate the codebase.  \n• \ntweak and apply the codebase to the organization’s existing positive dataset.  \n• \nimprove the accuracy score on the positive dataset and check the changes in skewness. \n \nfor this approach selected, 20 images from the daily processed document files with an image confidence \nscore of less than <0.7. wrote a python script to improve the existing image brightness by 30%. the \nfunction returns the original image if the image is already bright enough (determined by checking its \nmean pixel value).  \nimage performance metrics \nuse the metrics as signal-to-noise ratio (psnr), and mean squared error (mse). these metrics can be \nused to compare two images and evaluate their visual quality or fidelity similarity.  \nusing mse metrics on the above \nmse calculation assumes that the original image is the ground truth and the enhanced image is the \napproximation. they used psnr metrics as well. it compares the original image to the modified image \nand calculates how much noise has been introduced in the modified image. higher psnr values indicate \nthat the modified image is closer to the original image in terms of visual quality. \n \noutcomes: out of 20 documents, 11 document scores have been improved, two scores noted the same \nas before, and slight changes in the score of the other 7 papers were noted. \n \nfigure 12 output of the file after running the image enhancement \n \nfigure 13 change in the distribution of the data after enhancement \noverall: mean absolute error: 0.13245 \ncode reads the data into a panda’s data frame, applies the mae formula to each row of the data frame, \nand calculates the average of the mae values to get the final error rate. a lower mae indicates better \naccuracy in the predictions. \ngoing through multiple papers and replicating the code was challenging, especially when trying to \nreplicate the old paper, where the author has used tensorflow 2 as those functions still need to be \nsupported.  \na few of the images provided excellent values after image enhancement and running the ocr checks; \nunderstanding what caused this issue was also tiering way.",
            "children": []
        },
        {
            "id": "1.17",
            "name": "week 9",
            "nodeType": "title",
            "text": "week 9",
            "page": null,
            "goal": "week 9",
            "children": []
        },
        {
            "id": "1.18",
            "name": "week 9 the",
            "nodeType": "paragraph",
            "text": "week 9 \nthe goals involve implementing different deep-learning model papers to solve the brightness issues. \n \nthe detailed objectives were: \n• \nfind out the best paper and try to replicate the codebase.  \n• \ntweak and apply the codebase to the organization’s existing positive dataset.  \n \n \npaper 1: denoising diffusion for low-light image enhancement \n \nthe paper proposes a new approach for the post-processing of low-light images, called the low-light \npost-processing diffusion model (lpdm). low-light image enhancement techniques often introduce \nvarious image degradations, such as noise and color bias, which post-processing denoisers address. \nhowever, these denoisers often produce smoothed results lacking detail. \nissues: \nthe model replication was successful and was able to generate the denoised images; the issue \nhighlighted below made not to proceed with the paper. \n• \nwe replicated the code base and applied the same to the positive dataset but eventually figured \nout the pre-trained model size is more than 1 gb, which would be a problem executing the \nprod system.  \n \npaper 2: local color distributions prior to image enhancement \n \nthis paper proposes a novel method called lcd-net, which consists of three main steps. in the first step, \nwe extract the lcds of the input image using a clustering-based algorithm. in the second step, we use a \nsegmentation network to locate the over- and underexposed regions based on their lcds. finally, in the \nthird step, we use a color enhancement network to enhance these regions. \n \npaper 3: neural curve layers for global image enhancement \n \nthe article further demonstrates the effectiveness of curl by combining this global image \ntransformation block with a pixel-level (local) image multi-scale encoder-decoder backbone network. \nthe experiments show that curl produces state-of-the-art image quality compared to recently \nproposed deep learning approaches in both objective and perceptual metrics, setting new state-of-the-\nart performance on multiple public datasets. \n \ntesting \n• \ntaken daily analysis report, which has documented confidence score reported, and selected 50 \nrows of data. \n• \nsorted the 50 data sources with a confidence score < 0.7 and checked the change in confidence \nscore by performing the step below: hypothesis testing. \n• \nsorted the 50 data sources with a confidence score < 0.8 and checked the change in confidence \nscore by performing the step below: regression testing. \n• \npass the document through an image enhancement process using a deep learning model.  \n• \nrun the document through ocr api on the above 2 sorted documents based on the confidence \nscore. \nhypothesis test 1 \nperforming image enhancement on the sorted image files with a document confidence score of less than \n0.7. from the below histogram analysis of the image before and after deep learning enhancement, it is \nevident that there has been a change in the scores, but concerns the ideal score has been given to 8 \ndocuments. \n \nfigure 14 on the left, before image enhancement. on the right, after image enhancement. \nregression testing -  \nperforming image enhancement on the sorted image files with a document confidence score of greater \nthan 0.8. from the below histogram analysis of the image before and after deep learning enhancement. \nwe need further analysis as it includes the idea score of 1 and observed some change in scores. \n \nfigure 15 on the left before image enhancement on the right after image enhancement",
            "page": null,
            "goal": "week 9 \nthe goals involve implementing different deep-learning model papers to solve the brightness issues. \n \nthe detailed objectives were: \n• \nfind out the best paper and try to replicate the codebase.  \n• \ntweak and apply the codebase to the organization’s existing positive dataset.  \n \n \npaper 1: denoising diffusion for low-light image enhancement \n \nthe paper proposes a new approach for the post-processing of low-light images, called the low-light \npost-processing diffusion model (lpdm). low-light image enhancement techniques often introduce \nvarious image degradations, such as noise and color bias, which post-processing denoisers address. \nhowever, these denoisers often produce smoothed results lacking detail. \nissues: \nthe model replication was successful and was able to generate the denoised images; the issue \nhighlighted below made not to proceed with the paper. \n• \nwe replicated the code base and applied the same to the positive dataset but eventually figured \nout the pre-trained model size is more than 1 gb, which would be a problem executing the \nprod system.  \n \npaper 2: local color distributions prior to image enhancement \n \nthis paper proposes a novel method called lcd-net, which consists of three main steps. in the first step, \nwe extract the lcds of the input image using a clustering-based algorithm. in the second step, we use a \nsegmentation network to locate the over- and underexposed regions based on their lcds. finally, in the \nthird step, we use a color enhancement network to enhance these regions. \n \npaper 3: neural curve layers for global image enhancement \n \nthe article further demonstrates the effectiveness of curl by combining this global image \ntransformation block with a pixel-level (local) image multi-scale encoder-decoder backbone network. \nthe experiments show that curl produces state-of-the-art image quality compared to recently \nproposed deep learning approaches in both objective and perceptual metrics, setting new state-of-the-\nart performance on multiple public datasets. \n \ntesting \n• \ntaken daily analysis report, which has documented confidence score reported, and selected 50 \nrows of data. \n• \nsorted the 50 data sources with a confidence score < 0.7 and checked the change in confidence \nscore by performing the step below: hypothesis testing. \n• \nsorted the 50 data sources with a confidence score < 0.8 and checked the change in confidence \nscore by performing the step below: regression testing. \n• \npass the document through an image enhancement process using a deep learning model.  \n• \nrun the document through ocr api on the above 2 sorted documents based on the confidence \nscore. \nhypothesis test 1 \nperforming image enhancement on the sorted image files with a document confidence score of less than \n0.7. from the below histogram analysis of the image before and after deep learning enhancement, it is \nevident that there has been a change in the scores, but concerns the ideal score has been given to 8 \ndocuments. \n \nfigure 14 on the left, before image enhancement. on the right, after image enhancement. \nregression testing -  \nperforming image enhancement on the sorted image files with a document confidence score of greater \nthan 0.8. from the below histogram analysis of the image before and after deep learning enhancement. \nwe need further analysis as it includes the idea score of 1 and observed some change in scores. \n \nfigure 15 on the left before image enhancement on the right after image enhancement",
            "children": []
        },
        {
            "id": "1.19",
            "name": "week 10",
            "nodeType": "title",
            "text": "week 10",
            "page": null,
            "goal": "week 10",
            "children": []
        },
        {
            "id": "1.20",
            "name": "week 10: last",
            "nodeType": "paragraph",
            "text": "week 10: \nlast week tried a few papers on deep-learning image enhancement and found the curl model performs \nbetter, ideated code from the paper and implement it according to current requirements then finally \nexecute the same on the positive dataset. \nthe detailed objectives are: \n• \nfind out the best paper and try to replicate the codebase.  \n• \ntweak and apply the codebase to the existing positive dataset the organization has.  \n• \nimprove the accuracy score on the positive dataset and check the changes in skewness. \n \nactivities:   \nfor this approach selected 20 failed ocr images from one drive and wrote a code that results in an excel \nsheet with respective filenames and brightness and overall brightness threshold value. \nto process the brightness threshold and values, i used the python pillow library, which reads the image \nand calculates the respective brightness index, whereas the openpyxl library helps to put these values in \nexcel format at the end. \nthe code calculates the brightness index for each image using the imagestat module from the pillow \nlibrary. the brightness index is a single value that represents the image's overall brightness. the average \nbrightness threshold value is calculated by taking the mean of the brightness values for all the images. \nthis value can be used as a threshold to determine which images have adequate brightness levels and \nwhich images may require adjustment. \nimage brightness: \nthe average brightness threshold for 20 images processed is 82.216 \nimage performance calculations: \nmse calculation assumes that the original image is the ground truth and the enhanced image is the \napproximation. also, the mse metric has its limitations, and its values may not always reflect the \nperceptual quality of the images. therefore, using multiple metrics and human evaluations to measure \nimage performance accurately is recommended. \nso using psnr metrics as well. it compares the original image to the modified image and calculates how \nmuch noise has been introduced in the modified image. higher psnr values indicate that the modified \nimage is closer to the original image in terms of visual quality, while lower psnr values indicate that the \nmodified image has more noise and is farther from the original image value ranging from 0 - 90db. a higher \nmse score indicates that there is more distortion or noise in the modified image. \n \nfigure 16 overview of performance metrics \n \n \nfigure 23 sample image showing an increase in brightness \ncode reads the data into a pandas data frame, applies the mae formula to each row of the data frame, \nand calculates the average of the mae values to get the final error rate. a lower mae indicates better \naccuracy in the predictions. \ncomparison of ocr performance before and after image brightness enhancement. \n \nfigure 17 distribution of the data",
            "page": null,
            "goal": "week 10: \nlast week tried a few papers on deep-learning image enhancement and found the curl model performs \nbetter, ideated code from the paper and implement it according to current requirements then finally \nexecute the same on the positive dataset. \nthe detailed objectives are: \n• \nfind out the best paper and try to replicate the codebase.  \n• \ntweak and apply the codebase to the existing positive dataset the organization has.  \n• \nimprove the accuracy score on the positive dataset and check the changes in skewness. \n \nactivities:   \nfor this approach selected 20 failed ocr images from one drive and wrote a code that results in an excel \nsheet with respective filenames and brightness and overall brightness threshold value. \nto process the brightness threshold and values, i used the python pillow library, which reads the image \nand calculates the respective brightness index, whereas the openpyxl library helps to put these values in \nexcel format at the end. \nthe code calculates the brightness index for each image using the imagestat module from the pillow \nlibrary. the brightness index is a single value that represents the image's overall brightness. the average \nbrightness threshold value is calculated by taking the mean of the brightness values for all the images. \nthis value can be used as a threshold to determine which images have adequate brightness levels and \nwhich images may require adjustment. \nimage brightness: \nthe average brightness threshold for 20 images processed is 82.216 \nimage performance calculations: \nmse calculation assumes that the original image is the ground truth and the enhanced image is the \napproximation. also, the mse metric has its limitations, and its values may not always reflect the \nperceptual quality of the images. therefore, using multiple metrics and human evaluations to measure \nimage performance accurately is recommended. \nso using psnr metrics as well. it compares the original image to the modified image and calculates how \nmuch noise has been introduced in the modified image. higher psnr values indicate that the modified \nimage is closer to the original image in terms of visual quality, while lower psnr values indicate that the \nmodified image has more noise and is farther from the original image value ranging from 0 - 90db. a higher \nmse score indicates that there is more distortion or noise in the modified image. \n \nfigure 16 overview of performance metrics \n \n \nfigure 23 sample image showing an increase in brightness \ncode reads the data into a pandas data frame, applies the mae formula to each row of the data frame, \nand calculates the average of the mae values to get the final error rate. a lower mae indicates better \naccuracy in the predictions. \ncomparison of ocr performance before and after image brightness enhancement. \n \nfigure 17 distribution of the data",
            "children": []
        },
        {
            "id": "1.21",
            "name": "week 11",
            "nodeType": "title",
            "text": "week 11",
            "page": null,
            "goal": "week 11",
            "children": []
        },
        {
            "id": "1.22",
            "name": "week 11: the",
            "nodeType": "paragraph",
            "text": "week 11: \n \nthe goal is to find the optimal target brightness could be improved by selecting with more reasoning. \nfor this reason, finding an optimal target brightness for image enhancement is important.  \n \n• \ndefine the optimal target brightness. \n• \ndetermine there is a difference in targets of different target brightness. \n \nactivities:  below is the target brightness with the respective starting brightness, the respective column \nrepresents the error rate percentage comparing the image before pre-processing and post-processing. \nthe target brightness is calculated with the formula as shown below. \ne.g., if the image brightness index is 127, the function subtracts the image brightness from the \nbrightness index divided by the brightness value. \n(127 - brightness value)/(brightness value)) \nwhere brightness index is the average image brightness which got by calculating the mean from the pil \npython package ‘imagestat’ library. the above formula determines how much brightness needs to be \nincreased to reach the desired level of 127 target value.  \nonce the pixel value load through the (load) function and the size function helps to calculate the width \nand length of the picture and using for loops it iterates over each pixel, hence new value for the rgb is \ncalculated by multiplying the original value.  \n \nhypothesis: \n• \nerror rates improve across all brackets of starting brightness for target = 130, 140, 100 \n• \nfound out 127 is not the optimal target brightness. \n• \nfor some brackets of starting brightness applying the target brightness results in higher ocr \nerrors. eg: 110. 127 \n• \nthe best way to define is to implement target brightness according to the starting brightness.  \n• \nuse target brightness 140 if starting brightness is 25 -40 and 150 if 50-75. \n• \nfor the higher starting brightness from 75- 125 use 110 as target brightness. \nin this way, a jump from 4% to 10.5% in error rate improvement has been noticed. \nissue analysis: \nin the above analysis, it's evident that enhancing image brightness increases the overall percentage of the \nerror rate improvement but the sample dataset is not viable to proceed. so need to further proceed with \nthe bigger dataset. in each starting image brightness category we make sure there is 10 variable in total \nto proceed this way it gives some confidence in the analysis of the data.",
            "page": null,
            "goal": "week 11: \n \nthe goal is to find the optimal target brightness could be improved by selecting with more reasoning. \nfor this reason, finding an optimal target brightness for image enhancement is important.  \n \n• \ndefine the optimal target brightness. \n• \ndetermine there is a difference in targets of different target brightness. \n \nactivities:  below is the target brightness with the respective starting brightness, the respective column \nrepresents the error rate percentage comparing the image before pre-processing and post-processing. \nthe target brightness is calculated with the formula as shown below. \ne.g., if the image brightness index is 127, the function subtracts the image brightness from the \nbrightness index divided by the brightness value. \n(127 - brightness value)/(brightness value)) \nwhere brightness index is the average image brightness which got by calculating the mean from the pil \npython package ‘imagestat’ library. the above formula determines how much brightness needs to be \nincreased to reach the desired level of 127 target value.  \nonce the pixel value load through the (load) function and the size function helps to calculate the width \nand length of the picture and using for loops it iterates over each pixel, hence new value for the rgb is \ncalculated by multiplying the original value.  \n \nhypothesis: \n• \nerror rates improve across all brackets of starting brightness for target = 130, 140, 100 \n• \nfound out 127 is not the optimal target brightness. \n• \nfor some brackets of starting brightness applying the target brightness results in higher ocr \nerrors. eg: 110. 127 \n• \nthe best way to define is to implement target brightness according to the starting brightness.  \n• \nuse target brightness 140 if starting brightness is 25 -40 and 150 if 50-75. \n• \nfor the higher starting brightness from 75- 125 use 110 as target brightness. \nin this way, a jump from 4% to 10.5% in error rate improvement has been noticed. \nissue analysis: \nin the above analysis, it's evident that enhancing image brightness increases the overall percentage of the \nerror rate improvement but the sample dataset is not viable to proceed. so need to further proceed with \nthe bigger dataset. in each starting image brightness category we make sure there is 10 variable in total \nto proceed this way it gives some confidence in the analysis of the data.",
            "children": []
        },
        {
            "id": "1.23",
            "name": "week 12",
            "nodeType": "title",
            "text": "week 12",
            "page": null,
            "goal": "week 12",
            "children": []
        },
        {
            "id": "1.24",
            "name": "week 12: the",
            "nodeType": "paragraph",
            "text": "week 12: \n \nthe goal is to redo the last week's work as the sample dataset required for the initial analysis of finding \nthe optimal brightness, it has been found that the optimal target brightness could be improved by \nselecting with more reasoning.  \n \n• \ndefine the optimal target brightness. \n• \nuse a sample set of 50 with different starting brightness from 30 to 150. \n• \nperform brightness enhancement on pitch-dark images to find out if the black image happened \ndue to a lens/phone issue while the user taking the document photo.  \n \nactivities:  for the pitch-black image it has been found that it is not possible to increase the brightness \nvalue of a completely black (0 value) image. since there is no data to amplify or increase. brightness \nadjustments can be made to images that have existing pixel values representing different levels of \nbrightness.  \n \n \nfigure 18 analysing of the pitch black document images. \n \nthere was 2 image in each dataset with a brightness index value starting at 35 before enhancement \nincreasing the target brightness to 127 resulted in the not expected result. thus concluded that the \ndocument was not scanned properly by the user in the beginning and there have some other issues like \nlens issues and phone software-related issues in dealing with a dataset of pitch-black images received. \n \nanother task is to increase the target brightness with the respective starting brightness, the respective \ncolumn represents the error rate percentage comparing the image before pre-processing and post-\nprocessing. the target brightness is calculated with the formula as shown below. \ne.g., if the image brightness index is 127, the function subtracts the image brightness from the brightness \nindex divided by the brightness value. \n(127 - brightness value)/(brightness value)) \nwhere brightness index is the average image brightness which got by calculating the mean from the pil \npython package ‘imagestat’ library. the above formula determines how much brightness needs to be \nincreased to reach the desired level of 127 target value.  \n \nfigure 19 final percentage improvement. \n \n \n \nfigure 20 calculating the error rates. \n \nhypothesis: \n• \nerror rates improve majorly on optimal brightness 127 \n• \nthe weighted total average of 127 sees major changes.  \nin this way, a jump from 6 to 14% error rate improvement has been noticed.",
            "page": null,
            "goal": "week 12: \n \nthe goal is to redo the last week's work as the sample dataset required for the initial analysis of finding \nthe optimal brightness, it has been found that the optimal target brightness could be improved by \nselecting with more reasoning.  \n \n• \ndefine the optimal target brightness. \n• \nuse a sample set of 50 with different starting brightness from 30 to 150. \n• \nperform brightness enhancement on pitch-dark images to find out if the black image happened \ndue to a lens/phone issue while the user taking the document photo.  \n \nactivities:  for the pitch-black image it has been found that it is not possible to increase the brightness \nvalue of a completely black (0 value) image. since there is no data to amplify or increase. brightness \nadjustments can be made to images that have existing pixel values representing different levels of \nbrightness.  \n \n \nfigure 18 analysing of the pitch black document images. \n \nthere was 2 image in each dataset with a brightness index value starting at 35 before enhancement \nincreasing the target brightness to 127 resulted in the not expected result. thus concluded that the \ndocument was not scanned properly by the user in the beginning and there have some other issues like \nlens issues and phone software-related issues in dealing with a dataset of pitch-black images received. \n \nanother task is to increase the target brightness with the respective starting brightness, the respective \ncolumn represents the error rate percentage comparing the image before pre-processing and post-\nprocessing. the target brightness is calculated with the formula as shown below. \ne.g., if the image brightness index is 127, the function subtracts the image brightness from the brightness \nindex divided by the brightness value. \n(127 - brightness value)/(brightness value)) \nwhere brightness index is the average image brightness which got by calculating the mean from the pil \npython package ‘imagestat’ library. the above formula determines how much brightness needs to be \nincreased to reach the desired level of 127 target value.  \n \nfigure 19 final percentage improvement. \n \n \n \nfigure 20 calculating the error rates. \n \nhypothesis: \n• \nerror rates improve majorly on optimal brightness 127 \n• \nthe weighted total average of 127 sees major changes.  \nin this way, a jump from 6 to 14% error rate improvement has been noticed.",
            "children": []
        },
        {
            "id": "1.25",
            "name": "week 13",
            "nodeType": "title",
            "text": "week 13",
            "page": null,
            "goal": "week 13",
            "children": []
        },
        {
            "id": "1.26",
            "name": "week 13: the",
            "nodeType": "paragraph",
            "text": "week 13: \n \nthe goal of this week is to rectify the manual error in the error rate analysis and generate the hypothesis \nfrom the inference from the table. each of the below-starting brightness error rates from 25-50,50-75,75-\n100,100-125 and above is calculated the below way. \n \n \n \nfigure 21 overall percentage improvement. \nanalysis: from the analysis it has been found that enhancing an image beyond 100 doesn’t add more value \nand sometimes the error rate improvement declines to an extent. so further apply the enhancement to \nthe image with brightness less than <50 as an index. \nfurther from the task i have also helped the organization a glimpse into the analysis of the market \nperformance by fetching the data curated from the search console and providing insights on the market \ntrends and keywords to focus to build brand awareness and further lead generation.  \n \nfigure 22 keyword analysis of the company. \nas this was the last week of the internship there was a review/ feedback session organized by the company \nsupervisor where he has given insights about the performance of the organization so far in detail and \nsuggestions to go ahead and mentioned to summarise the overall work done on the confluence \ndocumentation page of the organization.",
            "page": null,
            "goal": "week 13: \n \nthe goal of this week is to rectify the manual error in the error rate analysis and generate the hypothesis \nfrom the inference from the table. each of the below-starting brightness error rates from 25-50,50-75,75-\n100,100-125 and above is calculated the below way. \n \n \n \nfigure 21 overall percentage improvement. \nanalysis: from the analysis it has been found that enhancing an image beyond 100 doesn’t add more value \nand sometimes the error rate improvement declines to an extent. so further apply the enhancement to \nthe image with brightness less than <50 as an index. \nfurther from the task i have also helped the organization a glimpse into the analysis of the market \nperformance by fetching the data curated from the search console and providing insights on the market \ntrends and keywords to focus to build brand awareness and further lead generation.  \n \nfigure 22 keyword analysis of the company. \nas this was the last week of the internship there was a review/ feedback session organized by the company \nsupervisor where he has given insights about the performance of the organization so far in detail and \nsuggestions to go ahead and mentioned to summarise the overall work done on the confluence \ndocumentation page of the organization.",
            "children": []
        }
    ]
}