{
    "id": "1",
    "name": "Reflective journal Entities",
    "nodeType": "section",
    "text": "week 1\nin the first week of my internship, i was formally onboarded. i was asked \nto read and confirm that i understood the security procedures of the \ncompany. i was provided access to their aws system, specifically the \nsagemaker jupyter notebook instances where i would be able to develop \ncode, access the datasets and execute new notebook implementations. \ni was part of the ‘liveness’ team, which consisted of ms le nga ho, ms \njennifer box and myself. our supervisor was ms matineh poushide. our team \nwas to have three standup meetings a week to make sure the work assigned \nto us was progressing at an acceptable pace, and to account for any delays \nthat were anticipated. the ceo, mr mike simpson would also attend most, if \nnot all these standup meetings.\nthe current state of the project was that the liveness ml model was not \nperforming well in real-life deployment when tested on different users, in \ndifferent lighting conditions, and in different scenarios. moreover, the \nliveness model was not performing well in its primary task of binary \nclassification: judging if the image was of a live person or a presentation \nattack. \nthe liveness team suspected that either the training data size was not \nsufficient for effective learning, or the training labels “live” and “fake” were \nnot representative of real life scenarios such as when users are in difficult \nlighting conditions or when a photo is used as a presentation attack.\nto solve this problem, the company had set about collecting their own \nimage dataset for training purposes. when i began my internship, the \ncompany had collected 700 live and 700 fake images manually. \nthe specifics of the project were detailed to me — my task would be to \nresearch the state of the art in gans and examine their viability in our \ninternship report (final)\npage \n of \n16\n40\nbusiness problem. afterwards we were required to conduct an associated \nliterature review where a similar method was used to enhance the learning \nand performance of machine learning models when the dataset was \ninsufficient.\nafter the research phase, we would begin the long work of reproducing \nthe work outlined in our chosen research paper(s) by generating the training \nweights of the gans we had selected for our business problem and use them \non the company’s dataset to enlarge the image data.\nin the next step, we would validate the images that were generated by a \nchosen metric to judge whether it would be acceptable to use it for model \ntraining. finally, using the enlarged dataset, we would test how well the \nmodel improves in its classification tasks compared to the baseline accuracy \nperformance.\nit was emphasised to us that this was an experimental task, and all \nexperiments require documentation. therefore, we were strongly \nencouraged to write a report about our research, experiments, and failures in \nthe company’s confluence page.\nbefore the week had ended, i was deep into research papers about the \nstate of the art gans used today, and the more recent development in their \nimprovement. as per instructions, i would document my work in the \nconfluence page.\nweek 2\nin week 2, i continued my research on the state-of-the-art in gans, this \ntime focusing on the gans that were conditional, i.e changing the facial \ncharacteristics based on age, race and gender. i also focused on gans that \nfocused on image quality and background. this week i was focused on \nwriting up my findings on confluence and in doing so, i learned a lot about \nthe architecture of gans and how they work. \ninternship report (final)\npage \n of \n17\n40\nduring one of our stand up meetings, we were asked to focus on those \ngans that have publicly available code and if possible, their associated \nweights. this was requested so as to save time training and writing code, \nespecially if the instructions in the paper were vague or unclear. since the \nliterature review might be read by people who did not want to get \nentrenched in the technical details of various gan architecture, i avoided \ntechnical details and focused on the approach that each author took and \ntheir respective advantage and shortcomings. i also included other details i \nthought were relevant so that future work on this topic would not require a \nfurther literature review about the topic of gans.\ni did this work alongside ms ho and the two of us kept in contact about \nthe progress of our work with regular stand up meetings where we made \nsure that the tasks due that week were on track. \nby the end of the week, we had compiled a literature review of the gans \nthat we believed were the state-of-the-art and summarised their high level \nideas. the final standup of the week with the team involved a summary of our \nwork, and overview of the best performing gans currently in literature. \ni found these standup meetings helpful for a unique reason. since the \nceo was not well versed in the technical detail of gans, i truly had to \nunderstand the concept and the different gan approaches in order to \nexplain the overall high level concept in a manner that was straightforward \nand not convoluted or loaded with unnecessary detail. before every stand up \nmeeting, i would prepare what i wanted to say, and sometimes kept notes \nabout specific gan architectures that i wanted to draw his attention to.\nfor his part, the ceo was very quick to catch onto the details and \nconcepts and was always steering us to the path we were originally set on, \nrather than getting lost in the weeds.\ninternship report (final)\npage \n of \n18\n40\nweek 3\nin week 3, i decided to focus on the gans that conditionally age the \nfacial features of a given image, while ms le focused on the gans that \nconditionally age the racial characteristics. my supervisor, ms poushide, \napproved my work for the week and i set about cloning the public code of \ntwo age-conditional gans that i had documented and believed to be of good \nquality. one of these codebases were the official work of the authors; the \nother was an interpretation of the paper’s instructions.\nright from the start, there were a few problems i encountered. the \nunofficial codebase implementation of one paper had multiple instances of \nincorrect python syntax and linting. the official codebase of the other gan \npaper was written in tensorflow version 1, which is now depreciated. \nordinarily, running an old tensorflow version with pip and the python virtual \nenvironment, venv, would have sidestepped this issue, but the company’s \naws sagemaker instance did not support any version of tensorflow below \n2.0. \ni informed my supervisor of this issue, and told her that i would begin \nwork on upgrading the official codebase from tensorflow 1.0 to tensorflow \n2.0. \nmy work in week 3 became glacial since my attention was refocused into \nsubmitting an assignment for another unit in my coursework. however, even \nduring this busy time, i was able to identify and collect various age-labelled \nfacial datasets and store them on the company’s aws instance for further \nprocessing.\nfor this, i also wrote a small script to download the compressed datasets \nfrom their respective repositories using the gnu wget package and unzipped \nthem for storage on the aws instance. i also wrote a script to tag age labels \nto faces for one dataset that did not have explicit age labelling, rather the \nage information was stored as metadata on the images. the script extracted \ninternship report (final)\npage \n of \n19\n40\nthe required age information and generated a label for the respective face \nimage.\ntowards the end of the week, the standup meeting with the team \nrevealed that we did not have a standardised way to measure the quality of \nimages generated by the gan. i realised this was an oversight on my part, \nbecause i did not document the various image quality metrics that each gan \npaper used. i promised that i would add this information to the confluence \npage as soon as possible. over the weekend, ms ho and i reviewed the \nmetrics used by each paper and added the pertinent information.\nthis actually revealed a common metric across all papers, which is the \nfréchet inception distance (fid) that measures the difference between two \nimages. we decided that this is also the metric we should use to judge the \nimages that we generate, and presented this recommendation during the last \nstand up meeting of the week.\nweek 4\nin week 4, the liveness team had our usual standup meeting on the first \nday of the week. i informed the team that while i was working on refining the \nage-conditional gan code, i had a new approach i wanted to propose. \ni informed the team that instead of focusing on feature-specific gans, \nwe should instead choose one state-of-the-art gan to enlarge the current \ndataset. this would ensure that the images generated would be of \nacceptable quality but on the other hand, there would be no improvement in \nthe diversity of the images, i.e age, race and gender. i theorised that since \nthe model has a certain performance rate with the current dataset, \nenlargement of this data with one gan would certainly improve the \nperformance.\nin my judgement, there was no evidence to suggest that the liveness \nmodel was performing poorly due to the lack of diversity in the data, rather \nthat the dataset was too small to begin with. i took up almost all the time in \ninternship report (final)\npage \n of \n20\n40\nthe meeting explaining how the gan generated image was actually the ‘in-\nbetween’ of the training images, and this approach by itself would make the \nmodel learn distinct underlying features. i also stressed that conditional \ngans, i.e the gans that focus on augmenting specific facial features like \nrace, age and gender are not known to generate truly realistic images. my \nconcern was that these images might cause the liveness model to learn \npoorly. \nthe team accepted my proposal with a condition — i would conduct \nfurther literature review on my proposed approach to study, document and \nreplicate similar work that other researchers had done. \nthis condition was set so as to avoid ‘reinventing the wheel’. this makes \nsense, because the company did not want to waste my time and theirs \nexperimenting with a new and unproven solution. \ntherefore, i was asked to find a research paper (one or more) that had \ndone similar work and only then would we use that approach to replicate the \nwork. i proposed that we also do a proof-of-concept demonstration using my \napproach so that we would be able to determine if this was in fact, the \ncorrect course of action and to change track if the results were not \nsatisfactory.\nthe team was satisfied the progress we were making so far, and i set \nabout finding a research paper that fit my proposed solution.\nduring the last stand up meeting of the week, i was asked to consult on \nanother project at truuth — document verification. the document \nverification team were considering the use of gans to augment their own \ntraining dataset and they were interested in my proposed approach. i \nexplained the overall concept, my proposed solution to the problem we were \ntasked with, and promised to follow up with a written report. \ni felt some personal validation that my ideas were being considered \nseriously and also being asked to consult with another team. i felt for the first \ninternship report (final)\npage \n of \n21\n40\ntime that i was part of a unique and cutting edge project and my (mostly) \ntheoretical data science knowledge was being transferred over to real life \nprojects.\nweek 5\ncontinuing on my work from last week, i had to find and summarise prior \nresearch that demonstrated that enlarging an image dataset using gans \nwould improve the performance of an image recognition model. conversely, \ni was looking for a research paper that would demonstrate that improving the \nunderlying racial and age distribution of the data would make a significant \nimpact on the performance of a model than if we simply enlarged the dataset \nwith an unconditional gan.\nmy research yielded answers to both questions. image recognition \nmodels perform approximately 1% better on diversely distributed data than \non imbalanced facial data. however, a few cutting-edge research papers also \nshowed that use of stylegan2 to diversify the image data could also \nenhance learning by adding an additional training framework to generate \nracially distributed image classes.\ni compiled my research into a new confluence page and presented my \nrecommendations to the team and the ceo. i recommended that we should \ngrow our dataset unconditionally with stylegan2 first. in parallel, we would \nalso grow the dataset whilst controlling the distribution of the racial and age \ngroups.\nthe model would learn three different instances of the data: baseline, i.e \nthe given dataset, the enlarged data via stylegan2 and, the enlarged data \nwith racial and age diverse groups. then, we could test the model training \nagainst unseen facial data and judge their individual performance.\nthis would allow for comparative accuracy scores to be taken and would \nhelp the company, and us, decide what approach to continue on in the \ninternship report (final)\npage \n of \n22\n40\nfuture. ms ho and i presented this proposal jointly to the ceo and ms \npoushide, and they were satisfied with our research and recommendation. \nweek 6\nin week 6, i presented another potential problem i encountered during \nmy research. gan image generation would inevitably perform poorly when \nthe training set was small, and our dataset, by all definitions, was small.  even \nthe state of the art stylegan2 would not generate high quality images if the \nstarting set of images were not large enough. \nthis lead me to research the dataset size problem and i found two \nacceptable solutions. one solution was to use mit’s differentiable \naugmentation (diffaugment) optimisation method, and the other was to use \nnvidia research's adaptive discriminator augmentation (ada). both of these \nsolutions claim to fix this problem and generate high quality images from \nsmall training samples. \ni chose the ada solution since nvidia research is also the author of \nstylegan2, and thankfully the ada-stylegan2 code and training weights \nwere publicly available for research purposes.\nthus, i proposed we use ada-stylegan2 for our experiments in image \ngeneration. as usual, i proceeded to document my work in confluence. \nduring the weekly meetings with the ceo and the team, i outlined my work \nand promised that we would soon begin a trial run of the image generation.\nso far, neither the ceo nor ms poushide raised an issue about the speed \nof our progress. we were still in the research phase and i was anxious to start \nthe experimental work. however, upon reflection, i realised that several hours \nof meaningful research saved many more hours of wasted effort. for this, i \nam very thankful to the ceo for his constant involvement in our project to \nsteer us to the correct path. he was not insisting that we not waste company \ntime, but rather that we not waste our time.\ninternship report (final)\npage \n of \n23\n40\nby the end of the week, i was finalising my literature review and \npreparing for the final guidelines from ms poushide about next steps we \nwould take for week 7.\nweek 7\nthe team discussed potential delays in week 7 due to university \nobligations like assignments, and our intention to attend an ibm sponsored \nhackathon, which would leave us little to no time to work on the project from \nthe company. \nwe had discussed using stylegan2-ada in the previous week’s \ndiscussions, and i had delegated this task to my co-worker le nga to perform \ntraining on our dataset. i initially planned to work on the previously \nmentioned conditional gan codebases to work out the bugs and start \ntraining in parallel, but in reality, we could work on this for at most one day of \nthe week. \nmike and matineh were very understanding with regard to the delay and \naccepted that the work would have to be pushed back. as a side note, our \nteam, which comprised of le nga, jennifer, myself, and another classmate, \nwon 2nd place at said hackathon. \nweek 8\nat the start of week 8, i was able to write some code for stylegan2-ada \nand put them on our aws instance. the next steps would involve feeding the \nimage data into the model and perform training for up to 10 hours. \nunfortunately, before this step could be started, i contracted covid-19 \nand was unable to do any work for the full duration of the week. at the same \ntime, le nga contracted the flu and was also unable to work through her \ntasks. we notified our supervisor, and she was very accomodating and told \nus to take the time we needed to get better. \ninternship report (final)\npage \n of \n24\n40\ntowards the tail end of the week i was able to get some work done, \nwhich was to import the data contained in the aws s3 buckets over to our \nsagemaker instance. \nthis week was very unproductive since i was unable to make any \nprogress in image generation and i was feeling concerned about the \ndeadlines and results we had promised. i did not know if we could even \ngenerate good results, let alone high quality images that the company could \nuse. \nweek 9\nin week 9, i was able to complete the code that i had started in week 7, \ni.e the code for training images using stylegan2-ada. this codebase, too, \nwas dependent on tensorflow version 1, and aws sagemaker does not \nsupport older tensorflow versions. i was able to work around this issue by \nusing different kernels, but the gpu cluster that we were working with did not \nsupport depreciated libraries. \ni conducted a private call with matineh and le nga, and we attempted to \nwork through the errors, and after an hours long meeting, we concluded that \nit was not feasible to use the tensorflow version of stylegan2 with our \nenvironment. there was a pytorch implementation of the same codebase, \nand we had no choice but to use that instead. no one on the team had \nexperience with pytorch, so it was up to us to learn it on the go and deal with \nthe situation as it was. \nthe team agreed to implement the pytorch version by the end of this \nweek, and show some samples if possible by next week. \nduring one of our meetings, mike brought up a problem that the \ndocument verification team were facing and was wondering if we had any \nideas that he could take back to them. they were facing issues with blurry \ndocuments being sent by clients that were partially or totally unreadable by \ninternship report (final)\npage \n of \n25\n40\nthe ocr service. we did not have suggestions at the time but promised we \nwould look into it and get back to him if we found a concrete solution. \nle nga agreed to work on the pytorch version of the codebase and i \nwould again try to work on the conditional gans to control the age and \ngender of the generated images. \nweek 10\nin week 10, we made a massive leap of progress that exceeded matineh’s \nexpectations. she had independently trained an age-conditional gan on a \npublic database of images but the sample images generated were low in \nresolution and could not be reliably used for training purposes. \ni had come across a few research papers in week 2 that described how \ngans can be used to upscale low resolution images. this technique was \ncolloquially known as super resolution. i was very grateful to one of these \npaper’s authors who had made their training weights available publicly so we \ncould directly perform inference. the results from inference were extremely \npromising. \nby the end of this week i had developed a pipeline to pass images \nthrough the inferencing process, which would automatically generate a \ndataset of upscaled, high resolution images. this lead me to explore other \npublicly available training weights that we could use for inferencing directly \non our other tasks, instead of reinventing the wheel. \ni learned that this ml technique is called “transfer learning”, and if we \nhad access to said weights for our desired tasks we could speed up our \nworkflow and we would meet our deadlines before expected. \nweek 11\nweek 11 was extremely productive for me. over the weekend, i was able \nto use the pre-trained weights of stylegan2 that were trained on flickr faces \nhigh quality (ffhq) dataset and apply them to the ageing task. during the \npresentation of my work on monday, matineh and mike were extremely \ninternship report (final)\npage \n of \n26\n40\nimpressed with the outputs. the results were incredibly realistic and of such \ngood quality that i was asked to consult with the document verification team \non the feasibility of using these images for “negative training”, i.e using these \nsamples to detect inauthentic documents that an adversarial actor might \nsubmit to our system. \ni promised that i would continue to supply them with gan generated \nimages using the pipeline i had created for their training purposes, and that \nby the end of the week, i would include documentation for this work so that it \ncould be accessible by anyone in the company. \nin another two days time, i had completed the task of gender \ntransformation using gans using a pre-trained network of male-to-female \nand female-to-male weights, also generated using stylegan2. i was able to \ngenerate excellent results on this task as well. mike and matineh were \nsimilarly very pleased with these results. \nthe next task that we needed to complete this week was to validate the \nimages, i.e whether certain landmarks in the background and the edges, \nwere being preserved. this was important, since we wanted to retain \nbackground detail and information for the liveness model to discriminate \npresentation attacks from live persons. \nthis task was entirely delegated to le nga, who has a keen eye and is \nexcellent at documenting her observations. meanwhile, i was asked to re-\nevaluate the problem that the document verification team had, i.e blurry \ndocument photos submitted by clients that ocr was failing to read. \nafter another full day of experimentation with the super resolution gan \npipeline, i concluded that this task was not something that could be suitable \nhandled by a gan, but rather by more conventional deep learning methods. \nin another day, i had applied two state-of-the-art (sota) deep learning \nsolutions to solve this problem. first, using one deep learning model, i \nperformed a retouching on these documents to correct for the low-light \ninternship report (final)\npage \n of \n27\n40\nproblem, and using a second model, i applied a de-blurring image \nrestoration model to un-blur the text on the document. \nthe performance was outstanding, exceeding the wildest expectations \nof the document verification team. previously, a poorly lit, blurry, license card \nthat received a 66% confidence score by the ocr service, now received a \n94.8% confidence score! these deep learning method were further adapted \nby me to process multiple images in a batched mode, which resulted in a \nrapid processing of image data. \nweek 12\nat the end of last week, le nga had completed her task of image \nvalidation and had concluded that it was completely reasonable to apply our \ncustom models to the dataset that we wanted to inflate. we began the \nprocess of batched inferencing using a custom scheduler to process large \namounts of data. \nin the meantime, mike had shared with me a paper by microsoft \nresearch (bae et al., 2022), where the researchers had improved facial \nrecognition models by retraining them on gan augmented data, which was \nessentially the same task we were performing, but they had additionally \nperformed other augmentations such as accessorising with sunglasses, \nchange in facial pose, hair style change etc. while it is clear why this \napproach would improve facial recognition, i did not know if this would \nimprove the liveness models since that was not my area of expertise. \nregardless, i went about searching for a solution to a “generalised gan \naugmentation”, and found a dual step solution. \nfirst, we would generate a “direction vector” from a text prompt, for \nexample, “a face with blonde hair” or “a face with a beard”. then we would \napply said direction vector to our chosen image with a specified strength of \napplication. the results were quite impressive, but there were many bugs to \nfix in the code and since this was very close to the end of our semester, i \ninternship report (final)\npage \n of \n28\n40\ninstead decided to upload the code i had worked on thus far to the aws \nsagemaker instance, and left it as a future project for the company, or for the \nnext batch of interns. \nby the end of week 12, we had also completed all the documentation \nrequested for our gan models, as well as detailed reproducibility instructions \nfor anyone who wished to tinker with the code. \ni had completed all tasks that i had promised to deliver to mike and \nmatineh, and we had officially concluded our work at the company ahead of \nschedule by a full week. \ninternship report (final)\npage \n of \n29\n40\n",
    "page": null,
    "goal": "Reflective journal Entities",
    "children": [
        {
            "id": "1.1",
            "name": "week 1",
            "nodeType": "title",
            "text": "week 1",
            "page": null,
            "goal": "week 1",
            "children": []
        },
        {
            "id": "1.2",
            "name": "week 1 in",
            "nodeType": "paragraph",
            "text": "week 1\nin the first week of my internship, i was formally onboarded. i was asked \nto read and confirm that i understood the security procedures of the \ncompany. i was provided access to their aws system, specifically the \nsagemaker jupyter notebook instances where i would be able to develop \ncode, access the datasets and execute new notebook implementations. \ni was part of the ‘liveness’ team, which consisted of ms le nga ho, ms \njennifer box and myself. our supervisor was ms matineh poushide. our team \nwas to have three standup meetings a week to make sure the work assigned \nto us was progressing at an acceptable pace, and to account for any delays \nthat were anticipated. the ceo, mr mike simpson would also attend most, if \nnot all these standup meetings.\nthe current state of the project was that the liveness ml model was not \nperforming well in real-life deployment when tested on different users, in \ndifferent lighting conditions, and in different scenarios. moreover, the \nliveness model was not performing well in its primary task of binary \nclassification: judging if the image was of a live person or a presentation \nattack. \nthe liveness team suspected that either the training data size was not \nsufficient for effective learning, or the training labels “live” and “fake” were \nnot representative of real life scenarios such as when users are in difficult \nlighting conditions or when a photo is used as a presentation attack.\nto solve this problem, the company had set about collecting their own \nimage dataset for training purposes. when i began my internship, the \ncompany had collected 700 live and 700 fake images manually. \nthe specifics of the project were detailed to me — my task would be to \nresearch the state of the art in gans and examine their viability in our \ninternship report (final)\npage \n of \n16\n40\nbusiness problem. afterwards we were required to conduct an associated \nliterature review where a similar method was used to enhance the learning \nand performance of machine learning models when the dataset was \ninsufficient.\nafter the research phase, we would begin the long work of reproducing \nthe work outlined in our chosen research paper(s) by generating the training \nweights of the gans we had selected for our business problem and use them \non the company’s dataset to enlarge the image data.\nin the next step, we would validate the images that were generated by a \nchosen metric to judge whether it would be acceptable to use it for model \ntraining. finally, using the enlarged dataset, we would test how well the \nmodel improves in its classification tasks compared to the baseline accuracy \nperformance.\nit was emphasised to us that this was an experimental task, and all \nexperiments require documentation. therefore, we were strongly \nencouraged to write a report about our research, experiments, and failures in \nthe company’s confluence page.\nbefore the week had ended, i was deep into research papers about the \nstate of the art gans used today, and the more recent development in their \nimprovement. as per instructions, i would document my work in the \nconfluence page.",
            "page": null,
            "goal": "week 1\nin the first week of my internship, i was formally onboarded. i was asked \nto read and confirm that i understood the security procedures of the \ncompany. i was provided access to their aws system, specifically the \nsagemaker jupyter notebook instances where i would be able to develop \ncode, access the datasets and execute new notebook implementations. \ni was part of the ‘liveness’ team, which consisted of ms le nga ho, ms \njennifer box and myself. our supervisor was ms matineh poushide. our team \nwas to have three standup meetings a week to make sure the work assigned \nto us was progressing at an acceptable pace, and to account for any delays \nthat were anticipated. the ceo, mr mike simpson would also attend most, if \nnot all these standup meetings.\nthe current state of the project was that the liveness ml model was not \nperforming well in real-life deployment when tested on different users, in \ndifferent lighting conditions, and in different scenarios. moreover, the \nliveness model was not performing well in its primary task of binary \nclassification: judging if the image was of a live person or a presentation \nattack. \nthe liveness team suspected that either the training data size was not \nsufficient for effective learning, or the training labels “live” and “fake” were \nnot representative of real life scenarios such as when users are in difficult \nlighting conditions or when a photo is used as a presentation attack.\nto solve this problem, the company had set about collecting their own \nimage dataset for training purposes. when i began my internship, the \ncompany had collected 700 live and 700 fake images manually. \nthe specifics of the project were detailed to me — my task would be to \nresearch the state of the art in gans and examine their viability in our \ninternship report (final)\npage \n of \n16\n40\nbusiness problem. afterwards we were required to conduct an associated \nliterature review where a similar method was used to enhance the learning \nand performance of machine learning models when the dataset was \ninsufficient.\nafter the research phase, we would begin the long work of reproducing \nthe work outlined in our chosen research paper(s) by generating the training \nweights of the gans we had selected for our business problem and use them \non the company’s dataset to enlarge the image data.\nin the next step, we would validate the images that were generated by a \nchosen metric to judge whether it would be acceptable to use it for model \ntraining. finally, using the enlarged dataset, we would test how well the \nmodel improves in its classification tasks compared to the baseline accuracy \nperformance.\nit was emphasised to us that this was an experimental task, and all \nexperiments require documentation. therefore, we were strongly \nencouraged to write a report about our research, experiments, and failures in \nthe company’s confluence page.\nbefore the week had ended, i was deep into research papers about the \nstate of the art gans used today, and the more recent development in their \nimprovement. as per instructions, i would document my work in the \nconfluence page.",
            "children": []
        },
        {
            "id": "1.3",
            "name": "week 2",
            "nodeType": "title",
            "text": "week 2",
            "page": null,
            "goal": "week 2",
            "children": []
        },
        {
            "id": "1.4",
            "name": "week 2 in",
            "nodeType": "paragraph",
            "text": "week 2\nin week 2, i continued my research on the state-of-the-art in gans, this \ntime focusing on the gans that were conditional, i.e changing the facial \ncharacteristics based on age, race and gender. i also focused on gans that \nfocused on image quality and background. this week i was focused on \nwriting up my findings on confluence and in doing so, i learned a lot about \nthe architecture of gans and how they work. \ninternship report (final)\npage \n of \n17\n40\nduring one of our stand up meetings, we were asked to focus on those \ngans that have publicly available code and if possible, their associated \nweights. this was requested so as to save time training and writing code, \nespecially if the instructions in the paper were vague or unclear. since the \nliterature review might be read by people who did not want to get \nentrenched in the technical details of various gan architecture, i avoided \ntechnical details and focused on the approach that each author took and \ntheir respective advantage and shortcomings. i also included other details i \nthought were relevant so that future work on this topic would not require a \nfurther literature review about the topic of gans.\ni did this work alongside ms ho and the two of us kept in contact about \nthe progress of our work with regular stand up meetings where we made \nsure that the tasks due that week were on track. \nby the end of the week, we had compiled a literature review of the gans \nthat we believed were the state-of-the-art and summarised their high level \nideas. the final standup of the week with the team involved a summary of our \nwork, and overview of the best performing gans currently in literature. \ni found these standup meetings helpful for a unique reason. since the \nceo was not well versed in the technical detail of gans, i truly had to \nunderstand the concept and the different gan approaches in order to \nexplain the overall high level concept in a manner that was straightforward \nand not convoluted or loaded with unnecessary detail. before every stand up \nmeeting, i would prepare what i wanted to say, and sometimes kept notes \nabout specific gan architectures that i wanted to draw his attention to.\nfor his part, the ceo was very quick to catch onto the details and \nconcepts and was always steering us to the path we were originally set on, \nrather than getting lost in the weeds.\ninternship report (final)\npage \n of \n18\n40",
            "page": null,
            "goal": "week 2\nin week 2, i continued my research on the state-of-the-art in gans, this \ntime focusing on the gans that were conditional, i.e changing the facial \ncharacteristics based on age, race and gender. i also focused on gans that \nfocused on image quality and background. this week i was focused on \nwriting up my findings on confluence and in doing so, i learned a lot about \nthe architecture of gans and how they work. \ninternship report (final)\npage \n of \n17\n40\nduring one of our stand up meetings, we were asked to focus on those \ngans that have publicly available code and if possible, their associated \nweights. this was requested so as to save time training and writing code, \nespecially if the instructions in the paper were vague or unclear. since the \nliterature review might be read by people who did not want to get \nentrenched in the technical details of various gan architecture, i avoided \ntechnical details and focused on the approach that each author took and \ntheir respective advantage and shortcomings. i also included other details i \nthought were relevant so that future work on this topic would not require a \nfurther literature review about the topic of gans.\ni did this work alongside ms ho and the two of us kept in contact about \nthe progress of our work with regular stand up meetings where we made \nsure that the tasks due that week were on track. \nby the end of the week, we had compiled a literature review of the gans \nthat we believed were the state-of-the-art and summarised their high level \nideas. the final standup of the week with the team involved a summary of our \nwork, and overview of the best performing gans currently in literature. \ni found these standup meetings helpful for a unique reason. since the \nceo was not well versed in the technical detail of gans, i truly had to \nunderstand the concept and the different gan approaches in order to \nexplain the overall high level concept in a manner that was straightforward \nand not convoluted or loaded with unnecessary detail. before every stand up \nmeeting, i would prepare what i wanted to say, and sometimes kept notes \nabout specific gan architectures that i wanted to draw his attention to.\nfor his part, the ceo was very quick to catch onto the details and \nconcepts and was always steering us to the path we were originally set on, \nrather than getting lost in the weeds.\ninternship report (final)\npage \n of \n18\n40",
            "children": []
        },
        {
            "id": "1.5",
            "name": "week 3",
            "nodeType": "title",
            "text": "week 3",
            "page": null,
            "goal": "week 3",
            "children": []
        },
        {
            "id": "1.6",
            "name": "week 3 in",
            "nodeType": "paragraph",
            "text": "week 3\nin week 3, i decided to focus on the gans that conditionally age the \nfacial features of a given image, while ms le focused on the gans that \nconditionally age the racial characteristics. my supervisor, ms poushide, \napproved my work for the week and i set about cloning the public code of \ntwo age-conditional gans that i had documented and believed to be of good \nquality. one of these codebases were the official work of the authors; the \nother was an interpretation of the paper’s instructions.\nright from the start, there were a few problems i encountered. the \nunofficial codebase implementation of one paper had multiple instances of \nincorrect python syntax and linting. the official codebase of the other gan \npaper was written in tensorflow version 1, which is now depreciated. \nordinarily, running an old tensorflow version with pip and the python virtual \nenvironment, venv, would have sidestepped this issue, but the company’s \naws sagemaker instance did not support any version of tensorflow below \n2.0. \ni informed my supervisor of this issue, and told her that i would begin \nwork on upgrading the official codebase from tensorflow 1.0 to tensorflow \n2.0. \nmy work in week 3 became glacial since my attention was refocused into \nsubmitting an assignment for another unit in my coursework. however, even \nduring this busy time, i was able to identify and collect various age-labelled \nfacial datasets and store them on the company’s aws instance for further \nprocessing.\nfor this, i also wrote a small script to download the compressed datasets \nfrom their respective repositories using the gnu wget package and unzipped \nthem for storage on the aws instance. i also wrote a script to tag age labels \nto faces for one dataset that did not have explicit age labelling, rather the \nage information was stored as metadata on the images. the script extracted \ninternship report (final)\npage \n of \n19\n40\nthe required age information and generated a label for the respective face \nimage.\ntowards the end of the week, the standup meeting with the team \nrevealed that we did not have a standardised way to measure the quality of \nimages generated by the gan. i realised this was an oversight on my part, \nbecause i did not document the various image quality metrics that each gan \npaper used. i promised that i would add this information to the confluence \npage as soon as possible. over the weekend, ms ho and i reviewed the \nmetrics used by each paper and added the pertinent information.\nthis actually revealed a common metric across all papers, which is the \nfréchet inception distance (fid) that measures the difference between two \nimages. we decided that this is also the metric we should use to judge the \nimages that we generate, and presented this recommendation during the last \nstand up meeting of the week.",
            "page": null,
            "goal": "week 3\nin week 3, i decided to focus on the gans that conditionally age the \nfacial features of a given image, while ms le focused on the gans that \nconditionally age the racial characteristics. my supervisor, ms poushide, \napproved my work for the week and i set about cloning the public code of \ntwo age-conditional gans that i had documented and believed to be of good \nquality. one of these codebases were the official work of the authors; the \nother was an interpretation of the paper’s instructions.\nright from the start, there were a few problems i encountered. the \nunofficial codebase implementation of one paper had multiple instances of \nincorrect python syntax and linting. the official codebase of the other gan \npaper was written in tensorflow version 1, which is now depreciated. \nordinarily, running an old tensorflow version with pip and the python virtual \nenvironment, venv, would have sidestepped this issue, but the company’s \naws sagemaker instance did not support any version of tensorflow below \n2.0. \ni informed my supervisor of this issue, and told her that i would begin \nwork on upgrading the official codebase from tensorflow 1.0 to tensorflow \n2.0. \nmy work in week 3 became glacial since my attention was refocused into \nsubmitting an assignment for another unit in my coursework. however, even \nduring this busy time, i was able to identify and collect various age-labelled \nfacial datasets and store them on the company’s aws instance for further \nprocessing.\nfor this, i also wrote a small script to download the compressed datasets \nfrom their respective repositories using the gnu wget package and unzipped \nthem for storage on the aws instance. i also wrote a script to tag age labels \nto faces for one dataset that did not have explicit age labelling, rather the \nage information was stored as metadata on the images. the script extracted \ninternship report (final)\npage \n of \n19\n40\nthe required age information and generated a label for the respective face \nimage.\ntowards the end of the week, the standup meeting with the team \nrevealed that we did not have a standardised way to measure the quality of \nimages generated by the gan. i realised this was an oversight on my part, \nbecause i did not document the various image quality metrics that each gan \npaper used. i promised that i would add this information to the confluence \npage as soon as possible. over the weekend, ms ho and i reviewed the \nmetrics used by each paper and added the pertinent information.\nthis actually revealed a common metric across all papers, which is the \nfréchet inception distance (fid) that measures the difference between two \nimages. we decided that this is also the metric we should use to judge the \nimages that we generate, and presented this recommendation during the last \nstand up meeting of the week.",
            "children": []
        },
        {
            "id": "1.7",
            "name": "week 4",
            "nodeType": "title",
            "text": "week 4",
            "page": null,
            "goal": "week 4",
            "children": []
        },
        {
            "id": "1.8",
            "name": "week 4 in",
            "nodeType": "paragraph",
            "text": "week 4\nin week 4, the liveness team had our usual standup meeting on the first \nday of the week. i informed the team that while i was working on refining the \nage-conditional gan code, i had a new approach i wanted to propose. \ni informed the team that instead of focusing on feature-specific gans, \nwe should instead choose one state-of-the-art gan to enlarge the current \ndataset. this would ensure that the images generated would be of \nacceptable quality but on the other hand, there would be no improvement in \nthe diversity of the images, i.e age, race and gender. i theorised that since \nthe model has a certain performance rate with the current dataset, \nenlargement of this data with one gan would certainly improve the \nperformance.\nin my judgement, there was no evidence to suggest that the liveness \nmodel was performing poorly due to the lack of diversity in the data, rather \nthat the dataset was too small to begin with. i took up almost all the time in \ninternship report (final)\npage \n of \n20\n40\nthe meeting explaining how the gan generated image was actually the ‘in-\nbetween’ of the training images, and this approach by itself would make the \nmodel learn distinct underlying features. i also stressed that conditional \ngans, i.e the gans that focus on augmenting specific facial features like \nrace, age and gender are not known to generate truly realistic images. my \nconcern was that these images might cause the liveness model to learn \npoorly. \nthe team accepted my proposal with a condition — i would conduct \nfurther literature review on my proposed approach to study, document and \nreplicate similar work that other researchers had done. \nthis condition was set so as to avoid ‘reinventing the wheel’. this makes \nsense, because the company did not want to waste my time and theirs \nexperimenting with a new and unproven solution. \ntherefore, i was asked to find a research paper (one or more) that had \ndone similar work and only then would we use that approach to replicate the \nwork. i proposed that we also do a proof-of-concept demonstration using my \napproach so that we would be able to determine if this was in fact, the \ncorrect course of action and to change track if the results were not \nsatisfactory.\nthe team was satisfied the progress we were making so far, and i set \nabout finding a research paper that fit my proposed solution.\nduring the last stand up meeting of the week, i was asked to consult on \nanother project at truuth — document verification. the document \nverification team were considering the use of gans to augment their own \ntraining dataset and they were interested in my proposed approach. i \nexplained the overall concept, my proposed solution to the problem we were \ntasked with, and promised to follow up with a written report. \ni felt some personal validation that my ideas were being considered \nseriously and also being asked to consult with another team. i felt for the first \ninternship report (final)\npage \n of \n21\n40\ntime that i was part of a unique and cutting edge project and my (mostly) \ntheoretical data science knowledge was being transferred over to real life \nprojects.",
            "page": null,
            "goal": "week 4\nin week 4, the liveness team had our usual standup meeting on the first \nday of the week. i informed the team that while i was working on refining the \nage-conditional gan code, i had a new approach i wanted to propose. \ni informed the team that instead of focusing on feature-specific gans, \nwe should instead choose one state-of-the-art gan to enlarge the current \ndataset. this would ensure that the images generated would be of \nacceptable quality but on the other hand, there would be no improvement in \nthe diversity of the images, i.e age, race and gender. i theorised that since \nthe model has a certain performance rate with the current dataset, \nenlargement of this data with one gan would certainly improve the \nperformance.\nin my judgement, there was no evidence to suggest that the liveness \nmodel was performing poorly due to the lack of diversity in the data, rather \nthat the dataset was too small to begin with. i took up almost all the time in \ninternship report (final)\npage \n of \n20\n40\nthe meeting explaining how the gan generated image was actually the ‘in-\nbetween’ of the training images, and this approach by itself would make the \nmodel learn distinct underlying features. i also stressed that conditional \ngans, i.e the gans that focus on augmenting specific facial features like \nrace, age and gender are not known to generate truly realistic images. my \nconcern was that these images might cause the liveness model to learn \npoorly. \nthe team accepted my proposal with a condition — i would conduct \nfurther literature review on my proposed approach to study, document and \nreplicate similar work that other researchers had done. \nthis condition was set so as to avoid ‘reinventing the wheel’. this makes \nsense, because the company did not want to waste my time and theirs \nexperimenting with a new and unproven solution. \ntherefore, i was asked to find a research paper (one or more) that had \ndone similar work and only then would we use that approach to replicate the \nwork. i proposed that we also do a proof-of-concept demonstration using my \napproach so that we would be able to determine if this was in fact, the \ncorrect course of action and to change track if the results were not \nsatisfactory.\nthe team was satisfied the progress we were making so far, and i set \nabout finding a research paper that fit my proposed solution.\nduring the last stand up meeting of the week, i was asked to consult on \nanother project at truuth — document verification. the document \nverification team were considering the use of gans to augment their own \ntraining dataset and they were interested in my proposed approach. i \nexplained the overall concept, my proposed solution to the problem we were \ntasked with, and promised to follow up with a written report. \ni felt some personal validation that my ideas were being considered \nseriously and also being asked to consult with another team. i felt for the first \ninternship report (final)\npage \n of \n21\n40\ntime that i was part of a unique and cutting edge project and my (mostly) \ntheoretical data science knowledge was being transferred over to real life \nprojects.",
            "children": []
        },
        {
            "id": "1.9",
            "name": "week 5",
            "nodeType": "title",
            "text": "week 5",
            "page": null,
            "goal": "week 5",
            "children": []
        },
        {
            "id": "1.10",
            "name": "week 5 continuing",
            "nodeType": "paragraph",
            "text": "week 5\ncontinuing on my work from last week, i had to find and summarise prior \nresearch that demonstrated that enlarging an image dataset using gans \nwould improve the performance of an image recognition model. conversely, \ni was looking for a research paper that would demonstrate that improving the \nunderlying racial and age distribution of the data would make a significant \nimpact on the performance of a model than if we simply enlarged the dataset \nwith an unconditional gan.\nmy research yielded answers to both questions. image recognition \nmodels perform approximately 1% better on diversely distributed data than \non imbalanced facial data. however, a few cutting-edge research papers also \nshowed that use of stylegan2 to diversify the image data could also \nenhance learning by adding an additional training framework to generate \nracially distributed image classes.\ni compiled my research into a new confluence page and presented my \nrecommendations to the team and the ceo. i recommended that we should \ngrow our dataset unconditionally with stylegan2 first. in parallel, we would \nalso grow the dataset whilst controlling the distribution of the racial and age \ngroups.\nthe model would learn three different instances of the data: baseline, i.e \nthe given dataset, the enlarged data via stylegan2 and, the enlarged data \nwith racial and age diverse groups. then, we could test the model training \nagainst unseen facial data and judge their individual performance.\nthis would allow for comparative accuracy scores to be taken and would \nhelp the company, and us, decide what approach to continue on in the \ninternship report (final)\npage \n of \n22\n40\nfuture. ms ho and i presented this proposal jointly to the ceo and ms \npoushide, and they were satisfied with our research and recommendation.",
            "page": null,
            "goal": "week 5\ncontinuing on my work from last week, i had to find and summarise prior \nresearch that demonstrated that enlarging an image dataset using gans \nwould improve the performance of an image recognition model. conversely, \ni was looking for a research paper that would demonstrate that improving the \nunderlying racial and age distribution of the data would make a significant \nimpact on the performance of a model than if we simply enlarged the dataset \nwith an unconditional gan.\nmy research yielded answers to both questions. image recognition \nmodels perform approximately 1% better on diversely distributed data than \non imbalanced facial data. however, a few cutting-edge research papers also \nshowed that use of stylegan2 to diversify the image data could also \nenhance learning by adding an additional training framework to generate \nracially distributed image classes.\ni compiled my research into a new confluence page and presented my \nrecommendations to the team and the ceo. i recommended that we should \ngrow our dataset unconditionally with stylegan2 first. in parallel, we would \nalso grow the dataset whilst controlling the distribution of the racial and age \ngroups.\nthe model would learn three different instances of the data: baseline, i.e \nthe given dataset, the enlarged data via stylegan2 and, the enlarged data \nwith racial and age diverse groups. then, we could test the model training \nagainst unseen facial data and judge their individual performance.\nthis would allow for comparative accuracy scores to be taken and would \nhelp the company, and us, decide what approach to continue on in the \ninternship report (final)\npage \n of \n22\n40\nfuture. ms ho and i presented this proposal jointly to the ceo and ms \npoushide, and they were satisfied with our research and recommendation.",
            "children": []
        },
        {
            "id": "1.11",
            "name": "week 6",
            "nodeType": "title",
            "text": "week 6",
            "page": null,
            "goal": "week 6",
            "children": []
        },
        {
            "id": "1.12",
            "name": "week 6 in",
            "nodeType": "paragraph",
            "text": "week 6\nin week 6, i presented another potential problem i encountered during \nmy research. gan image generation would inevitably perform poorly when \nthe training set was small, and our dataset, by all definitions, was small.  even \nthe state of the art stylegan2 would not generate high quality images if the \nstarting set of images were not large enough. \nthis lead me to research the dataset size problem and i found two \nacceptable solutions. one solution was to use mit’s differentiable \naugmentation (diffaugment) optimisation method, and the other was to use \nnvidia research's adaptive discriminator augmentation (ada). both of these \nsolutions claim to fix this problem and generate high quality images from \nsmall training samples. \ni chose the ada solution since nvidia research is also the author of \nstylegan2, and thankfully the ada-stylegan2 code and training weights \nwere publicly available for research purposes.\nthus, i proposed we use ada-stylegan2 for our experiments in image \ngeneration. as usual, i proceeded to document my work in confluence. \nduring the weekly meetings with the ceo and the team, i outlined my work \nand promised that we would soon begin a trial run of the image generation.\nso far, neither the ceo nor ms poushide raised an issue about the speed \nof our progress. we were still in the research phase and i was anxious to start \nthe experimental work. however, upon reflection, i realised that several hours \nof meaningful research saved many more hours of wasted effort. for this, i \nam very thankful to the ceo for his constant involvement in our project to \nsteer us to the correct path. he was not insisting that we not waste company \ntime, but rather that we not waste our time.\ninternship report (final)\npage \n of \n23\n40\nby the end of the week, i was finalising my literature review and \npreparing for the final guidelines from ms poushide about next steps we \nwould take for",
            "page": null,
            "goal": "week 6\nin week 6, i presented another potential problem i encountered during \nmy research. gan image generation would inevitably perform poorly when \nthe training set was small, and our dataset, by all definitions, was small.  even \nthe state of the art stylegan2 would not generate high quality images if the \nstarting set of images were not large enough. \nthis lead me to research the dataset size problem and i found two \nacceptable solutions. one solution was to use mit’s differentiable \naugmentation (diffaugment) optimisation method, and the other was to use \nnvidia research's adaptive discriminator augmentation (ada). both of these \nsolutions claim to fix this problem and generate high quality images from \nsmall training samples. \ni chose the ada solution since nvidia research is also the author of \nstylegan2, and thankfully the ada-stylegan2 code and training weights \nwere publicly available for research purposes.\nthus, i proposed we use ada-stylegan2 for our experiments in image \ngeneration. as usual, i proceeded to document my work in confluence. \nduring the weekly meetings with the ceo and the team, i outlined my work \nand promised that we would soon begin a trial run of the image generation.\nso far, neither the ceo nor ms poushide raised an issue about the speed \nof our progress. we were still in the research phase and i was anxious to start \nthe experimental work. however, upon reflection, i realised that several hours \nof meaningful research saved many more hours of wasted effort. for this, i \nam very thankful to the ceo for his constant involvement in our project to \nsteer us to the correct path. he was not insisting that we not waste company \ntime, but rather that we not waste our time.\ninternship report (final)\npage \n of \n23\n40\nby the end of the week, i was finalising my literature review and \npreparing for the final guidelines from ms poushide about next steps we \nwould take for",
            "children": []
        },
        {
            "id": "1.13",
            "name": "week 7",
            "nodeType": "title",
            "text": "week 7",
            "page": null,
            "goal": "week 7",
            "children": []
        },
        {
            "id": "1.14",
            "name": "week 7. week",
            "nodeType": "paragraph",
            "text": "week 7.\nweek 7\nthe team discussed potential delays in week 7 due to university \nobligations like assignments, and our intention to attend an ibm sponsored \nhackathon, which would leave us little to no time to work on the project from \nthe company. \nwe had discussed using stylegan2-ada in the previous week’s \ndiscussions, and i had delegated this task to my co-worker le nga to perform \ntraining on our dataset. i initially planned to work on the previously \nmentioned conditional gan codebases to work out the bugs and start \ntraining in parallel, but in reality, we could work on this for at most one day of \nthe week. \nmike and matineh were very understanding with regard to the delay and \naccepted that the work would have to be pushed back. as a side note, our \nteam, which comprised of le nga, jennifer, myself, and another classmate, \nwon 2nd place at said hackathon.",
            "page": null,
            "goal": "week 7.\nweek 7\nthe team discussed potential delays in week 7 due to university \nobligations like assignments, and our intention to attend an ibm sponsored \nhackathon, which would leave us little to no time to work on the project from \nthe company. \nwe had discussed using stylegan2-ada in the previous week’s \ndiscussions, and i had delegated this task to my co-worker le nga to perform \ntraining on our dataset. i initially planned to work on the previously \nmentioned conditional gan codebases to work out the bugs and start \ntraining in parallel, but in reality, we could work on this for at most one day of \nthe week. \nmike and matineh were very understanding with regard to the delay and \naccepted that the work would have to be pushed back. as a side note, our \nteam, which comprised of le nga, jennifer, myself, and another classmate, \nwon 2nd place at said hackathon.",
            "children": []
        },
        {
            "id": "1.15",
            "name": "week 8",
            "nodeType": "title",
            "text": "week 8",
            "page": null,
            "goal": "week 8",
            "children": []
        },
        {
            "id": "1.16",
            "name": "week 8 at",
            "nodeType": "paragraph",
            "text": "week 8\nat the start of week 8, i was able to write some code for stylegan2-ada \nand put them on our aws instance. the next steps would involve feeding the \nimage data into the model and perform training for up to 10 hours. \nunfortunately, before this step could be started, i contracted covid-19 \nand was unable to do any work for the full duration of the week. at the same \ntime, le nga contracted the flu and was also unable to work through her \ntasks. we notified our supervisor, and she was very accomodating and told \nus to take the time we needed to get better. \ninternship report (final)\npage \n of \n24\n40\ntowards the tail end of the week i was able to get some work done, \nwhich was to import the data contained in the aws s3 buckets over to our \nsagemaker instance. \nthis week was very unproductive since i was unable to make any \nprogress in image generation and i was feeling concerned about the \ndeadlines and results we had promised. i did not know if we could even \ngenerate good results, let alone high quality images that the company could \nuse.",
            "page": null,
            "goal": "week 8\nat the start of week 8, i was able to write some code for stylegan2-ada \nand put them on our aws instance. the next steps would involve feeding the \nimage data into the model and perform training for up to 10 hours. \nunfortunately, before this step could be started, i contracted covid-19 \nand was unable to do any work for the full duration of the week. at the same \ntime, le nga contracted the flu and was also unable to work through her \ntasks. we notified our supervisor, and she was very accomodating and told \nus to take the time we needed to get better. \ninternship report (final)\npage \n of \n24\n40\ntowards the tail end of the week i was able to get some work done, \nwhich was to import the data contained in the aws s3 buckets over to our \nsagemaker instance. \nthis week was very unproductive since i was unable to make any \nprogress in image generation and i was feeling concerned about the \ndeadlines and results we had promised. i did not know if we could even \ngenerate good results, let alone high quality images that the company could \nuse.",
            "children": []
        },
        {
            "id": "1.17",
            "name": "week 9",
            "nodeType": "title",
            "text": "week 9",
            "page": null,
            "goal": "week 9",
            "children": []
        },
        {
            "id": "1.18",
            "name": "week 9 in",
            "nodeType": "paragraph",
            "text": "week 9\nin week 9, i was able to complete the code that i had started in week 7, \ni.e the code for training images using stylegan2-ada. this codebase, too, \nwas dependent on tensorflow version 1, and aws sagemaker does not \nsupport older tensorflow versions. i was able to work around this issue by \nusing different kernels, but the gpu cluster that we were working with did not \nsupport depreciated libraries. \ni conducted a private call with matineh and le nga, and we attempted to \nwork through the errors, and after an hours long meeting, we concluded that \nit was not feasible to use the tensorflow version of stylegan2 with our \nenvironment. there was a pytorch implementation of the same codebase, \nand we had no choice but to use that instead. no one on the team had \nexperience with pytorch, so it was up to us to learn it on the go and deal with \nthe situation as it was. \nthe team agreed to implement the pytorch version by the end of this \nweek, and show some samples if possible by next week. \nduring one of our meetings, mike brought up a problem that the \ndocument verification team were facing and was wondering if we had any \nideas that he could take back to them. they were facing issues with blurry \ndocuments being sent by clients that were partially or totally unreadable by \ninternship report (final)\npage \n of \n25\n40\nthe ocr service. we did not have suggestions at the time but promised we \nwould look into it and get back to him if we found a concrete solution. \nle nga agreed to work on the pytorch version of the codebase and i \nwould again try to work on the conditional gans to control the age and \ngender of the generated images.",
            "page": null,
            "goal": "week 9\nin week 9, i was able to complete the code that i had started in week 7, \ni.e the code for training images using stylegan2-ada. this codebase, too, \nwas dependent on tensorflow version 1, and aws sagemaker does not \nsupport older tensorflow versions. i was able to work around this issue by \nusing different kernels, but the gpu cluster that we were working with did not \nsupport depreciated libraries. \ni conducted a private call with matineh and le nga, and we attempted to \nwork through the errors, and after an hours long meeting, we concluded that \nit was not feasible to use the tensorflow version of stylegan2 with our \nenvironment. there was a pytorch implementation of the same codebase, \nand we had no choice but to use that instead. no one on the team had \nexperience with pytorch, so it was up to us to learn it on the go and deal with \nthe situation as it was. \nthe team agreed to implement the pytorch version by the end of this \nweek, and show some samples if possible by next week. \nduring one of our meetings, mike brought up a problem that the \ndocument verification team were facing and was wondering if we had any \nideas that he could take back to them. they were facing issues with blurry \ndocuments being sent by clients that were partially or totally unreadable by \ninternship report (final)\npage \n of \n25\n40\nthe ocr service. we did not have suggestions at the time but promised we \nwould look into it and get back to him if we found a concrete solution. \nle nga agreed to work on the pytorch version of the codebase and i \nwould again try to work on the conditional gans to control the age and \ngender of the generated images.",
            "children": []
        },
        {
            "id": "1.19",
            "name": "week 10",
            "nodeType": "title",
            "text": "week 10",
            "page": null,
            "goal": "week 10",
            "children": []
        },
        {
            "id": "1.20",
            "name": "week 10 in",
            "nodeType": "paragraph",
            "text": "week 10\nin week 10, we made a massive leap of progress that exceeded matineh’s \nexpectations. she had independently trained an age-conditional gan on a \npublic database of images but the sample images generated were low in \nresolution and could not be reliably used for training purposes. \ni had come across a few research papers in week 2 that described how \ngans can be used to upscale low resolution images. this technique was \ncolloquially known as super resolution. i was very grateful to one of these \npaper’s authors who had made their training weights available publicly so we \ncould directly perform inference. the results from inference were extremely \npromising. \nby the end of this week i had developed a pipeline to pass images \nthrough the inferencing process, which would automatically generate a \ndataset of upscaled, high resolution images. this lead me to explore other \npublicly available training weights that we could use for inferencing directly \non our other tasks, instead of reinventing the wheel. \ni learned that this ml technique is called “transfer learning”, and if we \nhad access to said weights for our desired tasks we could speed up our \nworkflow and we would meet our deadlines before expected.",
            "page": null,
            "goal": "week 10\nin week 10, we made a massive leap of progress that exceeded matineh’s \nexpectations. she had independently trained an age-conditional gan on a \npublic database of images but the sample images generated were low in \nresolution and could not be reliably used for training purposes. \ni had come across a few research papers in week 2 that described how \ngans can be used to upscale low resolution images. this technique was \ncolloquially known as super resolution. i was very grateful to one of these \npaper’s authors who had made their training weights available publicly so we \ncould directly perform inference. the results from inference were extremely \npromising. \nby the end of this week i had developed a pipeline to pass images \nthrough the inferencing process, which would automatically generate a \ndataset of upscaled, high resolution images. this lead me to explore other \npublicly available training weights that we could use for inferencing directly \non our other tasks, instead of reinventing the wheel. \ni learned that this ml technique is called “transfer learning”, and if we \nhad access to said weights for our desired tasks we could speed up our \nworkflow and we would meet our deadlines before expected.",
            "children": []
        },
        {
            "id": "1.21",
            "name": "week 11",
            "nodeType": "title",
            "text": "week 11",
            "page": null,
            "goal": "week 11",
            "children": []
        },
        {
            "id": "1.22",
            "name": "week 11 week",
            "nodeType": "paragraph",
            "text": "week 11\nweek 11 was extremely productive for me. over the weekend, i was able \nto use the pre-trained weights of stylegan2 that were trained on flickr faces \nhigh quality (ffhq) dataset and apply them to the ageing task. during the \npresentation of my work on monday, matineh and mike were extremely \ninternship report (final)\npage \n of \n26\n40\nimpressed with the outputs. the results were incredibly realistic and of such \ngood quality that i was asked to consult with the document verification team \non the feasibility of using these images for “negative training”, i.e using these \nsamples to detect inauthentic documents that an adversarial actor might \nsubmit to our system. \ni promised that i would continue to supply them with gan generated \nimages using the pipeline i had created for their training purposes, and that \nby the end of the week, i would include documentation for this work so that it \ncould be accessible by anyone in the company. \nin another two days time, i had completed the task of gender \ntransformation using gans using a pre-trained network of male-to-female \nand female-to-male weights, also generated using stylegan2. i was able to \ngenerate excellent results on this task as well. mike and matineh were \nsimilarly very pleased with these results. \nthe next task that we needed to complete this week was to validate the \nimages, i.e whether certain landmarks in the background and the edges, \nwere being preserved. this was important, since we wanted to retain \nbackground detail and information for the liveness model to discriminate \npresentation attacks from live persons. \nthis task was entirely delegated to le nga, who has a keen eye and is \nexcellent at documenting her observations. meanwhile, i was asked to re-\nevaluate the problem that the document verification team had, i.e blurry \ndocument photos submitted by clients that ocr was failing to read. \nafter another full day of experimentation with the super resolution gan \npipeline, i concluded that this task was not something that could be suitable \nhandled by a gan, but rather by more conventional deep learning methods. \nin another day, i had applied two state-of-the-art (sota) deep learning \nsolutions to solve this problem. first, using one deep learning model, i \nperformed a retouching on these documents to correct for the low-light \ninternship report (final)\npage \n of \n27\n40\nproblem, and using a second model, i applied a de-blurring image \nrestoration model to un-blur the text on the document. \nthe performance was outstanding, exceeding the wildest expectations \nof the document verification team. previously, a poorly lit, blurry, license card \nthat received a 66% confidence score by the ocr service, now received a \n94.8% confidence score! these deep learning method were further adapted \nby me to process multiple images in a batched mode, which resulted in a \nrapid processing of image data.",
            "page": null,
            "goal": "week 11\nweek 11 was extremely productive for me. over the weekend, i was able \nto use the pre-trained weights of stylegan2 that were trained on flickr faces \nhigh quality (ffhq) dataset and apply them to the ageing task. during the \npresentation of my work on monday, matineh and mike were extremely \ninternship report (final)\npage \n of \n26\n40\nimpressed with the outputs. the results were incredibly realistic and of such \ngood quality that i was asked to consult with the document verification team \non the feasibility of using these images for “negative training”, i.e using these \nsamples to detect inauthentic documents that an adversarial actor might \nsubmit to our system. \ni promised that i would continue to supply them with gan generated \nimages using the pipeline i had created for their training purposes, and that \nby the end of the week, i would include documentation for this work so that it \ncould be accessible by anyone in the company. \nin another two days time, i had completed the task of gender \ntransformation using gans using a pre-trained network of male-to-female \nand female-to-male weights, also generated using stylegan2. i was able to \ngenerate excellent results on this task as well. mike and matineh were \nsimilarly very pleased with these results. \nthe next task that we needed to complete this week was to validate the \nimages, i.e whether certain landmarks in the background and the edges, \nwere being preserved. this was important, since we wanted to retain \nbackground detail and information for the liveness model to discriminate \npresentation attacks from live persons. \nthis task was entirely delegated to le nga, who has a keen eye and is \nexcellent at documenting her observations. meanwhile, i was asked to re-\nevaluate the problem that the document verification team had, i.e blurry \ndocument photos submitted by clients that ocr was failing to read. \nafter another full day of experimentation with the super resolution gan \npipeline, i concluded that this task was not something that could be suitable \nhandled by a gan, but rather by more conventional deep learning methods. \nin another day, i had applied two state-of-the-art (sota) deep learning \nsolutions to solve this problem. first, using one deep learning model, i \nperformed a retouching on these documents to correct for the low-light \ninternship report (final)\npage \n of \n27\n40\nproblem, and using a second model, i applied a de-blurring image \nrestoration model to un-blur the text on the document. \nthe performance was outstanding, exceeding the wildest expectations \nof the document verification team. previously, a poorly lit, blurry, license card \nthat received a 66% confidence score by the ocr service, now received a \n94.8% confidence score! these deep learning method were further adapted \nby me to process multiple images in a batched mode, which resulted in a \nrapid processing of image data.",
            "children": []
        },
        {
            "id": "1.23",
            "name": "week 12",
            "nodeType": "title",
            "text": "week 12",
            "page": null,
            "goal": "week 12",
            "children": []
        },
        {
            "id": "1.24",
            "name": "week 12 at",
            "nodeType": "paragraph",
            "text": "week 12\nat the end of last week, le nga had completed her task of image \nvalidation and had concluded that it was completely reasonable to apply our \ncustom models to the dataset that we wanted to inflate. we began the \nprocess of batched inferencing using a custom scheduler to process large \namounts of data. \nin the meantime, mike had shared with me a paper by microsoft \nresearch (bae et al., 2022), where the researchers had improved facial \nrecognition models by retraining them on gan augmented data, which was \nessentially the same task we were performing, but they had additionally \nperformed other augmentations such as accessorising with sunglasses, \nchange in facial pose, hair style change etc. while it is clear why this \napproach would improve facial recognition, i did not know if this would \nimprove the liveness models since that was not my area of expertise. \nregardless, i went about searching for a solution to a “generalised gan \naugmentation”, and found a dual step solution. \nfirst, we would generate a “direction vector” from a text prompt, for \nexample, “a face with blonde hair” or “a face with a beard”. then we would \napply said direction vector to our chosen image with a specified strength of \napplication. the results were quite impressive, but there were many bugs to \nfix in the code and since this was very close to the end of our semester, i \ninternship report (final)\npage \n of \n28\n40\ninstead decided to upload the code i had worked on thus far to the aws \nsagemaker instance, and left it as a future project for the company, or for the \nnext batch of interns. \nby the end of week 12, we had also completed all the documentation \nrequested for our gan models, as well as detailed reproducibility instructions \nfor anyone who wished to tinker with the code. \ni had completed all tasks that i had promised to deliver to mike and \nmatineh, and we had officially concluded our work at the company ahead of \nschedule by a full week. \ninternship report (final)\npage \n of \n29\n40",
            "page": null,
            "goal": "week 12\nat the end of last week, le nga had completed her task of image \nvalidation and had concluded that it was completely reasonable to apply our \ncustom models to the dataset that we wanted to inflate. we began the \nprocess of batched inferencing using a custom scheduler to process large \namounts of data. \nin the meantime, mike had shared with me a paper by microsoft \nresearch (bae et al., 2022), where the researchers had improved facial \nrecognition models by retraining them on gan augmented data, which was \nessentially the same task we were performing, but they had additionally \nperformed other augmentations such as accessorising with sunglasses, \nchange in facial pose, hair style change etc. while it is clear why this \napproach would improve facial recognition, i did not know if this would \nimprove the liveness models since that was not my area of expertise. \nregardless, i went about searching for a solution to a “generalised gan \naugmentation”, and found a dual step solution. \nfirst, we would generate a “direction vector” from a text prompt, for \nexample, “a face with blonde hair” or “a face with a beard”. then we would \napply said direction vector to our chosen image with a specified strength of \napplication. the results were quite impressive, but there were many bugs to \nfix in the code and since this was very close to the end of our semester, i \ninternship report (final)\npage \n of \n28\n40\ninstead decided to upload the code i had worked on thus far to the aws \nsagemaker instance, and left it as a future project for the company, or for the \nnext batch of interns. \nby the end of week 12, we had also completed all the documentation \nrequested for our gan models, as well as detailed reproducibility instructions \nfor anyone who wished to tinker with the code. \ni had completed all tasks that i had promised to deliver to mike and \nmatineh, and we had officially concluded our work at the company ahead of \nschedule by a full week. \ninternship report (final)\npage \n of \n29\n40",
            "children": []
        }
    ]
}