{
    "id": "1",
    "name": "Reflective journal Entities",
    "nodeType": "section",
    "text": "week 1\ndescribe\nin week 1, i got the chance to meet with my mentors in this internship which is stephen \nceo of itic systems and masoud saifian who is a phd candidate from mqu, and where i\ngot to know about the introduction about the company\n \nand their dealings with clients and\nhow they work in an organization,\n \ni got to know about\n \nhigh level idea of the project how \nwe are going to proceed\n \nand\n \nwith what\n \ntimeline, then i got a chance to\n \nsee the office and \nmy working environment which was new to me. i learnt about data exploration and data \nanalysis using various sources and tools, additionally i found the use of various tools to \ncommunicate online such as microsoft meet, confluence that will help us in \ncommunicating with each other when we are at remote location.\ninterpret\ni gained new insights in working in an organization and working of an organization. use\nof new tools for certain tasks gave me idea of flexibility when working\n \nbetween various \ndepartments, and data generation\n \nand collection of data from various sources gave me\nidea about mining big data and finding structure.\nevaluate\nafter getting insights about organization, working, project i found this week to be \nproductive for future as i was getting exposure of working in a company within a team and\nlearning about the business model taught by ceo of company. importance of teamwork \nusing various communication is the key for successful team task which i got to know by \nanalyzing company’s employees. hr introduced us with all the policy while working as an \nintern and importance of it which helped me in learning basic idea of how to perform a \ntask responsibly.\nplan\nas we progress further into weeks, i will be implementing all the\n \nlearning from week 1 into\nrest of the internship and in future employment as most of the rules and regulations are\npage 19 \nquite similar in organizations when working in an it department. learning new tools and \ntechniques for better understanding of the concept will be my priority as these methods \nwill help me in better coordination and communication in long term. the exposure from \nthese learnings will reflect on my upcoming weeks as i will try to implement them with \nmy own unique methods to get better results. \nweek 2 \ndescribe \nin week 2, i got the chance to learn about the project introduction, tools that we will be \nusing to work on the project, work that have already been done on that project and lastly, \ni got to know about my role in the project which was as an intern. new tools were asked to \nlearn to understand the basic concept of what i was doing with the data in an \norganizational setting. on other working day i was asked to generate data using \ngenerative ai tools like chatgpt to test the dummy data, then later in that week i was \nasked to demonstrate what i have done throughout the weak in a presentation and what \nmy ideas are for future tasks. \ninterpret \ni gained new insights in working with generative ai for commercial use and got to learn \nabout vast amount of data that i needed to generate and work with, i learnt about adobe \nproducts to use them on that same data to get some insights. for the first time i gained \nexperience in giving presentation in office setting with all the outcomes from that week. \nevaluate \nafter getting all the experience from that week i got to learn to communicate and work on \nmy soft skills in which i was lacking, i got to learn about data security, governance and \nother important aspects that will help me in industry. then i got to learn about the proper \nuse of tools for a specific task which was chatgpt in this case to generate the data. \noverall, i gained experience working with a team with stephen as our mentor and got to \nknow about the details of my first commercial project. \nplan \nas we progress further into weeks, i will be dealing with data analysis for insights, ui \ndesign for the project in a working prototype, machine learning algorithms to determine \nthe difference between fake essay data and real essay data, ai integration to check for any \nsuspected cheating or contracted working and project management aspects to manage \ntime, cost, and scope of the project to increase efficiency. lastly, i will be looking for \nproject outcome, result, and suggestions if there are changes needed to be done.  \npage 20 \nweek 3 \ndescribe \nin week 3, i got the chance to learn about machine learning algorithms that will be used \nin the project and i was introduced with essaygan which was an essay data  generation \ntool to generate dummy data to feed in that ml algorithm, then next working day i was \nasked to learn about the coding part of essaygan to know about the libraries that were \nused in that ai and to study them extensively to find any useful application in our project. \nthe last working day of week 3 working day i was asked to display my ideas of using \nessaygan into the project, and what libraries can be used in the generation process \nwithout any error, it was all presented via a team meeting and presentation in front of \nwhole team. \ninterpret \ni gained knowledge about existing algorithms that used in a public project that we are \nusing in our automated marking tool but in a different way to get any ideas from the \nexisting technology. i gained exposure to commercial tools and coding in an project to \nunderstand the idea behind the code, new libraries was introduced to me to interpret and \nuse in the machine learning algorithm and lastly, improved ideas about my soft skills \nwhile presenting and working with the team. \nevaluate \nafter getting all the experience from that week i got to learn to explore current tools with \nalgorithms which can be used in our project that shows. i increased my knowledge in it \ndomain and work with professionals to express my ideas and thinking behind the use of \ncertain library in a machine learning algorithm, essaygan was a completely new tool for \nme which was hard to learn initially but quite easy after some basic understanding. \nplan \nas we have already learnt about the project team, project information and generative ai in \nthe project next step for me was to work with ui design for the project in a working \nprototype, machine learning algorithms to determine the difference between fake essay \ndata and real essay data, ai integration to check for any suspected cheating or contracted \nworking and project management aspects to manage time, cost and scope of the project to \nincrease efficiency. lastly,i will be looking for project outcome, result and suggestions if \nthere are changes needed to be done.  \npage 21 \nweek 4 \ndescribe \nin week 4, i got indulge in the machine learning feature extraction part where i was given \nthe brief introduction about the feature and word extraction in the essay data to further \nanalyze it for automated marking. vector conversion of words into certain values was \nother task which was done in second day of working and then on the last working day of \nthe week i was asked to do the same process which i was doing for 3 weeks that was to \nshow my working, research and present my ideas for future works in that project. \ninterpret \ni gained knowledge various new feature extraction tools such as tf-idf to convert words \ninto vectors to feed into machine learning algorithms. i gained exposure in vectorization \nof words for efficient working with large amount of data using various techniques. \nalthough i had knowledge about this matter of subject through my university but getting \nhands on experience in this field was relatively new for me and taught me about the depth \nof using machine learning for different tasks and integration of all the algorithms with one \nanother. \nevaluate \nweek 4 was a hard week for me as there was plenty of research that needed to be done to \nstart working on the algorithm. after an extensive reading and research, i got to know \nabout the basic idea of why we are using vectorization and why we need feature extraction \nfor our automated marking tool, after learning and presenting my learning in front of the \nteam i am quite confident working with tf-dif idf vectorization techniques in the future \nprojects and rest of my internship program. \nplan \nas we have already learnt about the project team, project information and generative ai, \nfeature extraction in essay data in the project next step for me was to work with ui design \nfor the project in a working prototype, machine learning algorithms to determine the \ndifference between fake essay data and real essay data, ai integration to check for any \nsuspected cheating or contracted working and project management aspects to manage \ntime, cost and scope of the project to increase efficiency. lastly, i will be looking for \nproject outcome, result, and suggestions if there are changes needed to be done.  \nweek 5 \ndescribe \nin week 5, i was given choice to either work with bert which is a bidirectional encoder \nto encode words into vectors using ai or to work in the making of ui design for the \npage 22 \nproject or frontend of the project. as everyone was choosing the bert working, i \nchallenged myself to learn about other aspects of the it industry which was ui design for \nwhich had little to no knowledge about. throughout the week i learnt about the basics of \ndesigning and implementing but with the help of stephen at every step we collaborated \nand made a flowchart of working idea to implement it in our ui design. on the last \nworking day of week 5 i explained each feature of ui design that was done at that time to \nwhole team and what are my future ideas that i will work on that ui in upcoming weeks. \ninterpret \ni gained knowledge a completely new domain which was related to my work, and which \ngives me flexibility to work with completely different tasks in future in an organization. all \nthe reassert and analysis led to a prototype of the ui which was done while keeping in \nmind the actual idea of the project and simplification for others to understand what i was \ntrying to implement in that ui. a simple yet successful ui design for first 2 web page was \npresented in the last working day of the week and all the learning worked as intended. \nevaluate \nweek 5 was a hard week for me as there was 2.5 working days and a completely new task \nwas assigned to me which was never taught to me, nor i worked with it. i had to start from \nthe beginning of ui designing as completely new tools and methods was required, and due \nto short period of time i worked and learnt about ui design in my free time as well before \nthe team meeting to showcase them what i was working on and what can be done in the \nfuture of this project. a completely new idea was discovered as an addition to this ui and \nthen in team meeting it was discussed whether to introduce a new feature to current \nproject or not. \nplan \nas we have already learnt about the project team, project information and generative ai, \nfeature extraction in essay data and ui deign in the project next step for me was to work \nwith machine learning algorithms  to determine the difference between fake essay data \nand real essay data, ai integration to check for any suspected cheating or contracted \nworking and project management aspects to manage time, cost and scope of the project to \nincrease efficiency. lastly, i will be looking for project outcome, result, and suggestions if \nthere are changes needed to be done. \nweek 6 \ndescribe \nin week 6, i was given introduction about the use of various ai algorithms and comparison \nbetween the different ai but the main ai algorithm which we will work with was google bert \nwhich is a bidirectional transformer encoder to encode and decode our data when used in our \nmodel, fine tuning of bert was the task of the week to prepare bert for different input such as \npage 23 \ndifferent languages and different structure. as an output bert would be able to detect and \nencode any kind of language or input is the task that i was given with and to study its use and \napplication in our model was to be conducted and on the last working day of the week i was \nasked to show my working throughout the week to show how it can be fine-tuned for our model. \ninterpret \ni gained knowledge about the basic structure and working of bert model and other similar \nmodels to encode the words of an essay, we did comparison to find the most compatible model \nwhich can be used in our project, and it turned out to be bert can be an ideal model if fine-\ntuned properly before the work. a successful knowledge and research were done and presented at \nthe end of the week in the team meetings. \nevaluate \nweek 6 was a productive week as i gained knowledge about the new innovative tools that can be \nused in our current and future projects and this knowledge will help me in working in this subject \nwith efficiency and accuracy. fine-tuning or pre preparing bert was a new experience and i got \nto learn about the basic concept of ai models which has a higher growth potential in field of data \nscience. \nplan \nas we have already learnt about the project team, project information and generative ai, \nfeature extraction in essay data and ui deign and basic understanding of ai algorithms \nlike bert in the project next step for me was to work with, ai integration to check for any \nsuspected cheating or contracted working and project management aspects to manage \ntime, cost, and scope of the project to increase efficiency. lastly, i will be looking for \nproject outcome, result, and suggestions if there are changes needed to be done. \nweek 7 \ndescribe \n \nin week 7, i did learn about the infrastructure of the 3 most important features of this \nproject which is making an machine learning algorithm to generate a model which will \nscore our dataset essay and the other two features are answer summarization which we \ndid using commercial apis and existing tool that does similar functions to it lastly, \nquestion generation is done by the software team to generate questions generated by the \napi in use. \ninterpret \ni got to learn about the initial test train model using the bbc dataset which was a sample \ndataset to check our model accuracy and then i learnt to tweak the model according to \npage 24 \nour need that gave me new insight about the different libraries we used in this model. i \nlearnt about the existing tools and how we can use the commercial api in our project. \nevaluate \nafter completion of this week i successfully built a machine learning model from bbc \nnews topic paper based on classification for our model \n(https://github.com/dhrubasil/bbc-news-classification) i learnt about different models \nand its working throughout the process and lastly i got to present my insights about the \nimplementation of various models together as proposed in the flowchart[2]. \n \nweek 8 \ndescribe \nin week 8, using the same functions and machine learning algorithms i was introduced \nwith new data which was called asap dataset that consisted of marks of students from \nenglish exams and various e-rater as well as exam marks from different domain experts, \nin order to predict weather the marks were correctly allocated on the summary we used \nepoch training, bert tokenizer and bert model and the output results have been shown in \nthe working outputs below. \ninterpret \ninitial steps as a junior data analyst was to identify any irregularities or any noise in  in the \ndata in any form and we found out that rater domain_1 had score from 0-60 which was \nbeyond the limit of the specified model training so we tuned the data remove all the null \ncolumns and rows in order to increase the efficiency of the model and then took labels as \nfrom score 0 to 18 which was the most efficient score in this model and got about accuracy \nof 55% above. \nevaluate \nafter completion of this week i successfully built a machine learning model from asap \ndataset based on english exam, i learnt about different models and its working \nthroughout the process, cleaning of a industry data was the most interesting part i took a \nlot of efforts and handwork to analyze the data in initial phase using graphs, structure and \nlastly i got to present my insights about the implementation of various models together as \nproposed in the flowchart. \n \npage 25 \nplan \nas we have already learnt about the project team, project information and generative ai, \nfeature extraction in essay data and ui deign and basic understanding of ai algorithms \nlike bert and lastly ml models our next step was to further implement commercial api’s \ninto our project, ai integration to check for any suspected cheating or contracted working \nand project management aspects to manage time, cost, and scope of the project to \nincrease efficiency. lastly, i will be looking for project outcome, result, and suggestions if \nthere are changes needed to be done. \n \nweek 9 \ndescribe \nin week 9, we started working on both bbc and asap data to improve the efficiency of \nour model by fine tuning the bert model. as we got the initial results of both the models, \nwe tend to look at the training side of the model where we changed various parameters, \nimplemented more deep learning models with different learning rates, epoch trainings \nand others[2]. \ninterpret \ninitial model did give us more loss in the training phase and but after getting the gist of \nthe bert tuning all those losses turned down with each epoch and with increased accuracy \nof the model which taught us about the importance of tuning a deep learning model with \nvarious constrains and also i got to know that it we cannot always get good results with all \nmore learning rate or increased deep learning layers but it is about the dataset \ninterpretation and what the model need according to the need of the user. \nevaluate \nthis week was an important week for me as an individual in data science as i got to know \nabout the small things that matter in the machine learning modelling and analysis \nprocess. as it was taught that the more the parameters the better, but i got to learn that it \nis all about data structure, model handling and fine tuning and extracting every bit of a \nresult from the same model and by not changing the whole model. \n \nplan \nas we have already learnt about the project team, project information and generative ai, \nfeature extraction in essay data and ui deign and basic understanding of ai algorithms \nlike bert, and fine tuning of the machine learning model using industry data and lastly \nml models our next step was to further implement commercial api’s into our project, ai \nintegration to check for any suspected cheating or contracted working and project \npage 26 \nmanagement aspects to manage time, cost, and scope of the project to increase efficiency. \nlastly, i will be looking for project outcome, result, and suggestions if there are changes \nneeded to be done. \n \nweek 10 \ndescribe \nin week 10, for this week we were given with the corpus dataset which was a data that \ncontained the english test score results of the candidates in an english exam in the uk \n(https://ilexir.co.uk/datasets/index.html) when looking at the steps we performed in the \nearlier data exploration they were data with good structure, but this was not the case of \nthis data as there were 1244 files that was all sliced from years, different folder and was in \nvery different extension in each folder that made us work on the dataset alone whole week \nto further process and feed it as an input to our machine learning bert model [1]. \ninterpret \nfirst encountered with the unstructured data was a different experience for me as i was \nworking with mostly structured or semi structured data and that was easy to load, process \nand analyze but this week gave us all a challenging time and as a team we discussed what \napproach we can take to work on the data. \nevaluate \ndata preprocessing of this data was initially hard but after team effort and a lot of \ndiscussion we managed to create a single csv file to work with removing all the junk data \nand keeping the relevant fields for our ml implementation. \n \nplan \nas we have already learnt about the project team, project information and generative ai, \nfeature extraction in essay data, ui deign, basic understanding of ai algorithms like bert, \nand fine tuning of the machine learning model using industry data and lastly ml models \nand preprocessing unstructured data. lastly, i will be looking for project outcome, result, \nand suggestions if there are changes needed to be done. \n \npage 27 \nweek 11 \ndescribe \nin week 11, company meeting, and farewell party was organized to celebrate the 11 weeks \nof work in itic where we were invited by stephen for an afternoon lunch in a nice place \nwith masoud and jennifer. where we got many insights about the australian culture, \nworking, and job placement scenario and was asked to work on our cv for future \nopportunities. \ninterpret \nas it was a short break for the week we got to know about the importance of connection, \nspeaking skills with co-workers, soft skills in an organization and mostly the insights of \nworking in it in australia. \nevaluate \ninsights from all three mentors, stephen, masoud and yennifer it was truly a motivation \nto work hard to learn new techniques to achieve success in this field of industry. the \nrewarding part of this week was to discuss things outside the work about the challenges \nwe might face, the experience from my mentors and others gave me an insight about how \ni can work on my skills and enter the it domain. \nplan \nas we have already learnt about the project team, project information and generative ai, \nfeature extraction in essay data, ui deign, basic understanding of ai algorithms like bert, \nand fine tuning of the machine learning model using industry data and lastly ml models \nand preprocessing unstructured data. lastly, i will be looking for project outcome, result, \nand suggestions if there are changes needed to be done. \n \nweek 12 \ndescribe \nin week 12, we worked on the clean corpus data set for exploration and checked if there is \nfurther cleaning required or not and then we loaded that data into our machine learning \nalgorithm with bert then we did some visual charts such as bar graphs and line graphs to \nsee the data consistency and frequency. then we fed our tokenized data to the model and \n40 epoch trainings to get the result and present it on the last working day of the week. \npage 28 \ninterpret \noverall learning experience in this week was to interpret the data for input for pre-defined \nmodel was quite difficult and was time consuming. label setting and other fine tuning of \nthe model was the next step that was followed and in the last we got the accuracy of the \nmodel to see how it was performing. \nevaluate \nall the model training and fine tuning was a critical step when dealing with such dataset \nwith inconsistencies and overall, it was a successful week as we got to know about the \nhandling such dataset. rest of the steps was same as for the asap dataset and bbc dataset \nfor model training and testing. \n \nplan \nbefore the last week we have already set up a high level idea for our project, explored \nother tools that are currently in the market, set up a journey map or flowchart to follow \nsteps, researched about the code snippet related to machine learning, learnt about bert \nits operations and working, ui design including all the features that will be required in the \nweb page, bert model initialization, pre trainings and training models on different \ndatasets. now the very last task was to save all the progress in the repository for future \ninterns to work on and to move this project forward. \nweek 13 \ndescribe \nin week 13, it was all about the feedbacks how we did we progressed in the internship and \nproject, saving all the progress of the work we did throughout the internship into a \nrepository, making reports of all the outputs for future interns as a reference. \ninterpret \nthis week task was about to save all the checkpoints for the future students and to reflect \non our own performance throughout the internship. and lastly making a final report on \nthe project and new future ideas that can be added into this project. \nevaluate \nall the progress was saved and mailed to our supervisor, code stubs and snippets was \nsaved in the lms that is easily available to everyone and further flowchart was also \ndiscussed to plan for future students in order to work efficiently. \npage 29 \nplan \nbefore the last week we have already set up a high level idea for our project, explored \nother tools that are currently in the market, set up a journey map or flowchart to follow \nsteps, researched about the code snippet related to machine learning, learnt about bert \nits operations and working, ui design including all the features that will be required in the \nweb page, bert model initialization, pre trainings and training models on different \ndatasets. all the work that was intended for this batch is now completed in this week and \nall the workflow was saved for all the future interns to work on [8]. \n \n \n",
    "page": null,
    "goal": "Reflective journal Entities",
    "children": [
        {
            "id": "1.1",
            "name": "week 1",
            "nodeType": "title",
            "text": "week 1",
            "page": null,
            "goal": "week 1",
            "children": []
        },
        {
            "id": "1.2",
            "name": "week 1 describe",
            "nodeType": "paragraph",
            "text": "week 1\ndescribe\nin week 1, i got the chance to meet with my mentors in this internship which is stephen \nceo of itic systems and masoud saifian who is a phd candidate from mqu, and where i\ngot to know about the introduction about the company\n \nand their dealings with clients and\nhow they work in an organization,\n \ni got to know about\n \nhigh level idea of the project how \nwe are going to proceed\n \nand\n \nwith what\n \ntimeline, then i got a chance to\n \nsee the office and \nmy working environment which was new to me. i learnt about data exploration and data \nanalysis using various sources and tools, additionally i found the use of various tools to \ncommunicate online such as microsoft meet, confluence that will help us in \ncommunicating with each other when we are at remote location.\ninterpret\ni gained new insights in working in an organization and working of an organization. use\nof new tools for certain tasks gave me idea of flexibility when working\n \nbetween various \ndepartments, and data generation\n \nand collection of data from various sources gave me\nidea about mining big data and finding structure.\nevaluate\nafter getting insights about organization, working, project i found this week to be \nproductive for future as i was getting exposure of working in a company within a team and\nlearning about the business model taught by ceo of company. importance of teamwork \nusing various communication is the key for successful team task which i got to know by \nanalyzing company’s employees. hr introduced us with all the policy while working as an \nintern and importance of it which helped me in learning basic idea of how to perform a \ntask responsibly.\nplan\nas we progress further into weeks, i will be implementing all the\n \nlearning from week 1 into\nrest of the internship and in future employment as most of the rules and regulations are\npage 19 \nquite similar in organizations when working in an it department. learning new tools and \ntechniques for better understanding of the concept will be my priority as these methods \nwill help me in better coordination and communication in long term. the exposure from \nthese learnings will reflect on my upcoming weeks as i will try to implement them with \nmy own unique methods to get better results.",
            "page": null,
            "goal": "week 1\ndescribe\nin week 1, i got the chance to meet with my mentors in this internship which is stephen \nceo of itic systems and masoud saifian who is a phd candidate from mqu, and where i\ngot to know about the introduction about the company\n \nand their dealings with clients and\nhow they work in an organization,\n \ni got to know about\n \nhigh level idea of the project how \nwe are going to proceed\n \nand\n \nwith what\n \ntimeline, then i got a chance to\n \nsee the office and \nmy working environment which was new to me. i learnt about data exploration and data \nanalysis using various sources and tools, additionally i found the use of various tools to \ncommunicate online such as microsoft meet, confluence that will help us in \ncommunicating with each other when we are at remote location.\ninterpret\ni gained new insights in working in an organization and working of an organization. use\nof new tools for certain tasks gave me idea of flexibility when working\n \nbetween various \ndepartments, and data generation\n \nand collection of data from various sources gave me\nidea about mining big data and finding structure.\nevaluate\nafter getting insights about organization, working, project i found this week to be \nproductive for future as i was getting exposure of working in a company within a team and\nlearning about the business model taught by ceo of company. importance of teamwork \nusing various communication is the key for successful team task which i got to know by \nanalyzing company’s employees. hr introduced us with all the policy while working as an \nintern and importance of it which helped me in learning basic idea of how to perform a \ntask responsibly.\nplan\nas we progress further into weeks, i will be implementing all the\n \nlearning from week 1 into\nrest of the internship and in future employment as most of the rules and regulations are\npage 19 \nquite similar in organizations when working in an it department. learning new tools and \ntechniques for better understanding of the concept will be my priority as these methods \nwill help me in better coordination and communication in long term. the exposure from \nthese learnings will reflect on my upcoming weeks as i will try to implement them with \nmy own unique methods to get better results.",
            "children": []
        },
        {
            "id": "1.3",
            "name": "week 2",
            "nodeType": "title",
            "text": "week 2",
            "page": null,
            "goal": "week 2",
            "children": []
        },
        {
            "id": "1.4",
            "name": "week 2 describe",
            "nodeType": "paragraph",
            "text": "week 2 \ndescribe \nin week 2, i got the chance to learn about the project introduction, tools that we will be \nusing to work on the project, work that have already been done on that project and lastly, \ni got to know about my role in the project which was as an intern. new tools were asked to \nlearn to understand the basic concept of what i was doing with the data in an \norganizational setting. on other working day i was asked to generate data using \ngenerative ai tools like chatgpt to test the dummy data, then later in that week i was \nasked to demonstrate what i have done throughout the weak in a presentation and what \nmy ideas are for future tasks. \ninterpret \ni gained new insights in working with generative ai for commercial use and got to learn \nabout vast amount of data that i needed to generate and work with, i learnt about adobe \nproducts to use them on that same data to get some insights. for the first time i gained \nexperience in giving presentation in office setting with all the outcomes from that week. \nevaluate \nafter getting all the experience from that week i got to learn to communicate and work on \nmy soft skills in which i was lacking, i got to learn about data security, governance and \nother important aspects that will help me in industry. then i got to learn about the proper \nuse of tools for a specific task which was chatgpt in this case to generate the data. \noverall, i gained experience working with a team with stephen as our mentor and got to \nknow about the details of my first commercial project. \nplan \nas we progress further into weeks, i will be dealing with data analysis for insights, ui \ndesign for the project in a working prototype, machine learning algorithms to determine \nthe difference between fake essay data and real essay data, ai integration to check for any \nsuspected cheating or contracted working and project management aspects to manage \ntime, cost, and scope of the project to increase efficiency. lastly, i will be looking for \nproject outcome, result, and suggestions if there are changes needed to be done.  \npage 20",
            "page": null,
            "goal": "week 2 \ndescribe \nin week 2, i got the chance to learn about the project introduction, tools that we will be \nusing to work on the project, work that have already been done on that project and lastly, \ni got to know about my role in the project which was as an intern. new tools were asked to \nlearn to understand the basic concept of what i was doing with the data in an \norganizational setting. on other working day i was asked to generate data using \ngenerative ai tools like chatgpt to test the dummy data, then later in that week i was \nasked to demonstrate what i have done throughout the weak in a presentation and what \nmy ideas are for future tasks. \ninterpret \ni gained new insights in working with generative ai for commercial use and got to learn \nabout vast amount of data that i needed to generate and work with, i learnt about adobe \nproducts to use them on that same data to get some insights. for the first time i gained \nexperience in giving presentation in office setting with all the outcomes from that week. \nevaluate \nafter getting all the experience from that week i got to learn to communicate and work on \nmy soft skills in which i was lacking, i got to learn about data security, governance and \nother important aspects that will help me in industry. then i got to learn about the proper \nuse of tools for a specific task which was chatgpt in this case to generate the data. \noverall, i gained experience working with a team with stephen as our mentor and got to \nknow about the details of my first commercial project. \nplan \nas we progress further into weeks, i will be dealing with data analysis for insights, ui \ndesign for the project in a working prototype, machine learning algorithms to determine \nthe difference between fake essay data and real essay data, ai integration to check for any \nsuspected cheating or contracted working and project management aspects to manage \ntime, cost, and scope of the project to increase efficiency. lastly, i will be looking for \nproject outcome, result, and suggestions if there are changes needed to be done.  \npage 20",
            "children": []
        },
        {
            "id": "1.5",
            "name": "week 3",
            "nodeType": "title",
            "text": "week 3",
            "page": null,
            "goal": "week 3",
            "children": []
        },
        {
            "id": "1.6",
            "name": "week 3 describe",
            "nodeType": "paragraph",
            "text": "week 3 \ndescribe \nin week 3, i got the chance to learn about machine learning algorithms that will be used \nin the project and i was introduced with essaygan which was an essay data  generation \ntool to generate dummy data to feed in that ml algorithm, then next working day i was \nasked to learn about the coding part of essaygan to know about the libraries that were \nused in that ai and to study them extensively to find any useful application in our project. \nthe last working day of week 3 working day i was asked to display my ideas of using \nessaygan into the project, and what libraries can be used in the generation process \nwithout any error, it was all presented via a team meeting and presentation in front of \nwhole team. \ninterpret \ni gained knowledge about existing algorithms that used in a public project that we are \nusing in our automated marking tool but in a different way to get any ideas from the \nexisting technology. i gained exposure to commercial tools and coding in an project to \nunderstand the idea behind the code, new libraries was introduced to me to interpret and \nuse in the machine learning algorithm and lastly, improved ideas about my soft skills \nwhile presenting and working with the team. \nevaluate \nafter getting all the experience from that week i got to learn to explore current tools with \nalgorithms which can be used in our project that shows. i increased my knowledge in it \ndomain and work with professionals to express my ideas and thinking behind the use of \ncertain library in a machine learning algorithm, essaygan was a completely new tool for \nme which was hard to learn initially but quite easy after some basic understanding. \nplan \nas we have already learnt about the project team, project information and generative ai in \nthe project next step for me was to work with ui design for the project in a working \nprototype, machine learning algorithms to determine the difference between fake essay \ndata and real essay data, ai integration to check for any suspected cheating or contracted \nworking and project management aspects to manage time, cost and scope of the project to \nincrease efficiency. lastly,i will be looking for project outcome, result and suggestions if \nthere are changes needed to be done.  \npage 21",
            "page": null,
            "goal": "week 3 \ndescribe \nin week 3, i got the chance to learn about machine learning algorithms that will be used \nin the project and i was introduced with essaygan which was an essay data  generation \ntool to generate dummy data to feed in that ml algorithm, then next working day i was \nasked to learn about the coding part of essaygan to know about the libraries that were \nused in that ai and to study them extensively to find any useful application in our project. \nthe last working day of week 3 working day i was asked to display my ideas of using \nessaygan into the project, and what libraries can be used in the generation process \nwithout any error, it was all presented via a team meeting and presentation in front of \nwhole team. \ninterpret \ni gained knowledge about existing algorithms that used in a public project that we are \nusing in our automated marking tool but in a different way to get any ideas from the \nexisting technology. i gained exposure to commercial tools and coding in an project to \nunderstand the idea behind the code, new libraries was introduced to me to interpret and \nuse in the machine learning algorithm and lastly, improved ideas about my soft skills \nwhile presenting and working with the team. \nevaluate \nafter getting all the experience from that week i got to learn to explore current tools with \nalgorithms which can be used in our project that shows. i increased my knowledge in it \ndomain and work with professionals to express my ideas and thinking behind the use of \ncertain library in a machine learning algorithm, essaygan was a completely new tool for \nme which was hard to learn initially but quite easy after some basic understanding. \nplan \nas we have already learnt about the project team, project information and generative ai in \nthe project next step for me was to work with ui design for the project in a working \nprototype, machine learning algorithms to determine the difference between fake essay \ndata and real essay data, ai integration to check for any suspected cheating or contracted \nworking and project management aspects to manage time, cost and scope of the project to \nincrease efficiency. lastly,i will be looking for project outcome, result and suggestions if \nthere are changes needed to be done.  \npage 21",
            "children": []
        },
        {
            "id": "1.7",
            "name": "week 4",
            "nodeType": "title",
            "text": "week 4",
            "page": null,
            "goal": "week 4",
            "children": []
        },
        {
            "id": "1.8",
            "name": "week 4 describe",
            "nodeType": "paragraph",
            "text": "week 4 \ndescribe \nin week 4, i got indulge in the machine learning feature extraction part where i was given \nthe brief introduction about the feature and word extraction in the essay data to further \nanalyze it for automated marking. vector conversion of words into certain values was \nother task which was done in second day of working and then on the last working day of \nthe week i was asked to do the same process which i was doing for 3 weeks that was to \nshow my working, research and present my ideas for future works in that project. \ninterpret \ni gained knowledge various new feature extraction tools such as tf-idf to convert words \ninto vectors to feed into machine learning algorithms. i gained exposure in vectorization \nof words for efficient working with large amount of data using various techniques. \nalthough i had knowledge about this matter of subject through my university but getting \nhands on experience in this field was relatively new for me and taught me about the depth \nof using machine learning for different tasks and integration of all the algorithms with one \nanother. \nevaluate \nweek 4 was a hard week for me as there was plenty of research that needed to be done to \nstart working on the algorithm. after an extensive reading and research, i got to know \nabout the basic idea of why we are using vectorization and why we need feature extraction \nfor our automated marking tool, after learning and presenting my learning in front of the \nteam i am quite confident working with tf-dif idf vectorization techniques in the future \nprojects and rest of my internship program. \nplan \nas we have already learnt about the project team, project information and generative ai, \nfeature extraction in essay data in the project next step for me was to work with ui design \nfor the project in a working prototype, machine learning algorithms to determine the \ndifference between fake essay data and real essay data, ai integration to check for any \nsuspected cheating or contracted working and project management aspects to manage \ntime, cost and scope of the project to increase efficiency. lastly, i will be looking for \nproject outcome, result, and suggestions if there are changes needed to be done.",
            "page": null,
            "goal": "week 4 \ndescribe \nin week 4, i got indulge in the machine learning feature extraction part where i was given \nthe brief introduction about the feature and word extraction in the essay data to further \nanalyze it for automated marking. vector conversion of words into certain values was \nother task which was done in second day of working and then on the last working day of \nthe week i was asked to do the same process which i was doing for 3 weeks that was to \nshow my working, research and present my ideas for future works in that project. \ninterpret \ni gained knowledge various new feature extraction tools such as tf-idf to convert words \ninto vectors to feed into machine learning algorithms. i gained exposure in vectorization \nof words for efficient working with large amount of data using various techniques. \nalthough i had knowledge about this matter of subject through my university but getting \nhands on experience in this field was relatively new for me and taught me about the depth \nof using machine learning for different tasks and integration of all the algorithms with one \nanother. \nevaluate \nweek 4 was a hard week for me as there was plenty of research that needed to be done to \nstart working on the algorithm. after an extensive reading and research, i got to know \nabout the basic idea of why we are using vectorization and why we need feature extraction \nfor our automated marking tool, after learning and presenting my learning in front of the \nteam i am quite confident working with tf-dif idf vectorization techniques in the future \nprojects and rest of my internship program. \nplan \nas we have already learnt about the project team, project information and generative ai, \nfeature extraction in essay data in the project next step for me was to work with ui design \nfor the project in a working prototype, machine learning algorithms to determine the \ndifference between fake essay data and real essay data, ai integration to check for any \nsuspected cheating or contracted working and project management aspects to manage \ntime, cost and scope of the project to increase efficiency. lastly, i will be looking for \nproject outcome, result, and suggestions if there are changes needed to be done.",
            "children": []
        },
        {
            "id": "1.9",
            "name": "week 5",
            "nodeType": "title",
            "text": "week 5",
            "page": null,
            "goal": "week 5",
            "children": []
        },
        {
            "id": "1.10",
            "name": "week 5 describe",
            "nodeType": "paragraph",
            "text": "week 5 \ndescribe \nin week 5, i was given choice to either work with bert which is a bidirectional encoder \nto encode words into vectors using ai or to work in the making of ui design for the \npage 22 \nproject or frontend of the project. as everyone was choosing the bert working, i \nchallenged myself to learn about other aspects of the it industry which was ui design for \nwhich had little to no knowledge about. throughout the week i learnt about the basics of \ndesigning and implementing but with the help of stephen at every step we collaborated \nand made a flowchart of working idea to implement it in our ui design. on the last \nworking day of week 5 i explained each feature of ui design that was done at that time to \nwhole team and what are my future ideas that i will work on that ui in upcoming weeks. \ninterpret \ni gained knowledge a completely new domain which was related to my work, and which \ngives me flexibility to work with completely different tasks in future in an organization. all \nthe reassert and analysis led to a prototype of the ui which was done while keeping in \nmind the actual idea of the project and simplification for others to understand what i was \ntrying to implement in that ui. a simple yet successful ui design for first 2 web page was \npresented in the last working day of the week and all the learning worked as intended. \nevaluate \nweek 5 was a hard week for me as there was 2.5 working days and a completely new task \nwas assigned to me which was never taught to me, nor i worked with it. i had to start from \nthe beginning of ui designing as completely new tools and methods was required, and due \nto short period of time i worked and learnt about ui design in my free time as well before \nthe team meeting to showcase them what i was working on and what can be done in the \nfuture of this project. a completely new idea was discovered as an addition to this ui and \nthen in team meeting it was discussed whether to introduce a new feature to current \nproject or not. \nplan \nas we have already learnt about the project team, project information and generative ai, \nfeature extraction in essay data and ui deign in the project next step for me was to work \nwith machine learning algorithms  to determine the difference between fake essay data \nand real essay data, ai integration to check for any suspected cheating or contracted \nworking and project management aspects to manage time, cost and scope of the project to \nincrease efficiency. lastly, i will be looking for project outcome, result, and suggestions if \nthere are changes needed to be done.",
            "page": null,
            "goal": "week 5 \ndescribe \nin week 5, i was given choice to either work with bert which is a bidirectional encoder \nto encode words into vectors using ai or to work in the making of ui design for the \npage 22 \nproject or frontend of the project. as everyone was choosing the bert working, i \nchallenged myself to learn about other aspects of the it industry which was ui design for \nwhich had little to no knowledge about. throughout the week i learnt about the basics of \ndesigning and implementing but with the help of stephen at every step we collaborated \nand made a flowchart of working idea to implement it in our ui design. on the last \nworking day of week 5 i explained each feature of ui design that was done at that time to \nwhole team and what are my future ideas that i will work on that ui in upcoming weeks. \ninterpret \ni gained knowledge a completely new domain which was related to my work, and which \ngives me flexibility to work with completely different tasks in future in an organization. all \nthe reassert and analysis led to a prototype of the ui which was done while keeping in \nmind the actual idea of the project and simplification for others to understand what i was \ntrying to implement in that ui. a simple yet successful ui design for first 2 web page was \npresented in the last working day of the week and all the learning worked as intended. \nevaluate \nweek 5 was a hard week for me as there was 2.5 working days and a completely new task \nwas assigned to me which was never taught to me, nor i worked with it. i had to start from \nthe beginning of ui designing as completely new tools and methods was required, and due \nto short period of time i worked and learnt about ui design in my free time as well before \nthe team meeting to showcase them what i was working on and what can be done in the \nfuture of this project. a completely new idea was discovered as an addition to this ui and \nthen in team meeting it was discussed whether to introduce a new feature to current \nproject or not. \nplan \nas we have already learnt about the project team, project information and generative ai, \nfeature extraction in essay data and ui deign in the project next step for me was to work \nwith machine learning algorithms  to determine the difference between fake essay data \nand real essay data, ai integration to check for any suspected cheating or contracted \nworking and project management aspects to manage time, cost and scope of the project to \nincrease efficiency. lastly, i will be looking for project outcome, result, and suggestions if \nthere are changes needed to be done.",
            "children": []
        },
        {
            "id": "1.11",
            "name": "week 6",
            "nodeType": "title",
            "text": "week 6",
            "page": null,
            "goal": "week 6",
            "children": []
        },
        {
            "id": "1.12",
            "name": "week 6 describe",
            "nodeType": "paragraph",
            "text": "week 6 \ndescribe \nin week 6, i was given introduction about the use of various ai algorithms and comparison \nbetween the different ai but the main ai algorithm which we will work with was google bert \nwhich is a bidirectional transformer encoder to encode and decode our data when used in our \nmodel, fine tuning of bert was the task of the week to prepare bert for different input such as \npage 23 \ndifferent languages and different structure. as an output bert would be able to detect and \nencode any kind of language or input is the task that i was given with and to study its use and \napplication in our model was to be conducted and on the last working day of the week i was \nasked to show my working throughout the week to show how it can be fine-tuned for our model. \ninterpret \ni gained knowledge about the basic structure and working of bert model and other similar \nmodels to encode the words of an essay, we did comparison to find the most compatible model \nwhich can be used in our project, and it turned out to be bert can be an ideal model if fine-\ntuned properly before the work. a successful knowledge and research were done and presented at \nthe end of the week in the team meetings. \nevaluate \nweek 6 was a productive week as i gained knowledge about the new innovative tools that can be \nused in our current and future projects and this knowledge will help me in working in this subject \nwith efficiency and accuracy. fine-tuning or pre preparing bert was a new experience and i got \nto learn about the basic concept of ai models which has a higher growth potential in field of data \nscience. \nplan \nas we have already learnt about the project team, project information and generative ai, \nfeature extraction in essay data and ui deign and basic understanding of ai algorithms \nlike bert in the project next step for me was to work with, ai integration to check for any \nsuspected cheating or contracted working and project management aspects to manage \ntime, cost, and scope of the project to increase efficiency. lastly, i will be looking for \nproject outcome, result, and suggestions if there are changes needed to be done.",
            "page": null,
            "goal": "week 6 \ndescribe \nin week 6, i was given introduction about the use of various ai algorithms and comparison \nbetween the different ai but the main ai algorithm which we will work with was google bert \nwhich is a bidirectional transformer encoder to encode and decode our data when used in our \nmodel, fine tuning of bert was the task of the week to prepare bert for different input such as \npage 23 \ndifferent languages and different structure. as an output bert would be able to detect and \nencode any kind of language or input is the task that i was given with and to study its use and \napplication in our model was to be conducted and on the last working day of the week i was \nasked to show my working throughout the week to show how it can be fine-tuned for our model. \ninterpret \ni gained knowledge about the basic structure and working of bert model and other similar \nmodels to encode the words of an essay, we did comparison to find the most compatible model \nwhich can be used in our project, and it turned out to be bert can be an ideal model if fine-\ntuned properly before the work. a successful knowledge and research were done and presented at \nthe end of the week in the team meetings. \nevaluate \nweek 6 was a productive week as i gained knowledge about the new innovative tools that can be \nused in our current and future projects and this knowledge will help me in working in this subject \nwith efficiency and accuracy. fine-tuning or pre preparing bert was a new experience and i got \nto learn about the basic concept of ai models which has a higher growth potential in field of data \nscience. \nplan \nas we have already learnt about the project team, project information and generative ai, \nfeature extraction in essay data and ui deign and basic understanding of ai algorithms \nlike bert in the project next step for me was to work with, ai integration to check for any \nsuspected cheating or contracted working and project management aspects to manage \ntime, cost, and scope of the project to increase efficiency. lastly, i will be looking for \nproject outcome, result, and suggestions if there are changes needed to be done.",
            "children": []
        },
        {
            "id": "1.13",
            "name": "week 7",
            "nodeType": "title",
            "text": "week 7",
            "page": null,
            "goal": "week 7",
            "children": []
        },
        {
            "id": "1.14",
            "name": "week 7 describe",
            "nodeType": "paragraph",
            "text": "week 7 \ndescribe \n \nin week 7, i did learn about the infrastructure of the 3 most important features of this \nproject which is making an machine learning algorithm to generate a model which will \nscore our dataset essay and the other two features are answer summarization which we \ndid using commercial apis and existing tool that does similar functions to it lastly, \nquestion generation is done by the software team to generate questions generated by the \napi in use. \ninterpret \ni got to learn about the initial test train model using the bbc dataset which was a sample \ndataset to check our model accuracy and then i learnt to tweak the model according to \npage 24 \nour need that gave me new insight about the different libraries we used in this model. i \nlearnt about the existing tools and how we can use the commercial api in our project. \nevaluate \nafter completion of this week i successfully built a machine learning model from bbc \nnews topic paper based on classification for our model \n(https://github.com/dhrubasil/bbc-news-classification) i learnt about different models \nand its working throughout the process and lastly i got to present my insights about the \nimplementation of various models together as proposed in the flowchart[2].",
            "page": null,
            "goal": "week 7 \ndescribe \n \nin week 7, i did learn about the infrastructure of the 3 most important features of this \nproject which is making an machine learning algorithm to generate a model which will \nscore our dataset essay and the other two features are answer summarization which we \ndid using commercial apis and existing tool that does similar functions to it lastly, \nquestion generation is done by the software team to generate questions generated by the \napi in use. \ninterpret \ni got to learn about the initial test train model using the bbc dataset which was a sample \ndataset to check our model accuracy and then i learnt to tweak the model according to \npage 24 \nour need that gave me new insight about the different libraries we used in this model. i \nlearnt about the existing tools and how we can use the commercial api in our project. \nevaluate \nafter completion of this week i successfully built a machine learning model from bbc \nnews topic paper based on classification for our model \n(https://github.com/dhrubasil/bbc-news-classification) i learnt about different models \nand its working throughout the process and lastly i got to present my insights about the \nimplementation of various models together as proposed in the flowchart[2].",
            "children": []
        },
        {
            "id": "1.15",
            "name": "week 8",
            "nodeType": "title",
            "text": "week 8",
            "page": null,
            "goal": "week 8",
            "children": []
        },
        {
            "id": "1.16",
            "name": "week 8 describe",
            "nodeType": "paragraph",
            "text": "week 8 \ndescribe \nin week 8, using the same functions and machine learning algorithms i was introduced \nwith new data which was called asap dataset that consisted of marks of students from \nenglish exams and various e-rater as well as exam marks from different domain experts, \nin order to predict weather the marks were correctly allocated on the summary we used \nepoch training, bert tokenizer and bert model and the output results have been shown in \nthe working outputs below. \ninterpret \ninitial steps as a junior data analyst was to identify any irregularities or any noise in  in the \ndata in any form and we found out that rater domain_1 had score from 0-60 which was \nbeyond the limit of the specified model training so we tuned the data remove all the null \ncolumns and rows in order to increase the efficiency of the model and then took labels as \nfrom score 0 to 18 which was the most efficient score in this model and got about accuracy \nof 55% above. \nevaluate \nafter completion of this week i successfully built a machine learning model from asap \ndataset based on english exam, i learnt about different models and its working \nthroughout the process, cleaning of a industry data was the most interesting part i took a \nlot of efforts and handwork to analyze the data in initial phase using graphs, structure and \nlastly i got to present my insights about the implementation of various models together as \nproposed in the flowchart. \n \npage 25 \nplan \nas we have already learnt about the project team, project information and generative ai, \nfeature extraction in essay data and ui deign and basic understanding of ai algorithms \nlike bert and lastly ml models our next step was to further implement commercial api’s \ninto our project, ai integration to check for any suspected cheating or contracted working \nand project management aspects to manage time, cost, and scope of the project to \nincrease efficiency. lastly, i will be looking for project outcome, result, and suggestions if \nthere are changes needed to be done.",
            "page": null,
            "goal": "week 8 \ndescribe \nin week 8, using the same functions and machine learning algorithms i was introduced \nwith new data which was called asap dataset that consisted of marks of students from \nenglish exams and various e-rater as well as exam marks from different domain experts, \nin order to predict weather the marks were correctly allocated on the summary we used \nepoch training, bert tokenizer and bert model and the output results have been shown in \nthe working outputs below. \ninterpret \ninitial steps as a junior data analyst was to identify any irregularities or any noise in  in the \ndata in any form and we found out that rater domain_1 had score from 0-60 which was \nbeyond the limit of the specified model training so we tuned the data remove all the null \ncolumns and rows in order to increase the efficiency of the model and then took labels as \nfrom score 0 to 18 which was the most efficient score in this model and got about accuracy \nof 55% above. \nevaluate \nafter completion of this week i successfully built a machine learning model from asap \ndataset based on english exam, i learnt about different models and its working \nthroughout the process, cleaning of a industry data was the most interesting part i took a \nlot of efforts and handwork to analyze the data in initial phase using graphs, structure and \nlastly i got to present my insights about the implementation of various models together as \nproposed in the flowchart. \n \npage 25 \nplan \nas we have already learnt about the project team, project information and generative ai, \nfeature extraction in essay data and ui deign and basic understanding of ai algorithms \nlike bert and lastly ml models our next step was to further implement commercial api’s \ninto our project, ai integration to check for any suspected cheating or contracted working \nand project management aspects to manage time, cost, and scope of the project to \nincrease efficiency. lastly, i will be looking for project outcome, result, and suggestions if \nthere are changes needed to be done.",
            "children": []
        },
        {
            "id": "1.17",
            "name": "week 9",
            "nodeType": "title",
            "text": "week 9",
            "page": null,
            "goal": "week 9",
            "children": []
        },
        {
            "id": "1.18",
            "name": "week 9 describe",
            "nodeType": "paragraph",
            "text": "week 9 \ndescribe \nin week 9, we started working on both bbc and asap data to improve the efficiency of \nour model by fine tuning the bert model. as we got the initial results of both the models, \nwe tend to look at the training side of the model where we changed various parameters, \nimplemented more deep learning models with different learning rates, epoch trainings \nand others[2]. \ninterpret \ninitial model did give us more loss in the training phase and but after getting the gist of \nthe bert tuning all those losses turned down with each epoch and with increased accuracy \nof the model which taught us about the importance of tuning a deep learning model with \nvarious constrains and also i got to know that it we cannot always get good results with all \nmore learning rate or increased deep learning layers but it is about the dataset \ninterpretation and what the model need according to the need of the user. \nevaluate \nthis week was an important week for me as an individual in data science as i got to know \nabout the small things that matter in the machine learning modelling and analysis \nprocess. as it was taught that the more the parameters the better, but i got to learn that it \nis all about data structure, model handling and fine tuning and extracting every bit of a \nresult from the same model and by not changing the whole model. \n \nplan \nas we have already learnt about the project team, project information and generative ai, \nfeature extraction in essay data and ui deign and basic understanding of ai algorithms \nlike bert, and fine tuning of the machine learning model using industry data and lastly \nml models our next step was to further implement commercial api’s into our project, ai \nintegration to check for any suspected cheating or contracted working and project \npage 26 \nmanagement aspects to manage time, cost, and scope of the project to increase efficiency. \nlastly, i will be looking for project outcome, result, and suggestions if there are changes \nneeded to be done.",
            "page": null,
            "goal": "week 9 \ndescribe \nin week 9, we started working on both bbc and asap data to improve the efficiency of \nour model by fine tuning the bert model. as we got the initial results of both the models, \nwe tend to look at the training side of the model where we changed various parameters, \nimplemented more deep learning models with different learning rates, epoch trainings \nand others[2]. \ninterpret \ninitial model did give us more loss in the training phase and but after getting the gist of \nthe bert tuning all those losses turned down with each epoch and with increased accuracy \nof the model which taught us about the importance of tuning a deep learning model with \nvarious constrains and also i got to know that it we cannot always get good results with all \nmore learning rate or increased deep learning layers but it is about the dataset \ninterpretation and what the model need according to the need of the user. \nevaluate \nthis week was an important week for me as an individual in data science as i got to know \nabout the small things that matter in the machine learning modelling and analysis \nprocess. as it was taught that the more the parameters the better, but i got to learn that it \nis all about data structure, model handling and fine tuning and extracting every bit of a \nresult from the same model and by not changing the whole model. \n \nplan \nas we have already learnt about the project team, project information and generative ai, \nfeature extraction in essay data and ui deign and basic understanding of ai algorithms \nlike bert, and fine tuning of the machine learning model using industry data and lastly \nml models our next step was to further implement commercial api’s into our project, ai \nintegration to check for any suspected cheating or contracted working and project \npage 26 \nmanagement aspects to manage time, cost, and scope of the project to increase efficiency. \nlastly, i will be looking for project outcome, result, and suggestions if there are changes \nneeded to be done.",
            "children": []
        },
        {
            "id": "1.19",
            "name": "week 10",
            "nodeType": "title",
            "text": "week 10",
            "page": null,
            "goal": "week 10",
            "children": []
        },
        {
            "id": "1.20",
            "name": "week 10 describe",
            "nodeType": "paragraph",
            "text": "week 10 \ndescribe \nin week 10, for this week we were given with the corpus dataset which was a data that \ncontained the english test score results of the candidates in an english exam in the uk \n(https://ilexir.co.uk/datasets/index.html) when looking at the steps we performed in the \nearlier data exploration they were data with good structure, but this was not the case of \nthis data as there were 1244 files that was all sliced from years, different folder and was in \nvery different extension in each folder that made us work on the dataset alone whole week \nto further process and feed it as an input to our machine learning bert model [1]. \ninterpret \nfirst encountered with the unstructured data was a different experience for me as i was \nworking with mostly structured or semi structured data and that was easy to load, process \nand analyze but this week gave us all a challenging time and as a team we discussed what \napproach we can take to work on the data. \nevaluate \ndata preprocessing of this data was initially hard but after team effort and a lot of \ndiscussion we managed to create a single csv file to work with removing all the junk data \nand keeping the relevant fields for our ml implementation. \n \nplan \nas we have already learnt about the project team, project information and generative ai, \nfeature extraction in essay data, ui deign, basic understanding of ai algorithms like bert, \nand fine tuning of the machine learning model using industry data and lastly ml models \nand preprocessing unstructured data. lastly, i will be looking for project outcome, result, \nand suggestions if there are changes needed to be done. \n \npage 27",
            "page": null,
            "goal": "week 10 \ndescribe \nin week 10, for this week we were given with the corpus dataset which was a data that \ncontained the english test score results of the candidates in an english exam in the uk \n(https://ilexir.co.uk/datasets/index.html) when looking at the steps we performed in the \nearlier data exploration they were data with good structure, but this was not the case of \nthis data as there were 1244 files that was all sliced from years, different folder and was in \nvery different extension in each folder that made us work on the dataset alone whole week \nto further process and feed it as an input to our machine learning bert model [1]. \ninterpret \nfirst encountered with the unstructured data was a different experience for me as i was \nworking with mostly structured or semi structured data and that was easy to load, process \nand analyze but this week gave us all a challenging time and as a team we discussed what \napproach we can take to work on the data. \nevaluate \ndata preprocessing of this data was initially hard but after team effort and a lot of \ndiscussion we managed to create a single csv file to work with removing all the junk data \nand keeping the relevant fields for our ml implementation. \n \nplan \nas we have already learnt about the project team, project information and generative ai, \nfeature extraction in essay data, ui deign, basic understanding of ai algorithms like bert, \nand fine tuning of the machine learning model using industry data and lastly ml models \nand preprocessing unstructured data. lastly, i will be looking for project outcome, result, \nand suggestions if there are changes needed to be done. \n \npage 27",
            "children": []
        },
        {
            "id": "1.21",
            "name": "week 11",
            "nodeType": "title",
            "text": "week 11",
            "page": null,
            "goal": "week 11",
            "children": []
        },
        {
            "id": "1.22",
            "name": "week 11 describe",
            "nodeType": "paragraph",
            "text": "week 11 \ndescribe \nin week 11, company meeting, and farewell party was organized to celebrate the 11 weeks \nof work in itic where we were invited by stephen for an afternoon lunch in a nice place \nwith masoud and jennifer. where we got many insights about the australian culture, \nworking, and job placement scenario and was asked to work on our cv for future \nopportunities. \ninterpret \nas it was a short break for the week we got to know about the importance of connection, \nspeaking skills with co-workers, soft skills in an organization and mostly the insights of \nworking in it in australia. \nevaluate \ninsights from all three mentors, stephen, masoud and yennifer it was truly a motivation \nto work hard to learn new techniques to achieve success in this field of industry. the \nrewarding part of this week was to discuss things outside the work about the challenges \nwe might face, the experience from my mentors and others gave me an insight about how \ni can work on my skills and enter the it domain. \nplan \nas we have already learnt about the project team, project information and generative ai, \nfeature extraction in essay data, ui deign, basic understanding of ai algorithms like bert, \nand fine tuning of the machine learning model using industry data and lastly ml models \nand preprocessing unstructured data. lastly, i will be looking for project outcome, result, \nand suggestions if there are changes needed to be done.",
            "page": null,
            "goal": "week 11 \ndescribe \nin week 11, company meeting, and farewell party was organized to celebrate the 11 weeks \nof work in itic where we were invited by stephen for an afternoon lunch in a nice place \nwith masoud and jennifer. where we got many insights about the australian culture, \nworking, and job placement scenario and was asked to work on our cv for future \nopportunities. \ninterpret \nas it was a short break for the week we got to know about the importance of connection, \nspeaking skills with co-workers, soft skills in an organization and mostly the insights of \nworking in it in australia. \nevaluate \ninsights from all three mentors, stephen, masoud and yennifer it was truly a motivation \nto work hard to learn new techniques to achieve success in this field of industry. the \nrewarding part of this week was to discuss things outside the work about the challenges \nwe might face, the experience from my mentors and others gave me an insight about how \ni can work on my skills and enter the it domain. \nplan \nas we have already learnt about the project team, project information and generative ai, \nfeature extraction in essay data, ui deign, basic understanding of ai algorithms like bert, \nand fine tuning of the machine learning model using industry data and lastly ml models \nand preprocessing unstructured data. lastly, i will be looking for project outcome, result, \nand suggestions if there are changes needed to be done.",
            "children": []
        },
        {
            "id": "1.23",
            "name": "week 12",
            "nodeType": "title",
            "text": "week 12",
            "page": null,
            "goal": "week 12",
            "children": []
        },
        {
            "id": "1.24",
            "name": "week 12 describe",
            "nodeType": "paragraph",
            "text": "week 12 \ndescribe \nin week 12, we worked on the clean corpus data set for exploration and checked if there is \nfurther cleaning required or not and then we loaded that data into our machine learning \nalgorithm with bert then we did some visual charts such as bar graphs and line graphs to \nsee the data consistency and frequency. then we fed our tokenized data to the model and \n40 epoch trainings to get the result and present it on the last working day of the week. \npage 28 \ninterpret \noverall learning experience in this week was to interpret the data for input for pre-defined \nmodel was quite difficult and was time consuming. label setting and other fine tuning of \nthe model was the next step that was followed and in the last we got the accuracy of the \nmodel to see how it was performing. \nevaluate \nall the model training and fine tuning was a critical step when dealing with such dataset \nwith inconsistencies and overall, it was a successful week as we got to know about the \nhandling such dataset. rest of the steps was same as for the asap dataset and bbc dataset \nfor model training and testing. \n \nplan \nbefore the last week we have already set up a high level idea for our project, explored \nother tools that are currently in the market, set up a journey map or flowchart to follow \nsteps, researched about the code snippet related to machine learning, learnt about bert \nits operations and working, ui design including all the features that will be required in the \nweb page, bert model initialization, pre trainings and training models on different \ndatasets. now the very last task was to save all the progress in the repository for future \ninterns to work on and to move this project forward.",
            "page": null,
            "goal": "week 12 \ndescribe \nin week 12, we worked on the clean corpus data set for exploration and checked if there is \nfurther cleaning required or not and then we loaded that data into our machine learning \nalgorithm with bert then we did some visual charts such as bar graphs and line graphs to \nsee the data consistency and frequency. then we fed our tokenized data to the model and \n40 epoch trainings to get the result and present it on the last working day of the week. \npage 28 \ninterpret \noverall learning experience in this week was to interpret the data for input for pre-defined \nmodel was quite difficult and was time consuming. label setting and other fine tuning of \nthe model was the next step that was followed and in the last we got the accuracy of the \nmodel to see how it was performing. \nevaluate \nall the model training and fine tuning was a critical step when dealing with such dataset \nwith inconsistencies and overall, it was a successful week as we got to know about the \nhandling such dataset. rest of the steps was same as for the asap dataset and bbc dataset \nfor model training and testing. \n \nplan \nbefore the last week we have already set up a high level idea for our project, explored \nother tools that are currently in the market, set up a journey map or flowchart to follow \nsteps, researched about the code snippet related to machine learning, learnt about bert \nits operations and working, ui design including all the features that will be required in the \nweb page, bert model initialization, pre trainings and training models on different \ndatasets. now the very last task was to save all the progress in the repository for future \ninterns to work on and to move this project forward.",
            "children": []
        },
        {
            "id": "1.25",
            "name": "week 13",
            "nodeType": "title",
            "text": "week 13",
            "page": null,
            "goal": "week 13",
            "children": []
        },
        {
            "id": "1.26",
            "name": "week 13 describe",
            "nodeType": "paragraph",
            "text": "week 13 \ndescribe \nin week 13, it was all about the feedbacks how we did we progressed in the internship and \nproject, saving all the progress of the work we did throughout the internship into a \nrepository, making reports of all the outputs for future interns as a reference. \ninterpret \nthis week task was about to save all the checkpoints for the future students and to reflect \non our own performance throughout the internship. and lastly making a final report on \nthe project and new future ideas that can be added into this project. \nevaluate \nall the progress was saved and mailed to our supervisor, code stubs and snippets was \nsaved in the lms that is easily available to everyone and further flowchart was also \ndiscussed to plan for future students in order to work efficiently. \npage 29 \nplan \nbefore the last week we have already set up a high level idea for our project, explored \nother tools that are currently in the market, set up a journey map or flowchart to follow \nsteps, researched about the code snippet related to machine learning, learnt about bert \nits operations and working, ui design including all the features that will be required in the \nweb page, bert model initialization, pre trainings and training models on different \ndatasets. all the work that was intended for this batch is now completed in this week and \nall the workflow was saved for all the future interns to work on [8].",
            "page": null,
            "goal": "week 13 \ndescribe \nin week 13, it was all about the feedbacks how we did we progressed in the internship and \nproject, saving all the progress of the work we did throughout the internship into a \nrepository, making reports of all the outputs for future interns as a reference. \ninterpret \nthis week task was about to save all the checkpoints for the future students and to reflect \non our own performance throughout the internship. and lastly making a final report on \nthe project and new future ideas that can be added into this project. \nevaluate \nall the progress was saved and mailed to our supervisor, code stubs and snippets was \nsaved in the lms that is easily available to everyone and further flowchart was also \ndiscussed to plan for future students in order to work efficiently. \npage 29 \nplan \nbefore the last week we have already set up a high level idea for our project, explored \nother tools that are currently in the market, set up a journey map or flowchart to follow \nsteps, researched about the code snippet related to machine learning, learnt about bert \nits operations and working, ui design including all the features that will be required in the \nweb page, bert model initialization, pre trainings and training models on different \ndatasets. all the work that was intended for this batch is now completed in this week and \nall the workflow was saved for all the future interns to work on [8].",
            "children": []
        }
    ]
}