{
    "id": "1",
    "name": "Reflective journal Entities",
    "nodeType": "section",
    "text": "week 1 – onboarding and team building \non the first day at domain, other interns and i met dave from the it department who are very helpful \nand nice. we had our computer set up in the morning. we then got introduced to our team. i met with \nmy supervisor, ricky, he gave me a brief introduction to the team i was assigned. he also run me \nthrough the working style at domain, and the organizational structure of the data department. i had a \nreally good impression of the company and the people at domain. they are very friendly and willing to \nteach me step by step and explain stuffs to me. \n \nduring the whole week, i mainly spent time on building project knowledge, setting up tools, and getting \nto know the members at svp team. i read documentations of the svp data to understand what the \nteam does, how svp supports the core data assets for domain, what have the team achieved so far and \nwhat are the next step of the svp project. besides, i had individual meetings with some of the members \nin the svp team, i talked to them to understand what their roles are and how the teams work in an \nagile manner. i also started learning how to use snowflake on linkedin to prepare myself for the \nupcoming challenges. \n \nover the week, i gained new knowledge about property data and its components. i built connections \nwith my teammates. i completed the course of learning snowflakedb on linkedin. i also had a better \nunderstand of domain’s businesses. i found networking with team members particularly rewarding as \nthey are nice and willing to share their experiences with me and i am really grateful about that. \n \n \n \nweek 2 - learn to use dbt and snowflake \nthe main tasks of week 2 were to continue building project knowledge and learn to use data build tool \n(dbt) and snowflake. to achieve these goals, i read through a lot of documentations that are relevant \nto the svp team and the project i am working on from the confluence pages. through learning by doing, \n \n \n20 \n \ni explored the svp data using snowflake and dbt and gain a better understanding of the data structure \nand the relationship among tables.  \n \ni took the course ‘learning snowflakedb’ taught by lynn langit on linkedin11 out of work hours. i \nlearned the basic function of snowflake. i also took the online dbt fundamentals course13 offered by \nthe dbt labs to get familiar with the data transformation tool. i gained general understanding of how \nto use dbt cloud, create models, manage data sources, write tests, generate documentations, and set \nup deployment in dbt. i also learned to use yaml to write code in dbt to prepare myself for the \nupcoming tasks. \n \nover the week, i gained a better understanding of the svp data, snowflake, and dbt, along with \nknowledge of writing code in dbt. specifically, it was rewarding to participate in a tuesday \nbrainstorming session where all svp team members sat down and discussed possible solutions to a \nbottleneck of the project. it was exciting to see how people from different backgrounds with different \nexpertise come together to solve a problem, where product owners contribute their property \nknowledges and business sense, while data engineers try to provide solutions to meet the requirements \nand expectations from the business side. \n \n \n \nweek 3 - write unit tests in dbt \nthis week, the goal was to write unit tests for some data models in dbt. i quired the input and output \ntables from snowflake and analyzed the data. the aim was to identify the data flow logic between the \ninput and output tables and to make sure certain input will produce expected output. meanwhile, i \ncontinue my learning from the dbt fundamental course.  \n \nricky had a few sessions with me to clarify my understanding of the task and walked me through a \ndemo on how to conduct the whole process from analyzing tables to drafting unit tests in dbt. i also \ntalked with a few team members to gain a better understanding of the project i was working on, to \nclarify some project jargon and to network with them.   \n \n \n21 \n \n \nover the week, drafting unit tests was the most difficult experience during this period, as it took some \ntime to understand how dbt template works and how unit tests are conducted in dbt. there were \nmany issues arose during the whole process, and identifying the problems was challenging and time-\nconsuming, requiring going back and forth to check for errors in the code or the models. despite that, \nit was also a rewarding experience as i learned a lot along the way. i gained not only technical \nknowledge, but also enhance my skills in problem solving and thinking out of the box. \n \n \n \nweek 4 - write unit tests in dbt \nthis week, the goal was to continue writing unit tests in dbt. given more understanding on unit testing, \ni decided to further examine the data before jumping to write the code. i run sql query to examine the \noutput tables in snowflake. i applied knowledge i learned from the course ‘comp6350 data system’ \nand drew er diagrams to help me understand what and how data was transformed from the input \ntables to the output tables. after i sorted out the logic behind the transformation process, i started to \nwriting test descriptions and code in dbt to validate the data transformation accuracy. \n \nmeanwhile, i attended a dbt cloud demo workshop which helped me to better understand data \nengineering components in dbt. in addition, i continued to learn how to better utilize different \nfunctions in dbt from the dbt fundamentals course throughout the week. i also asked for ricky’s advice \non improving the code i wrote.  \n \nover the week, debugging unit testing was the most difficult experience during this period, as it \nrequired a lot of trial and error throughout the process. despite that, it was also a rewarding experience \nas it enhanced my problem-solving skills and improved my communication skills. i was also encouraged \nto reach out for help from nice people in the team. \n \n \n \n \n \n \n22 \n \nweek 5- create stored procedures in snowflake \nthe goal of this week was to improve the stored procedures for automating data extraction for drafting \nunit testing in snowflake.  \n \nafter completing the unit testing tasks last week, my supervisor, ricky, drafted a stored procedure in \nsnowflake to facilitate the unit testing in dbt. before that, we need to manually type the output result \nas sql select statement for dbt to read and generate a table for unit testing. therefore, the stored \nprocedure aimed to automatically generate the result output as sql select statement from a normal \nsql input query, so users don’t need to do it manually. \n \nmy task was to improve the stored procedures so when we call the function, it can automatically get \nthe schema name and table name from an input query and return the output table as a sql select \nstatement for use in dbt unit testing. since i didn’t have experience in writing stored procedure, i \nconstantly reached out to ricky for advice. \n \nover the week, the task was difficult, especially since snowflake does not support mysql language. i \nmust rewrite my code to match with snowflake sql language. moreover, debugging the code in the \nstored procedure was challenging and required a lot of time to test and troubleshoot. it is normal to \nget stuck when writing code, but i learn to use different debugging tools that can help me identify the \nproblem and suggest possible solutions with a fresh eye. i also strengthen my patience when coping \nwith difficult coding problems. overall, the experience was rewarding. \n \n \n \nweek 6 - learn to use command line tool in managing workflows \nthe goal for this week was to learn how to manage multiple workflows using command line tool. \n \non tuesday, ricky reviewed my code in the stored procedures i drafted last week, and we worked \ntogether to debug the code. we successfully resolved the issues in the coding and completed the stored \nprocedure that could go live and used by others to facilitate unit testing. ricky also discussed with me \n \n \n23 \n \non possible future improvement in the process. i learned how to work and think like a data engineer \nfrom this experience.  \n \ni also had a great chat with my mentor, chenxuan, who returned to office from parental leave. he \nshared a lot of his work experiences and tips with me. he recommended me to use command line tool \nsuch as iterm to better manage different versions of software under different projects. it is very \ncommon for data engineer to use cli instead of gui as this allow more flexibility in code developing \nand managing platforms. \n \nover the week, i installed all necessary tools for new terminal setup and used them to explore svp data \nto get familiar with the tool. i watched on youtube to understand the basic function in cli. in addition, \ni learned how to manage multiple versions of software, such as python, dbt and snowflake, in the \ncomputer for matching different project requirements. \n \n \n \nweek 7 - use git and github for version control \nthe goal for this week was to get familiar with git and github and use them for version control in \nmanaging projects. \n \nthis week, my mentor, chenxuan, walked me through how to use git and github in our company \nsetting. he also assisted me in setting up the tooling and provided a lot of guidance on learning git. i \nfollowed his suggestions and took a crash course on youtube offered by freecodecamp.org to get a \nbetter understanding on how to use github. i followed the instruction to create repository in github \nand learned how to commit, push, and pull request from github. since i use visual studio code (vs \ncode) as my terminal tool, i also walked through the tutorial on how to use source control in vs code \non its website. \n \n \n \n24 \n \nover the week, i started to use git for version controls. i created branch and explored data in dbt \nthrough vs code. i became more familiar with cli and git. this helped me establish a good practice at \nwork. \n \nbesides, on thursday, we had the midterm presentation on zoom to report what we have done in our \ninternship so far. it was interesting to know what other students did in their internship and what they \nhave learned so far. \n \n \n \nweek 8 – explore piperider \nthe goal for this week was to explore piperider, understand business metrics in data, and get myself \nprepared for the upcoming tasks using piperider.  \n \npiperider is a data reliability tool that works with dbt. i follow the tutorial on piperider website and \nread the documentation to understand how to use it with dbt. i also search online to understand what \nbusiness metrics are in the sense of data. i installed piperider on vs code and explored it with the svp \ndataset.  \n \nthe main challenge was installing piperider on vs code due to configuration issue. since i was not so \nfamiliar with cli tools and how to setup the tool, it took me a whole day to setup. i also need to ask for \nmy mentor’s help throughout the process.  but i also learned that it was something you will always \nencounter when dealing with data tools setup. since lots of tools very new and open source, you need \nto read through the documentation to see how to configure the settings and manage different versions \nto ensure they are compatible with other tools you used. this is something i need to work on to develop \nmindset of data engineer. it was also nice to talk with my mentor as he gave me a lot of advice on using \ncli tools, he shared his past work experiences as a junior data engineer and how he learned and \novercome the challenges in his previous job. i learned a lot from him.  \n \n \n \n \n \n \n25 \n \nweek 9 - diagnostic analysis on property profile \nthe goal for this week was to run diagnostic analysis on the property profile dataset. the aim was to \nidentify the difference in property profile between internal svp data and external apm data source. \n \ni used sql to query data from snowflake, drew relevant columns from the two datasets and combined \nthem into one table for comparison. then i analyzed the data and identifies what caused the difference \nin the property details, such as property type, property category and valuation, between svp and apm \ndatasets. i summarized my findings on the confluence page.  \n \nthe most rewarding part of this week was that i could find some interesting findings from my analysis \nwhich provided business value for the team. i found out that there are a lot of typo and mistake in the \napm data causing duplication and misclassification of the property type and category. i also found that \nthe mapping logic is different between the two datasets. in addition, i identified abnormal calculation \nin the property valuation in the svp dataset. it was excited that my findings could provide value to the \nteam and narrow down the focus on resolving the difference between the two datasets. \n \n \n \nweek 10 - diagnostic analysis on core listing activity \nthe goal for this week was to run diagnostic analysis on the core listing activity dataset.  \n \nsimilar to last week, i used sql to query data from snowflake, drew relevant columns from the two \ndatasets and combined them into one table for comparison. then i analyzed the data and identifies \nwhat caused the difference in the core listing activity, such as activity start date, activity end date, \nactivity type and activity channel, between svp and apm datasets. i then summarized my findings on \nthe confluence page.  \n \nagain, the most rewarding part of this week was that i could find some interesting findings from my \nanalysis which provided business value for the team. i provided comparison of number of sales and \nrent activities with different level of tolerance in activity start date, such as 1 day, 1 week and 1 month. \n \n \n26 \n \ni also identify there is mismatch in activity channel and activity type between the two datasets. it was \nexcited to see my findings could provide value to the team and narrow down the focus on resolving the \ndifference between the two datasets. \n \n \n \nweek 11 – build business metrics in piperider \nthe goal for this week was to build business metrics for the two datasets: property profile and core \nlisting activity in piperider.  \n \ni have spent some time exploring piperider and bisiness metrics during week 8. this week was to \nexecute it. however, i encountered blockage when setting it up on my desktop. i spent half day \ndebugging the setting to configure piperider with dbt in vs code. with the help of my mentor, i finally \nsuccessfully ran dbt and piperider in vs code. \n \nthen i spent the rest of the week to understand business metrics and write business metrics in .ymal \nfile in dbt and ran it with piperider. i have created eight business metrics for the property profile \ndataset. the aim of business metrics is to keep track of the data quality over time and to observe is \nthere any abnormal behaviors in the data.  in piperider, it will generate line graph or bar chart to \nvisualize the data so you can get a quick overview of the data you want to focus on and to analyze it \nwith time constraint. all my findings were documented on confluence page. \n \nthe best thing of this week was that i learned more about quality data and business metrics. i was able \nto build some of it and to provide some useful information to the team using the metrics. i also learned \nhow to compare report in piperider. this is very useful when you want to have a quick understanding \nof the data to see what the differences between the base and the target datasets are. piperider also \nprovide suggested data assertions so you can easily apply them to your data to enforce data quality \nassessments by just a few clicks.  \n \n \n \n \n \n \n27 \n \nweek 12 - data profiling and assess data quality using piperider \nthe goal for this week was to continue building business metrics on the core listing activity dataset, \nrunning data profiling on both property profile and core listing activity datasets, and assessing the data \nquality using piperider.  \n \ni spent a day to create six business metrics for the core listing activity dataset. it was a different dataset \nwith property profile.  core listing activity focused on analyzing activity start date and activity channel. \nthe main purpose was to compare the sale and rent activity across time.  \n \nthe rest of the week was to investigate some abnormal data findings from the leadscope team. the \nmain problem was that there is mismatch in the apm and svp data for the core listing activity dataset. \nmy duty was to look into it and identify whether svp data is truly reflecting the data in apm dataset. \nthe goal was to close the gap between them. i queried data of the two tables from snowflake and \nanalyzed the differences. the i summarized my findings to my mentor and completed my work for the \nweek.  \n \nthe internship was approaching the end and i have summarized and updated all my findings of the \nanalysis i did in the past few weeks on the confluence page. especially for comparison between svp \nand apm data on property profile and core listing activity datasets. also included business metrics and \ndata profiling of the data.   \n \n \n \nweek 13 – final presentation \nthis is the final week of the internship. all the interns presented our works to the data department \nteam leaders. we had a great closure of our internship.  \n \n \n \n \n \n",
    "page": null,
    "goal": "Reflective journal Entities",
    "children": [
        {
            "id": "1.1",
            "name": "week 1",
            "nodeType": "title",
            "text": "week 1",
            "page": null,
            "goal": "week 1",
            "children": []
        },
        {
            "id": "1.2",
            "name": "week 1 –",
            "nodeType": "paragraph",
            "text": "week 1 – onboarding and team building \non the first day at domain, other interns and i met dave from the it department who are very helpful \nand nice. we had our computer set up in the morning. we then got introduced to our team. i met with \nmy supervisor, ricky, he gave me a brief introduction to the team i was assigned. he also run me \nthrough the working style at domain, and the organizational structure of the data department. i had a \nreally good impression of the company and the people at domain. they are very friendly and willing to \nteach me step by step and explain stuffs to me. \n \nduring the whole week, i mainly spent time on building project knowledge, setting up tools, and getting \nto know the members at svp team. i read documentations of the svp data to understand what the \nteam does, how svp supports the core data assets for domain, what have the team achieved so far and \nwhat are the next step of the svp project. besides, i had individual meetings with some of the members \nin the svp team, i talked to them to understand what their roles are and how the teams work in an \nagile manner. i also started learning how to use snowflake on linkedin to prepare myself for the \nupcoming challenges. \n \nover the week, i gained new knowledge about property data and its components. i built connections \nwith my teammates. i completed the course of learning snowflakedb on linkedin. i also had a better \nunderstand of domain’s businesses. i found networking with team members particularly rewarding as \nthey are nice and willing to share their experiences with me and i am really grateful about that.",
            "page": null,
            "goal": "week 1 – onboarding and team building \non the first day at domain, other interns and i met dave from the it department who are very helpful \nand nice. we had our computer set up in the morning. we then got introduced to our team. i met with \nmy supervisor, ricky, he gave me a brief introduction to the team i was assigned. he also run me \nthrough the working style at domain, and the organizational structure of the data department. i had a \nreally good impression of the company and the people at domain. they are very friendly and willing to \nteach me step by step and explain stuffs to me. \n \nduring the whole week, i mainly spent time on building project knowledge, setting up tools, and getting \nto know the members at svp team. i read documentations of the svp data to understand what the \nteam does, how svp supports the core data assets for domain, what have the team achieved so far and \nwhat are the next step of the svp project. besides, i had individual meetings with some of the members \nin the svp team, i talked to them to understand what their roles are and how the teams work in an \nagile manner. i also started learning how to use snowflake on linkedin to prepare myself for the \nupcoming challenges. \n \nover the week, i gained new knowledge about property data and its components. i built connections \nwith my teammates. i completed the course of learning snowflakedb on linkedin. i also had a better \nunderstand of domain’s businesses. i found networking with team members particularly rewarding as \nthey are nice and willing to share their experiences with me and i am really grateful about that.",
            "children": []
        },
        {
            "id": "1.3",
            "name": "week 2",
            "nodeType": "title",
            "text": "week 2",
            "page": null,
            "goal": "week 2",
            "children": []
        },
        {
            "id": "1.4",
            "name": "week 2 -",
            "nodeType": "paragraph",
            "text": "week 2 - learn to use dbt and snowflake \nthe main tasks of week 2 were to continue building project knowledge and learn to use data build tool \n(dbt) and snowflake. to achieve these goals, i read through a lot of documentations that are relevant \nto the svp team and the project i am working on from the confluence pages. through learning by doing, \n \n \n20 \n \ni explored the svp data using snowflake and dbt and gain a better understanding of the data structure \nand the relationship among tables.  \n \ni took the course ‘learning snowflakedb’ taught by lynn langit on linkedin11 out of work hours. i \nlearned the basic function of snowflake. i also took the online dbt fundamentals course13 offered by \nthe dbt labs to get familiar with the data transformation tool. i gained general understanding of how \nto use dbt cloud, create models, manage data sources, write tests, generate documentations, and set \nup deployment in dbt. i also learned to use yaml to write code in dbt to prepare myself for the \nupcoming tasks. \n \nover the week, i gained a better understanding of the svp data, snowflake, and dbt, along with \nknowledge of writing code in dbt. specifically, it was rewarding to participate in a tuesday \nbrainstorming session where all svp team members sat down and discussed possible solutions to a \nbottleneck of the project. it was exciting to see how people from different backgrounds with different \nexpertise come together to solve a problem, where product owners contribute their property \nknowledges and business sense, while data engineers try to provide solutions to meet the requirements \nand expectations from the business side.",
            "page": null,
            "goal": "week 2 - learn to use dbt and snowflake \nthe main tasks of week 2 were to continue building project knowledge and learn to use data build tool \n(dbt) and snowflake. to achieve these goals, i read through a lot of documentations that are relevant \nto the svp team and the project i am working on from the confluence pages. through learning by doing, \n \n \n20 \n \ni explored the svp data using snowflake and dbt and gain a better understanding of the data structure \nand the relationship among tables.  \n \ni took the course ‘learning snowflakedb’ taught by lynn langit on linkedin11 out of work hours. i \nlearned the basic function of snowflake. i also took the online dbt fundamentals course13 offered by \nthe dbt labs to get familiar with the data transformation tool. i gained general understanding of how \nto use dbt cloud, create models, manage data sources, write tests, generate documentations, and set \nup deployment in dbt. i also learned to use yaml to write code in dbt to prepare myself for the \nupcoming tasks. \n \nover the week, i gained a better understanding of the svp data, snowflake, and dbt, along with \nknowledge of writing code in dbt. specifically, it was rewarding to participate in a tuesday \nbrainstorming session where all svp team members sat down and discussed possible solutions to a \nbottleneck of the project. it was exciting to see how people from different backgrounds with different \nexpertise come together to solve a problem, where product owners contribute their property \nknowledges and business sense, while data engineers try to provide solutions to meet the requirements \nand expectations from the business side.",
            "children": []
        },
        {
            "id": "1.5",
            "name": "week 3",
            "nodeType": "title",
            "text": "week 3",
            "page": null,
            "goal": "week 3",
            "children": []
        },
        {
            "id": "1.6",
            "name": "week 3 -",
            "nodeType": "paragraph",
            "text": "week 3 - write unit tests in dbt \nthis week, the goal was to write unit tests for some data models in dbt. i quired the input and output \ntables from snowflake and analyzed the data. the aim was to identify the data flow logic between the \ninput and output tables and to make sure certain input will produce expected output. meanwhile, i \ncontinue my learning from the dbt fundamental course.  \n \nricky had a few sessions with me to clarify my understanding of the task and walked me through a \ndemo on how to conduct the whole process from analyzing tables to drafting unit tests in dbt. i also \ntalked with a few team members to gain a better understanding of the project i was working on, to \nclarify some project jargon and to network with them.   \n \n \n21 \n \n \nover the week, drafting unit tests was the most difficult experience during this period, as it took some \ntime to understand how dbt template works and how unit tests are conducted in dbt. there were \nmany issues arose during the whole process, and identifying the problems was challenging and time-\nconsuming, requiring going back and forth to check for errors in the code or the models. despite that, \nit was also a rewarding experience as i learned a lot along the way. i gained not only technical \nknowledge, but also enhance my skills in problem solving and thinking out of the box.",
            "page": null,
            "goal": "week 3 - write unit tests in dbt \nthis week, the goal was to write unit tests for some data models in dbt. i quired the input and output \ntables from snowflake and analyzed the data. the aim was to identify the data flow logic between the \ninput and output tables and to make sure certain input will produce expected output. meanwhile, i \ncontinue my learning from the dbt fundamental course.  \n \nricky had a few sessions with me to clarify my understanding of the task and walked me through a \ndemo on how to conduct the whole process from analyzing tables to drafting unit tests in dbt. i also \ntalked with a few team members to gain a better understanding of the project i was working on, to \nclarify some project jargon and to network with them.   \n \n \n21 \n \n \nover the week, drafting unit tests was the most difficult experience during this period, as it took some \ntime to understand how dbt template works and how unit tests are conducted in dbt. there were \nmany issues arose during the whole process, and identifying the problems was challenging and time-\nconsuming, requiring going back and forth to check for errors in the code or the models. despite that, \nit was also a rewarding experience as i learned a lot along the way. i gained not only technical \nknowledge, but also enhance my skills in problem solving and thinking out of the box.",
            "children": []
        },
        {
            "id": "1.7",
            "name": "week 4",
            "nodeType": "title",
            "text": "week 4",
            "page": null,
            "goal": "week 4",
            "children": []
        },
        {
            "id": "1.8",
            "name": "week 4 -",
            "nodeType": "paragraph",
            "text": "week 4 - write unit tests in dbt \nthis week, the goal was to continue writing unit tests in dbt. given more understanding on unit testing, \ni decided to further examine the data before jumping to write the code. i run sql query to examine the \noutput tables in snowflake. i applied knowledge i learned from the course ‘comp6350 data system’ \nand drew er diagrams to help me understand what and how data was transformed from the input \ntables to the output tables. after i sorted out the logic behind the transformation process, i started to \nwriting test descriptions and code in dbt to validate the data transformation accuracy. \n \nmeanwhile, i attended a dbt cloud demo workshop which helped me to better understand data \nengineering components in dbt. in addition, i continued to learn how to better utilize different \nfunctions in dbt from the dbt fundamentals course throughout the week. i also asked for ricky’s advice \non improving the code i wrote.  \n \nover the week, debugging unit testing was the most difficult experience during this period, as it \nrequired a lot of trial and error throughout the process. despite that, it was also a rewarding experience \nas it enhanced my problem-solving skills and improved my communication skills. i was also encouraged \nto reach out for help from nice people in the team. \n \n \n \n \n \n \n22",
            "page": null,
            "goal": "week 4 - write unit tests in dbt \nthis week, the goal was to continue writing unit tests in dbt. given more understanding on unit testing, \ni decided to further examine the data before jumping to write the code. i run sql query to examine the \noutput tables in snowflake. i applied knowledge i learned from the course ‘comp6350 data system’ \nand drew er diagrams to help me understand what and how data was transformed from the input \ntables to the output tables. after i sorted out the logic behind the transformation process, i started to \nwriting test descriptions and code in dbt to validate the data transformation accuracy. \n \nmeanwhile, i attended a dbt cloud demo workshop which helped me to better understand data \nengineering components in dbt. in addition, i continued to learn how to better utilize different \nfunctions in dbt from the dbt fundamentals course throughout the week. i also asked for ricky’s advice \non improving the code i wrote.  \n \nover the week, debugging unit testing was the most difficult experience during this period, as it \nrequired a lot of trial and error throughout the process. despite that, it was also a rewarding experience \nas it enhanced my problem-solving skills and improved my communication skills. i was also encouraged \nto reach out for help from nice people in the team. \n \n \n \n \n \n \n22",
            "children": []
        },
        {
            "id": "1.9",
            "name": "week 5",
            "nodeType": "title",
            "text": "week 5",
            "page": null,
            "goal": "week 5",
            "children": []
        },
        {
            "id": "1.10",
            "name": "week 5- create",
            "nodeType": "paragraph",
            "text": "week 5- create stored procedures in snowflake \nthe goal of this week was to improve the stored procedures for automating data extraction for drafting \nunit testing in snowflake.  \n \nafter completing the unit testing tasks last week, my supervisor, ricky, drafted a stored procedure in \nsnowflake to facilitate the unit testing in dbt. before that, we need to manually type the output result \nas sql select statement for dbt to read and generate a table for unit testing. therefore, the stored \nprocedure aimed to automatically generate the result output as sql select statement from a normal \nsql input query, so users don’t need to do it manually. \n \nmy task was to improve the stored procedures so when we call the function, it can automatically get \nthe schema name and table name from an input query and return the output table as a sql select \nstatement for use in dbt unit testing. since i didn’t have experience in writing stored procedure, i \nconstantly reached out to ricky for advice. \n \nover the week, the task was difficult, especially since snowflake does not support mysql language. i \nmust rewrite my code to match with snowflake sql language. moreover, debugging the code in the \nstored procedure was challenging and required a lot of time to test and troubleshoot. it is normal to \nget stuck when writing code, but i learn to use different debugging tools that can help me identify the \nproblem and suggest possible solutions with a fresh eye. i also strengthen my patience when coping \nwith difficult coding problems. overall, the experience was rewarding.",
            "page": null,
            "goal": "week 5- create stored procedures in snowflake \nthe goal of this week was to improve the stored procedures for automating data extraction for drafting \nunit testing in snowflake.  \n \nafter completing the unit testing tasks last week, my supervisor, ricky, drafted a stored procedure in \nsnowflake to facilitate the unit testing in dbt. before that, we need to manually type the output result \nas sql select statement for dbt to read and generate a table for unit testing. therefore, the stored \nprocedure aimed to automatically generate the result output as sql select statement from a normal \nsql input query, so users don’t need to do it manually. \n \nmy task was to improve the stored procedures so when we call the function, it can automatically get \nthe schema name and table name from an input query and return the output table as a sql select \nstatement for use in dbt unit testing. since i didn’t have experience in writing stored procedure, i \nconstantly reached out to ricky for advice. \n \nover the week, the task was difficult, especially since snowflake does not support mysql language. i \nmust rewrite my code to match with snowflake sql language. moreover, debugging the code in the \nstored procedure was challenging and required a lot of time to test and troubleshoot. it is normal to \nget stuck when writing code, but i learn to use different debugging tools that can help me identify the \nproblem and suggest possible solutions with a fresh eye. i also strengthen my patience when coping \nwith difficult coding problems. overall, the experience was rewarding.",
            "children": []
        },
        {
            "id": "1.11",
            "name": "week 6",
            "nodeType": "title",
            "text": "week 6",
            "page": null,
            "goal": "week 6",
            "children": []
        },
        {
            "id": "1.12",
            "name": "week 6 -",
            "nodeType": "paragraph",
            "text": "week 6 - learn to use command line tool in managing workflows \nthe goal for this week was to learn how to manage multiple workflows using command line tool. \n \non tuesday, ricky reviewed my code in the stored procedures i drafted last week, and we worked \ntogether to debug the code. we successfully resolved the issues in the coding and completed the stored \nprocedure that could go live and used by others to facilitate unit testing. ricky also discussed with me \n \n \n23 \n \non possible future improvement in the process. i learned how to work and think like a data engineer \nfrom this experience.  \n \ni also had a great chat with my mentor, chenxuan, who returned to office from parental leave. he \nshared a lot of his work experiences and tips with me. he recommended me to use command line tool \nsuch as iterm to better manage different versions of software under different projects. it is very \ncommon for data engineer to use cli instead of gui as this allow more flexibility in code developing \nand managing platforms. \n \nover the week, i installed all necessary tools for new terminal setup and used them to explore svp data \nto get familiar with the tool. i watched on youtube to understand the basic function in cli. in addition, \ni learned how to manage multiple versions of software, such as python, dbt and snowflake, in the \ncomputer for matching different project requirements.",
            "page": null,
            "goal": "week 6 - learn to use command line tool in managing workflows \nthe goal for this week was to learn how to manage multiple workflows using command line tool. \n \non tuesday, ricky reviewed my code in the stored procedures i drafted last week, and we worked \ntogether to debug the code. we successfully resolved the issues in the coding and completed the stored \nprocedure that could go live and used by others to facilitate unit testing. ricky also discussed with me \n \n \n23 \n \non possible future improvement in the process. i learned how to work and think like a data engineer \nfrom this experience.  \n \ni also had a great chat with my mentor, chenxuan, who returned to office from parental leave. he \nshared a lot of his work experiences and tips with me. he recommended me to use command line tool \nsuch as iterm to better manage different versions of software under different projects. it is very \ncommon for data engineer to use cli instead of gui as this allow more flexibility in code developing \nand managing platforms. \n \nover the week, i installed all necessary tools for new terminal setup and used them to explore svp data \nto get familiar with the tool. i watched on youtube to understand the basic function in cli. in addition, \ni learned how to manage multiple versions of software, such as python, dbt and snowflake, in the \ncomputer for matching different project requirements.",
            "children": []
        },
        {
            "id": "1.13",
            "name": "week 7",
            "nodeType": "title",
            "text": "week 7",
            "page": null,
            "goal": "week 7",
            "children": []
        },
        {
            "id": "1.14",
            "name": "week 7 -",
            "nodeType": "paragraph",
            "text": "week 7 - use git and github for version control \nthe goal for this week was to get familiar with git and github and use them for version control in \nmanaging projects. \n \nthis week, my mentor, chenxuan, walked me through how to use git and github in our company \nsetting. he also assisted me in setting up the tooling and provided a lot of guidance on learning git. i \nfollowed his suggestions and took a crash course on youtube offered by freecodecamp.org to get a \nbetter understanding on how to use github. i followed the instruction to create repository in github \nand learned how to commit, push, and pull request from github. since i use visual studio code (vs \ncode) as my terminal tool, i also walked through the tutorial on how to use source control in vs code \non its website. \n \n \n \n24 \n \nover the week, i started to use git for version controls. i created branch and explored data in dbt \nthrough vs code. i became more familiar with cli and git. this helped me establish a good practice at \nwork. \n \nbesides, on thursday, we had the midterm presentation on zoom to report what we have done in our \ninternship so far. it was interesting to know what other students did in their internship and what they \nhave learned so far.",
            "page": null,
            "goal": "week 7 - use git and github for version control \nthe goal for this week was to get familiar with git and github and use them for version control in \nmanaging projects. \n \nthis week, my mentor, chenxuan, walked me through how to use git and github in our company \nsetting. he also assisted me in setting up the tooling and provided a lot of guidance on learning git. i \nfollowed his suggestions and took a crash course on youtube offered by freecodecamp.org to get a \nbetter understanding on how to use github. i followed the instruction to create repository in github \nand learned how to commit, push, and pull request from github. since i use visual studio code (vs \ncode) as my terminal tool, i also walked through the tutorial on how to use source control in vs code \non its website. \n \n \n \n24 \n \nover the week, i started to use git for version controls. i created branch and explored data in dbt \nthrough vs code. i became more familiar with cli and git. this helped me establish a good practice at \nwork. \n \nbesides, on thursday, we had the midterm presentation on zoom to report what we have done in our \ninternship so far. it was interesting to know what other students did in their internship and what they \nhave learned so far.",
            "children": []
        },
        {
            "id": "1.15",
            "name": "week 8",
            "nodeType": "title",
            "text": "week 8",
            "page": null,
            "goal": "week 8",
            "children": []
        },
        {
            "id": "1.16",
            "name": "week 8 –",
            "nodeType": "paragraph",
            "text": "week 8 – explore piperider \nthe goal for this week was to explore piperider, understand business metrics in data, and get myself \nprepared for the upcoming tasks using piperider.  \n \npiperider is a data reliability tool that works with dbt. i follow the tutorial on piperider website and \nread the documentation to understand how to use it with dbt. i also search online to understand what \nbusiness metrics are in the sense of data. i installed piperider on vs code and explored it with the svp \ndataset.  \n \nthe main challenge was installing piperider on vs code due to configuration issue. since i was not so \nfamiliar with cli tools and how to setup the tool, it took me a whole day to setup. i also need to ask for \nmy mentor’s help throughout the process.  but i also learned that it was something you will always \nencounter when dealing with data tools setup. since lots of tools very new and open source, you need \nto read through the documentation to see how to configure the settings and manage different versions \nto ensure they are compatible with other tools you used. this is something i need to work on to develop \nmindset of data engineer. it was also nice to talk with my mentor as he gave me a lot of advice on using \ncli tools, he shared his past work experiences as a junior data engineer and how he learned and \novercome the challenges in his previous job. i learned a lot from him.  \n \n \n \n \n \n \n25",
            "page": null,
            "goal": "week 8 – explore piperider \nthe goal for this week was to explore piperider, understand business metrics in data, and get myself \nprepared for the upcoming tasks using piperider.  \n \npiperider is a data reliability tool that works with dbt. i follow the tutorial on piperider website and \nread the documentation to understand how to use it with dbt. i also search online to understand what \nbusiness metrics are in the sense of data. i installed piperider on vs code and explored it with the svp \ndataset.  \n \nthe main challenge was installing piperider on vs code due to configuration issue. since i was not so \nfamiliar with cli tools and how to setup the tool, it took me a whole day to setup. i also need to ask for \nmy mentor’s help throughout the process.  but i also learned that it was something you will always \nencounter when dealing with data tools setup. since lots of tools very new and open source, you need \nto read through the documentation to see how to configure the settings and manage different versions \nto ensure they are compatible with other tools you used. this is something i need to work on to develop \nmindset of data engineer. it was also nice to talk with my mentor as he gave me a lot of advice on using \ncli tools, he shared his past work experiences as a junior data engineer and how he learned and \novercome the challenges in his previous job. i learned a lot from him.  \n \n \n \n \n \n \n25",
            "children": []
        },
        {
            "id": "1.17",
            "name": "week 9",
            "nodeType": "title",
            "text": "week 9",
            "page": null,
            "goal": "week 9",
            "children": []
        },
        {
            "id": "1.18",
            "name": "week 9 -",
            "nodeType": "paragraph",
            "text": "week 9 - diagnostic analysis on property profile \nthe goal for this week was to run diagnostic analysis on the property profile dataset. the aim was to \nidentify the difference in property profile between internal svp data and external apm data source. \n \ni used sql to query data from snowflake, drew relevant columns from the two datasets and combined \nthem into one table for comparison. then i analyzed the data and identifies what caused the difference \nin the property details, such as property type, property category and valuation, between svp and apm \ndatasets. i summarized my findings on the confluence page.  \n \nthe most rewarding part of this week was that i could find some interesting findings from my analysis \nwhich provided business value for the team. i found out that there are a lot of typo and mistake in the \napm data causing duplication and misclassification of the property type and category. i also found that \nthe mapping logic is different between the two datasets. in addition, i identified abnormal calculation \nin the property valuation in the svp dataset. it was excited that my findings could provide value to the \nteam and narrow down the focus on resolving the difference between the two datasets.",
            "page": null,
            "goal": "week 9 - diagnostic analysis on property profile \nthe goal for this week was to run diagnostic analysis on the property profile dataset. the aim was to \nidentify the difference in property profile between internal svp data and external apm data source. \n \ni used sql to query data from snowflake, drew relevant columns from the two datasets and combined \nthem into one table for comparison. then i analyzed the data and identifies what caused the difference \nin the property details, such as property type, property category and valuation, between svp and apm \ndatasets. i summarized my findings on the confluence page.  \n \nthe most rewarding part of this week was that i could find some interesting findings from my analysis \nwhich provided business value for the team. i found out that there are a lot of typo and mistake in the \napm data causing duplication and misclassification of the property type and category. i also found that \nthe mapping logic is different between the two datasets. in addition, i identified abnormal calculation \nin the property valuation in the svp dataset. it was excited that my findings could provide value to the \nteam and narrow down the focus on resolving the difference between the two datasets.",
            "children": []
        },
        {
            "id": "1.19",
            "name": "week 10",
            "nodeType": "title",
            "text": "week 10",
            "page": null,
            "goal": "week 10",
            "children": []
        },
        {
            "id": "1.20",
            "name": "week 10 -",
            "nodeType": "paragraph",
            "text": "week 10 - diagnostic analysis on core listing activity \nthe goal for this week was to run diagnostic analysis on the core listing activity dataset.  \n \nsimilar to last week, i used sql to query data from snowflake, drew relevant columns from the two \ndatasets and combined them into one table for comparison. then i analyzed the data and identifies \nwhat caused the difference in the core listing activity, such as activity start date, activity end date, \nactivity type and activity channel, between svp and apm datasets. i then summarized my findings on \nthe confluence page.  \n \nagain, the most rewarding part of this week was that i could find some interesting findings from my \nanalysis which provided business value for the team. i provided comparison of number of sales and \nrent activities with different level of tolerance in activity start date, such as 1 day, 1 week and 1 month. \n \n \n26 \n \ni also identify there is mismatch in activity channel and activity type between the two datasets. it was \nexcited to see my findings could provide value to the team and narrow down the focus on resolving the \ndifference between the two datasets.",
            "page": null,
            "goal": "week 10 - diagnostic analysis on core listing activity \nthe goal for this week was to run diagnostic analysis on the core listing activity dataset.  \n \nsimilar to last week, i used sql to query data from snowflake, drew relevant columns from the two \ndatasets and combined them into one table for comparison. then i analyzed the data and identifies \nwhat caused the difference in the core listing activity, such as activity start date, activity end date, \nactivity type and activity channel, between svp and apm datasets. i then summarized my findings on \nthe confluence page.  \n \nagain, the most rewarding part of this week was that i could find some interesting findings from my \nanalysis which provided business value for the team. i provided comparison of number of sales and \nrent activities with different level of tolerance in activity start date, such as 1 day, 1 week and 1 month. \n \n \n26 \n \ni also identify there is mismatch in activity channel and activity type between the two datasets. it was \nexcited to see my findings could provide value to the team and narrow down the focus on resolving the \ndifference between the two datasets.",
            "children": []
        },
        {
            "id": "1.21",
            "name": "week 11",
            "nodeType": "title",
            "text": "week 11",
            "page": null,
            "goal": "week 11",
            "children": []
        },
        {
            "id": "1.22",
            "name": "week 11 –",
            "nodeType": "paragraph",
            "text": "week 11 – build business metrics in piperider \nthe goal for this week was to build business metrics for the two datasets: property profile and core \nlisting activity in piperider.  \n \ni have spent some time exploring piperider and bisiness metrics during week 8. this week was to \nexecute it. however, i encountered blockage when setting it up on my desktop. i spent half day \ndebugging the setting to configure piperider with dbt in vs code. with the help of my mentor, i finally \nsuccessfully ran dbt and piperider in vs code. \n \nthen i spent the rest of the week to understand business metrics and write business metrics in .ymal \nfile in dbt and ran it with piperider. i have created eight business metrics for the property profile \ndataset. the aim of business metrics is to keep track of the data quality over time and to observe is \nthere any abnormal behaviors in the data.  in piperider, it will generate line graph or bar chart to \nvisualize the data so you can get a quick overview of the data you want to focus on and to analyze it \nwith time constraint. all my findings were documented on confluence page. \n \nthe best thing of this week was that i learned more about quality data and business metrics. i was able \nto build some of it and to provide some useful information to the team using the metrics. i also learned \nhow to compare report in piperider. this is very useful when you want to have a quick understanding \nof the data to see what the differences between the base and the target datasets are. piperider also \nprovide suggested data assertions so you can easily apply them to your data to enforce data quality \nassessments by just a few clicks.  \n \n \n \n \n \n \n27",
            "page": null,
            "goal": "week 11 – build business metrics in piperider \nthe goal for this week was to build business metrics for the two datasets: property profile and core \nlisting activity in piperider.  \n \ni have spent some time exploring piperider and bisiness metrics during week 8. this week was to \nexecute it. however, i encountered blockage when setting it up on my desktop. i spent half day \ndebugging the setting to configure piperider with dbt in vs code. with the help of my mentor, i finally \nsuccessfully ran dbt and piperider in vs code. \n \nthen i spent the rest of the week to understand business metrics and write business metrics in .ymal \nfile in dbt and ran it with piperider. i have created eight business metrics for the property profile \ndataset. the aim of business metrics is to keep track of the data quality over time and to observe is \nthere any abnormal behaviors in the data.  in piperider, it will generate line graph or bar chart to \nvisualize the data so you can get a quick overview of the data you want to focus on and to analyze it \nwith time constraint. all my findings were documented on confluence page. \n \nthe best thing of this week was that i learned more about quality data and business metrics. i was able \nto build some of it and to provide some useful information to the team using the metrics. i also learned \nhow to compare report in piperider. this is very useful when you want to have a quick understanding \nof the data to see what the differences between the base and the target datasets are. piperider also \nprovide suggested data assertions so you can easily apply them to your data to enforce data quality \nassessments by just a few clicks.  \n \n \n \n \n \n \n27",
            "children": []
        },
        {
            "id": "1.23",
            "name": "week 12",
            "nodeType": "title",
            "text": "week 12",
            "page": null,
            "goal": "week 12",
            "children": []
        },
        {
            "id": "1.24",
            "name": "week 12 -",
            "nodeType": "paragraph",
            "text": "week 12 - data profiling and assess data quality using piperider \nthe goal for this week was to continue building business metrics on the core listing activity dataset, \nrunning data profiling on both property profile and core listing activity datasets, and assessing the data \nquality using piperider.  \n \ni spent a day to create six business metrics for the core listing activity dataset. it was a different dataset \nwith property profile.  core listing activity focused on analyzing activity start date and activity channel. \nthe main purpose was to compare the sale and rent activity across time.  \n \nthe rest of the week was to investigate some abnormal data findings from the leadscope team. the \nmain problem was that there is mismatch in the apm and svp data for the core listing activity dataset. \nmy duty was to look into it and identify whether svp data is truly reflecting the data in apm dataset. \nthe goal was to close the gap between them. i queried data of the two tables from snowflake and \nanalyzed the differences. the i summarized my findings to my mentor and completed my work for the \nweek.  \n \nthe internship was approaching the end and i have summarized and updated all my findings of the \nanalysis i did in the past few weeks on the confluence page. especially for comparison between svp \nand apm data on property profile and core listing activity datasets. also included business metrics and \ndata profiling of the data.",
            "page": null,
            "goal": "week 12 - data profiling and assess data quality using piperider \nthe goal for this week was to continue building business metrics on the core listing activity dataset, \nrunning data profiling on both property profile and core listing activity datasets, and assessing the data \nquality using piperider.  \n \ni spent a day to create six business metrics for the core listing activity dataset. it was a different dataset \nwith property profile.  core listing activity focused on analyzing activity start date and activity channel. \nthe main purpose was to compare the sale and rent activity across time.  \n \nthe rest of the week was to investigate some abnormal data findings from the leadscope team. the \nmain problem was that there is mismatch in the apm and svp data for the core listing activity dataset. \nmy duty was to look into it and identify whether svp data is truly reflecting the data in apm dataset. \nthe goal was to close the gap between them. i queried data of the two tables from snowflake and \nanalyzed the differences. the i summarized my findings to my mentor and completed my work for the \nweek.  \n \nthe internship was approaching the end and i have summarized and updated all my findings of the \nanalysis i did in the past few weeks on the confluence page. especially for comparison between svp \nand apm data on property profile and core listing activity datasets. also included business metrics and \ndata profiling of the data.",
            "children": []
        },
        {
            "id": "1.25",
            "name": "week 13",
            "nodeType": "title",
            "text": "week 13",
            "page": null,
            "goal": "week 13",
            "children": []
        },
        {
            "id": "1.26",
            "name": "week 13 –",
            "nodeType": "paragraph",
            "text": "week 13 – final presentation \nthis is the final week of the internship. all the interns presented our works to the data department \nteam leaders. we had a great closure of our internship.",
            "page": null,
            "goal": "week 13 – final presentation \nthis is the final week of the internship. all the interns presented our works to the data department \nteam leaders. we had a great closure of our internship.",
            "children": []
        }
    ]
}