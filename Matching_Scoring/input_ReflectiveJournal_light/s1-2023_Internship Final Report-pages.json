{
    "id": "1",
    "name": "Reflective journal Entities",
    "nodeType": "section",
    "text": "week 1 \nin week 1, i learned about the technological changes that occur and how they impact \ninvestigations in the modern age.  learned about the 7 phases that need to be followed in the \n17 \n \nincident response life cycle to manage incidents from detection to resolution. the 7-phase \nthe model can be used and adapted to fit almost any incident. also gained knowledge about \nhow some steps can be followed to help reduce incidents. the actions and steps will be valuable \nnot just during investigations but also as \"remediation solutions\" in a final report of the \nincident response. i learned how threat landscape can be a dynamically changing ecosystem \nand the primary factors influencing these changes are an increase in the complexity of it \nproducts, \nvarious \nexternal \nfactors \nlike \nthe \nfinancial \ncrisis, \nnew vulnerabilities, the sophistication of available tools and attacks, resources, available skills, \nnetworks facilitating knowledge transfer, and growth in illegal profits in cyberspace. got a \nthe clear understanding of the difference between cyber adversaries and cyber defenders and \nthe role of the nation states and non-nation states in cyberattacks. learned economic \ncrimes and identity theft and how these crime types can generate significant profit for \noffenders and causes considerable financial loss for the victims. gained knowledge on how \nintellectual property theft and piracy occur, and how people give free passes of their \npersonal information over the internet media has opened the doors for cyber stalking and \nonline harassment and given rise to identity theft. learned about the legal challenges in \ndigital investigations due to changing privacy and data protection laws across borders. got \nknowledge of different amendments under the electronics communication privacy act got \nknowledge of different amendments under the electronics communication privacy act. got \nto know about the challenges faced during cybercrime investigations and international \nenforcement. learned about different technical websites for cyber security. during the \nreporting period, knowing more about occurrences and how to address them captivated my \ninterest and alleviated any stress or fatigue, making it one of the most delightful experiences. \nit was also enlightening to discover more about the c-cirm objectives and requirements. \n \n• week 2 \n \nin week 2, i was able to get knowledge of the incident response plan. i was able to learn the \nimportance of an incident response plan and know that without an incident response plan, a \ncompany may fail to identify an attack in the first place, or, if it is, it may fail to take the \nrequired precautions to limit damage, get rid of the attackers, and recover safely. i was able to \nlearn the capability of building an effective incident response plan. learned how external and \ninternal communication procedures take place and how teams should explicitly define primary \ndeliverables, including a realistic target completion time and recurrence intervals. we were \nalso taught that teams should plan for everything and should prepare a field forensic kit ready \nin case of an emergency which can be used as a set of shared resources in addition to daily use \nat their assigned workstations. got to know about the admissibility of forensic evidence and \nusing write blockers for all interface types so that the pieces of evidence remain safe and \ncannot be tampered with. i learned how a strong cybersecurity program must have an incident \nresponse plan in place for handling incidents. its goal is to create and test precise \ncountermeasures that an organization might and probably ought to implement to lessen the \neffects of a breach from both internal and external threats. while not all attacks can be stopped, \nan organization's incident response strategy should place a strong emphasis on \nreadiness, adaptability, and anticipation. writing an incident response plan that considers how \nan occurrence that would affect the entire security posture is impossible without knowledge \n18 \n \nabout an organization's continuity strategy, failovers, or most crucial systems. got an in-depth \nknowledge of collecting initial facts which comes under the category of investigative \nchecklists which are not all-inclusive but are typically common and useful for incident \nresponse investigations. it is essential to retain case notes and keep them professional from \nthe initial detection and notification to the final report and disclosure documents. also, it is \ncrucial to understand that events will not always be included in chronological sequence when \ncreating an assault chronology. (when discovered, not when they happened) vulnerability \nassessment and management were explained. there will always be several difficulties due to \nthe volume of data used for vulnerability assessment and management at any given time but \nstill, it’s necessary to prioritize vulnerability and risk. before doing vulnerability assessments, \neach asset must be profiled to provide a risk sensitivity score and a workflow should be defined \nfor processing risks. was able to learn and understand how to examine a vulnerability \nassessment for information that influences the likelihood and seriousness of the potential \ndanger. learned that - intelligence services, vendor announcements, published research, \nscanning, and other methods can all be used to find sources of vulnerability information and \nthe risk ratings can be adjusted accordingly and detailed analysis can be conducted. an efficient \nworkflow that is comprised of efficient processes and procedures for addressing vulnerabilities \nmust be created after vulnerabilities have been classified by severity level. \n \n• week 3 \n \nthis week was a very important long week and we learned a lot of essential and important \nskills during this duration of the internship.  we got knowledge of baselining networks helps \nadministrators identify and define what is normal for enterprise networks and identify patterns \nthat indicate signs of trouble. learned about network sensor deployment and how the \nplacement of the network sensor is possibly the most important factor in setting up an effective \nmonitoring system and how incident response teams can utilize statistical monitoring to help \nidentify malicious communications and unauthorized port scans. got a clear understanding of \nheader and full packet logging which serves the purpose of collecting transferred system data \nand closely monitoring the network activity. i learned indicators of compromise (iocs) which \nserve as forensic evidence of potential intrusions on a host system or network that can be used \nto enable information security (infosec) professionals and system administrators to detect \nintrusion attempts or other malicious activities. got knowledge of the potential signs of \ncompromise in a network and the need for network monitoring and taking notes of the \npotentially actionable information while generating a timeline of events taking place in a \nnetwork. the iterative lead-gathering process was taught stating that leads have detectable \ncharacteristics which can be turned into indicators that can be used to generate reliable \ninformation. also learned the ioc creation process which can be defined as the process of \ndocumenting the characteristics and the artifacts of an incident in a structured manner. got an \nunderstanding of event-based alert monitoring, snort, and snort rules which helps in identifying \nthe difference between normal internet activities and malicious activities in real-time. \n \ninvestigative principles and the lead and evidence development process were explained this \nweek. every case type and investigation has its own considerations whether the goal is \nlitigation or developing a stronger security posture and for that understanding, the elements of \nproof are a must. i was able to acquire knowledge of locard’s exchange principle which states \nthat “when a person or object comes into contact with another person or object, a cross-transfer \nof materials occurs.” according to locard's concept in forensic science, both items taken from \nand brought to the crime scene by the offender can be utilized as forensic evidence. all the \n19 \n \nnecessary steps, tasks, and actions that should be performed at the crime scene were taught \nalong with the incident scene management process. how we should approach and secure the \nincident scene by determining scene boundaries, a detailed search of the scene should be \nconducted by evaluating physical and digital evidence possibilities. photos and videos should \nbe captured. all the evidence should be collected, recorded, and preserved for further \ninvestigation. a final survey should be conducted before releasing the scene. learned that \nincident scene management and evidence management should be incorporated into the \ninvestigation toolkit. the toolkit should have search warrant consent and authorization forms \nalong with evidence collection containers, evidence labels, identifiers, seals, notes, maps, \ndrawings, digital cameras, memory, and batteries. gained an understanding of the evidence \ndynamics by the behavior of the evidence. the chain of custody form should be filled out after \nthe investigation is completed which includes the date and time of the evidence acquisition, \nprocesses, and procedures performed during the investigation, the person handling the \nevidence, original location of the collected evidence, method of evidence collection with the \nmethod of examination and analysis, and the reason for collecting the evidence. chain of \ncustody is basically the documentation of acquisition, control, analysis, and disposition of \nphysical and digital evidence. interviews with detailed documentation should also be \nperformed by creating a comprehensive question list that can act as the main tool in the \ninvestigation process. in domain 9, i was able to learn the collection and analysis of threat \nintelligence with an understanding of the maneuver warfare mindset. learned about the ooda \nloop concept which can help observe, orient, decide, act, and evaluate the threat. got \nknowledge to deal with the unexpected in addition to refining our decision-making and \ncomprehending our strengths and weaknesses. the centre of gravity was explained stating \nevery organization has a core area of strength and capability that is both present and being used \nand flexibility and adaptability can be attained by understanding an organization’s capabilities \nand training to address multiple address scenarios. the threat intelligence cycle was introduced \nto take raw information and produce a product that can be used for action. \n \n• week 4 \n \nweek 4 provided me with a thorough understanding of network and digital forensics. i learned \nabout the general method of completing the analysis. the need of setting realistic and \ndemanding goals, as well as the need to investigate potential locations and data sources and \nunderstand how to use them. it explained how to access the encountered data and how to handle \nthe evidence. learning to explain the investigation's approach, which may generate clues, and \nto explore the systems for artifacts of data alteration or theft, which may expose the attacker's \ntactics. i was able to learn the technique required for malware identification, as well as analysis \nmethods and specialist forensic tools, as well as their limitations. it was discovered that sorting \nand filtering data is a critical step and that the incident response team should do conversion \nand normalization when conducting data analysis. the notion of file carving was taught, which \nis the process of reassembling computer files from pieces in the absence of filesystem metadata. \n \nfirst responder responsibilities were covered during this week of the internship. the first \nperson to identify the event scene should document the current situation of the incident \nlocation. the scene area should be secured with something obvious stating that the scene is \noff-limits, and the area should be physically monitored. we discovered the order of volatility, \nwhich relates to the order in which evidence should be gathered. evidence should be gathered \nbeginning with the most volatile and progressing to the least volatile. learned about the file \nsystem generated by the operating system, which is where the fingerprints of system users can \n20 \n \nbe located. understood the typical windows directory structure and ntfs (new technology \nfile system), the file system used by the windows nt operating system (os) for storing and \nretrieving files on hard disc drives (hdds) and solid-state devices (ssds). i now know where \nto look for endpoint data, the windows registry, deleted files, and the windows recycle bin. i \ndiscovered the distinction between free and slack space. \n \ni discovered network-based data collection systems, which collect data from the network rather \nthan from individual host devices. the devices work on the basis of a \"wiretapping concept,\" \nin which information is obtained from the network traffic stream as it travels down the network \nsegment.  grasped how network data collection enables real-time network monitoring, \nnetwork performance evaluation, network charging, and traffic testing and filtering; it aids in \nnetwork intrusion detection, identifies network vulnerabilities, measures performance, \nevaluates service levels, and even detects unusual activity in the early stages and undesirable \ntraffic control. manage to acquire the critical aspects to consider when evaluating whether a \nlive response (lr) is acceptable while investigating an incident, problems encountered while \nexecuting a live response, and tools that should be selected and reviewed while performing a \nlive response. learned about the factors that must be addressed when collecting data, as well \nas the need to constantly assess what data must be collected throughout an incident. best \npractices for live collecting were also explained, as was the need of applying them effectively. \nlearned about the steps that must be taken if something goes wrong during the live response. \n \n• week 5 \nthis week we mainly focused on malware triage, i was able to learn about triage, which is \nbasically the examination of a security incident to determine whether or not there is a security \nevent, its priority, and the need for escalation. the process requires adequate planning and \nstrategy, adequate expertise and experience, a thorough understanding of the environment, and \nappropriate tools based on the incident response team's goals. it is a dangerous task that \nshould only be performed by qualified specialists in the proper setting and only when \nabsolutely essential. i learned about malware handling protocols and how malware analysis \ndata may be used to build indicators of compromise (iocs), which can subsequently be used \nto scan a larger population of systems. processes and standards should encompass all elements \nof dealing with malware, from pre-analysis to final storage or disposition even after the case \nhas been closed. learned that any information that will help in the triage process should be \nsupplied, and analysts should question investigators when information is insufficient. malware \ndistribution should be done correctly; when giving malware to third parties, ensure that it is \ndone on your terms, after much thought and planning. acquired a clear understanding that \nteams should avoid visiting harmful websites for a variety of reasons, some of which can have \nan impact on both the victim organization and others. i discovered that malware analysis is \ndivided into two broad categories: static and dynamic analysis, and i studied the distinction \nbetween the two. static analysis searches for indications of malicious intent in files without \nexecuting the application. whereas dynamic analysis uses a closed system (also called a \nsandbox) to start the malicious application in a protected environment and just monitor what it \ndoes. \nthis week gave me a thorough grasp of the incident containment and remediation process,  and \nthe steps required to contain and mitigate the repercussions of a security issue, as well as \nidentify and remedy the incident's fundamental cause. this method is critical in digital \nforensics since it aids in limiting the damage caused by a security breach and preventing such \n21 \n \nincidents in the future. i learned about the steps involved in the containment and remediation \nprocedure and the initial steps that need to be followed in determining the nature of the \nincidence and assessing its magnitude and implications. this includes gathering information \nabout the incident, such as the type of assault, the technology and information used, and the \npotential damage. i discovered that the issue must also be managed in order to prevent more \nharm, which involves isolating impacted systems, disconnecting communication, and taking \nother actions to slow the assault's development. learned how to determine the root cause of an \nissue by studying log files, network activity, and other evidence to pinpoint the source of the \nincident. we learned to execute remediation after determining the root cause of the issue, which \nincludes correcting vulnerabilities, modifying security rules, and taking additional \nprecautionary measures to avoid such incidents in the future. \n \n• week 6 \nweek 6 was the last week of the training session for the ccirm program. during this week, i \nwas able to gain in-depth understanding of incident reporting and lessons learned as a result of \nthe containment, eradication, and recovery from a serious security issue. it requires looking \ninto the situation and discovering how it came to be in the first place. in other words, this step \nis about understanding how and why something happened and documenting each step of the \nprocess throughout the investigation. i've learned that event documentation must be done \ncorrectly and must be exact, detailed, and readable. it must handle all of the issues, meet all of \nthe minimum standards, and be done promptly. i discovered that making a report is always \nsuggested when responding to an incident or analyzing evidence, regardless of how big or small \nthe event is, and the report should fulfill all legal and policy conditions. the report information \ncan be used in a variety of ways. - provides input and status updates - passes on specific and \nunique knowledge, forces action-related ideas, assists the investigator in discovering \nconnections and finding flaws - and may be utilized for training. i figured out how to create a \nlessons-learned document for any incident response operation requiring considerable \nparticipation, planning, and execution. a lessons-learned report will greatly reduce the amount \nof time and effort required to respond to another enterprise-wide issue. \n i learned about developing playbooks and reaction scenarios, which are a low-cost and \neffective approach to practicing incident response skills and identifying potential issues with \nincident response systems. we learned about the concept of a playbook, which is essentially a \ndocumented set of instructions describing how to respond to a specific type of security risk. \ngot knowledge on playbooks that are created ahead of time and are designed to be executed in \nan organized and consistent manner. they frequently include detailed instructions for \ndiscovering, containing, and resolving the issue, as well as engaging with stakeholders and \nreporting the occurrence. i learned about response scenarios, which are hypothetical situations \nused to evaluate the success of playbooks and response plans. these scenarios are meant to \nreveal gaps in the response plan and improve overall response effectiveness by simulating real-\nworld security issues. playbooks should be examined weekly, with required updates and \nadjustments made, and annual testing planned to identify any present gaps. \nthis week i was also assigned to read for the presentation task: \n1.  forensic challenge (fc) \n22 \n \n2. introduction to cyber security forensics (icsf) \n3. battle room 9 forensics, lab guide & exercises (br9f)  \njeff asked us to read weeks 1 to 4 and follow the instructions from the forensics challenges \nthat include an introduction to cyber security forensics and provide a powerpoint presentation \nthat reports to wade six things that we have learned from two of any four weeks (three from \neach). the presentation was done in week 6 with wade.  \n \n• week 7 \nin week 7 we only had the session on monday in which jeff explained and assigned the task \nthat needs to be done during the mid-semester break. in the session, we discussed the project \nmethodologies and project methodologies can be varied depending on the type of project we \nare assigned to. with a consistent framework in place, decision-makers may establish, \nsupport, and implement the best management practices in their organizational environment. \ngot knowledge of how a good business approach raises the likelihood of success, prevents \ntime and effort waste, removes superfluous actions, and assures uniform reporting and \nanalysis. we briefly discussed the scrum methodology which is the most popular one at the \nmoment. talked about the principles of scrum and how it is an agile framework that is \nadaptable, rapid, flexible, and effective at delivering value to the customer throughout the \nproject's development. scrum masters are project managers who are well-versed in scrum \nmethodology and such methodologies are now used by the vast majority of businesses. we \nwere assigned to do detailed research on different methodologies that are used in today’s \norganizations. jeff also asked us to go through the cyber forensics document and assigned \nthe tasks for battle room 9. we learned about the autopsy process which is required for the \ntask in battle room 9. jeff walked us through all the steps required for performing the tasks \nfor the battle room because the document needs to be followed to answer the questions asked \nin the forensics challenge. we learned to export ophcrack and registry explorer to the \nvirtual desktop. \n \n• week 8 \nin week 8, we worked as a team and completed the project ares tasks within a specific time \nframe. we also had weekly tasks scheduled which we needed to report back to our company \nstakeholder wade irvin. so basically, in week 8, we started performing hands-on tasks for \nthe things which have learned during weeks 1-7. for battle room 9 we were provided a \nwindows 10 virtual computer that can be accessed via an ssh or vnc interface supplied in \nthe scenario. in this scenario, we employed forensic tools to do research and offer evidence to \nsupport an intrusion case. i was able to demonstrate the ability to conduct data recovery, disc \nimage analysis, and forensics analysis. we were also introduced to the battle room 9 \nforensic challenge guide which had step-wise guidance on how to install the particular \nforensic tools and perform tasks for that particular project area. jeff also asked us to go \nthrough forensic challenge and perform the tasks as instructed from week 1 to 4. the tool \nwe used this week was autopsy which is a digital forensics platform that also serves as a \ngraphical interface to the sleuth kit and other digital forensics tools. it is used to investigate \nincidents on a variety of computer systems by law enforcement, military, and corporate \nforensic examiners. it is also used to recover data from peripheral devices such as usb flash \ndrives, external hard drives, and digital cameras. while completing the objectives in project \nares battle room 09, we were introduced to numerous critical forensic abilities while using \nthe autopsy software. the software was used for performing timeline analysis, hash filtering, \nkeyword searches, artifact discovery, data carving, and multimedia analysis. for password-\n23 \n \ncracking tasks ophcrack tool was used which is a free open-source (gpl) program that \nbreaks windows log-in passwords using ntlm hashes and rainbow tables. a rainbow table \n(also known as a dictionary) is a precomputed table used to cache the result of cryptographic \nhash functions, typically used to crack password hashes. the program supports importing \nhashes from a variety of formats, including dumping directly from windows sam files. \nophcrack can crack most passwords in a matter of minutes on most pcs and the software is \ncompatible with windows, linux/unix, and mac os x. along with these tools we also used \nwindows registry for finding the solutions for several tasks. windows registry editor \n(regedit), commonly known as registry explorer, is a graphical application in the windows \noperating system (os) that enables authorized users to view and modify the windows \nregistry. this tool is used in network and digital forensics to allow investigators to access \nsecret systems and user files in order to obtain the important evidence that would not be \nvisible through the file explorer. this utility can retrieve user and system data, modifications \nto files and logs, timestamps, dates, passwords, and the location of deleted files. at the end of \nthe week, i gave a presentation to the stakeholder wade and the team explaining the process \nof the incident response life cycle and critical dependency on logs. \n \n• week 9 \nfor week 9, we continued doing the project areas tasks for battle room 9. we learned about \ndifferent hash functions and used md5 hash verification in the tasks to check the integrity of \na file via an alphanumeric string. also, for the tasks, we used registry explorer which \nenables us to load and browse local registry hives. it basically consists of information about \nthe windows system setup, data for a secure random number generator (rng), and the list of \ncurrently mounted systems containing a file system. we also loaded the local registry hives \nfrom a forensic image file. we used the dcode utility tool for one of the tasks to calculate \nthe date/time values from the various timestamps that may be found inside the image files. \nfrom the tasks, i learned about the unique alphanumeric identifier called the security id \n(sid). the relative id (rid) is the last part of sid which uniquely identifies a security \nprincipal relative to the local or domain security authority that issued the sid. after \nexporting the ophcrack to the desktop in week 8 we utilized the tool this week for finding out \nthe password for the systems. it uses the rainbow table to attempt to crack the attempts of \nwindows accounts when given the appropriate registry hives. i was able to learn about ntfs \nfile system which is a proprietary journaling file system developed by microsoft which is \nused for storing, organizing, and finding files on hard disk efficiently.  we were asked to go \nthrough all the domains and perform the quiz for that particular domain. i was also asked to \npresent a power-point presentation to the stakeholder wade and the team explaining static \nand live digital forensics: methods, practices, and tools. i was able to learn about static \nanalysis which is a traditional approach in which the system is analyzed forensically after \ntaking the memory dump and shutting down the system. it is focused on examining a \nduplicate copy of the disc to extract memory contents, such as deleted files, history of web \nbrowsing, file fragments, network connections, open files, user login history, and so on to \ncreate a timeline that gives a view of partial or total statistics about the activities performed \non the victim’s system before shutting it down. the whole process involves analysing the \ncode, metadata, structure, and control of digital artifacts to identify malicious or suspicious \nbehavior. in static analysis, several software and hardware technologies such as fundl and \nregcon are utilised for memory dumping and sorting of evidentiary data for analysis and \n24 \n \ndisplay. forensic data is collected via various external devices such as usbs, external hard \ndrives, cds and dvds, and then brought into the forensic lab for investigators to undertake \nvarious operations/steps to forensically analyse evidentiary material. some of the tools that \nwe used in project ares while doing the static analysis were encase, ftk (forensics toolkit), \nhash keeper, caine, registry recon and safeback. while in live forensics the \ncompromised system remains operational, information is gathered, analyzed, and reports are \ngenerated in live digital forensics. the tools used for live digital forensic analysis can provide \nvery clear pictures of knowledge such as memory dumps, running processes, open network \nconnections, and unencrypted versions of encrypted files, while such memory contents \ncannot be acquired properly in static analysis. the tools used for live digital forensic analysis \ncan provide very clear pictures of knowledge such as memory dumps, running processes, \nopen network connections and unencrypted versions of encrypted files, while such memory \ncontents cannot be acquired properly in static analysis. some of the tools for live digital \nforensic are the sleuth kit, os forensics, evidence eliminator, net-sleuth, and wireshark. \n \n• week 10 \nfor week 10 we started working on dual task for 2 battle rooms. this week we started task \nwith battle room 11 as well which is basically for system security analyst where we give \ntrials on reconnaissance, domain administration, policy, log collection and system analysis. \nwe were able to learn about usb historian which is a utility by 4discovery that parses the \nusb connection history of a machine based on specific registry hives. we used plugins and \ningest modules that provided us some automated processing capabilities for the some of the \ntasks. the modules can especially be useful for parsing complex file types that are not easily \nreadable. we were able to understand the magic file number which is a file true format and \ncan often be identified based on the file signature. this type of signature basically refers to \nthe first byte of hex in the file that is unique to a specific file type. we loaded and browsed \nregistry hives exported from a forensic image file. keyword list was also created and \nsearched for via ingest modules for password cracking tasks. we did trial and error process \nfor cracking the password but then we switched to keyword searching process by loading \nregistry hives to identify potential clues in the unlocated or slack space.  \nwe also talked about procedures and framework used by governments in today’s era. jeff \ngave a brief description on the actions that can be performed in case of data breach. overall \nthe main focus was to emphasize on the need for a comprehensive understanding of \ncybersecurity beyond financial aspects, the importance of education and communication with \nstakeholders, and the responsibilities of individuals at all levels within an organization. \ni was also asked to give presentation on essential eight which was developed by the \naustralian cyber security centre (acsc) and they prioritised these mitigation strategies, in \nthe form of the strategies to mitigate cyber security incidents, to help organisations protect \nthemselves against various cyber threats. the most effective of these mitigation strategies are \nthe essential eight.  \n25 \n \nthe essential eight (e8) is basically a cybersecurity framework developed by the australian \nsignals directorate (asd) to provide guidance on mitigating targeted cyber intrusions. it \nconsists of eight essential mitigation strategies that organizations can implement to enhance \ntheir cybersecurity posture. these strategies are based on analysis of real-world incidents and \nare designed to be effective against a range of adversaries. \nthe mitigation strategies that constitute the essential eight are:      \n• application control \n• patch applications \n• configure microsoft office macro settings \n• user application hardening \n• restrict administrative privileges \n• patch operating systems \n• multi-factor authentication \n• regular backups. \nimplementing essential eight can help mitigate target cyber intrusions, and helps \norganization enhance their overall security posture and reduce the risk of successful cyber- \nattacks. cybersecurity breaches can result in financial losses, reputational damage, and legal \nimplications for organizations. by adopting the essential eight framework, organizations can \nproactively address common security vulnerabilities and reduce the likelihood and impact of \ncyber incidents. it helps in mitigating the potential risks associated with cyber threats. \n \n• week 11 \nalong with battle room 9 we started doing the task for battle room 11 which was basically \nbased on kali linux. i was able to learn all the tools required for performing the tasks in the \nkali linux. i used the apt program which is a common command line tool and package \nmanager among debian based linux system. apt is built as a high-level command line \nfunction, and apt-get or apt-cache were utilized for specialized tasks. i also got familiarized \nwith the systemctl which is a system management which is used in ubuntu based operating \nsystem to determine the status of the services which is being used. the tool can be helpful in \nmany ways like to start, stop, query, enable, disable and generally check and change a service \nstatus. i also learned the handy commands that aid in the management of systemd. for \ninstance, journalctl is a utility for manipulating data in the journal or systemd log. i also got \nequipped with windows operating tool like icacls which is a very handy tool for managing \ndirectory access control lists (dacls). it can grant and deny access, backup control lists and \nis robust enough to even manage inherited vs explicit grants and denials. another advantage \nof icacls is that the tool can be used in the scripts. it might be handy for management of a \nwindows file server. icacls can be used for new servers to distribute access to shared spaces \nbased on established policy and procedures. in this week jeff also asked me to give \npresentation on explaining what kali linux is and why organizations and engineers use it. \nkali linux is known for its extensive collection of pre-installed software and tools tailored to \nmeet the needs of penetration testers and cybersecurity professionals.  some key features and \ncapabilities of kali linux are: penetration testing tools, forensics and incident response, \n26 \n \nsecurity assessments, and customizability. performed hashing of the memory images and \nchecked if the image has been corrupted or tampered with in transit by measuring the size of \nthe file before and after the transport. were able to obtain a list of all the active network \nconnections without doing the names lookup with the help of netstat command. this \ncommand runs more quickly than the version that resolves name to the address and used in \nthe course of local troubleshooting. for example: like identifying an open port or identifying \nwhich ip and/or application is listening to a particular port. in addition to printing network \nconnections, netstat can print routing tables, interface statistics, masquerade connections and \nmulticast membership. also learned the difference between windows and linux netstat \ncommands which have very subtle differences even though the binary name is the same. was \nable to find the domain name and ip address of the windows client system with the help of \nnslookup program which is a network administration command line tool for many computers \noperating system for querying the domain name system (dns) to obtain the domain name \nor ip address mapping or for any other specific dns record. basically, the name ‘nslookup’ \nmeans ‘name server lookup’. nslookup does not use the operating system local domain name \nsystem resolver library to perform its queries, and thus may behave differently from dig. a \nforward lookup is a lookup where the name is specified for resolving the ip whereas a reverse \nlookup is where you specify the ip and it returns the name.  both entries require the \nconfiguration though. we also tested the connectivity between 2 network devices for one of \nthe tasks with the help of ping command. it works by sending a series of internet control \nmessage protocol (icmp) messages to the target host and waiting for an icmp echo message \nfrom and to the host and device and that’s how we were able to an icmp request from the \nkali client to the target machine. other than that, we used hping command for transferring \nfiles under supported protocols, handles fragmentation, as well as arbitrary packet sizes. it is \nsometimes referred to as swiss army knife for packets in which we can create custom crafted \npackets using command line switches.  \n• week 12 \nin week 12 we just focused on completing battle room 11 tasks and doing the weekly \npresentation with the stakeholder wade. we continued with the tasks and used different kali \nlinux tool. learned how to pipe the output of the system output command with the help of \nsysteminfo and findstr. tools like systeminfo are handy in themselves, but piping large \nquantities of data to findstr can help pare that data down quickly. findstr has many \nparameters that can be specified but the only one required is the string itself. also, while \nusing the command for the tasks, we realized that findstr is case sensitive while searching \nstrings in files or piped data. findstr can also be used by itself to look for data in files as well. \nalso resolved the naming issues by able to find the host name for the windows system for \nthe tasks for the battle room. learned to identify a network path with traceroute, which is a \ntroubleshooting application for measuring distance by time between routing points between \nthe client and the target. it can be used to determine where there may be network difficulties. \nthe program is typically implemented by sending udp messages with increasing time to live \nmessages with high network ports and waiting for the icmp responses indicating whether the \ntime to live has expired, which give the traceroute the ip of that hop, or whether a host or port \nis unreachable, indicating that the traceroute is complete. while the behaviour of traceroute \namong different implantation appears to be the same, there may be some differences in the \n27 \n \nusage, including the same name of the program. in this week, i gave a presentation explaining \nmicrosoft sentinel and shadow hunter and the microsoft licensing required for these tools. \ngot knowledge on these cloud native solutions that is designed to help organizations collect, \nanalyze, and respond to security threats and incidents. it combines security event monitoring \nand analytics, threat intelligence, and automated response capabilities to provide a \ncomprehensive security monitoring and management platform. it provides security \ninformation and event management (siem) and security orchestration, automation and \nresponse (soar). got knowledge on key features which includes collecting data at cloud \nscale, detecting previously undetected threats, investigating threats with artificial intelligence \nand responding to incidents rapidly with built in orchestration and automation of common \ntasks. was also able to get knowledge on shadow hunter which is a tool used for detecting \nand mitigating the risks associated with the unauthorized network access through shadow it. \nboth microsoft sentinel and shadow hunter provide organizations with the necessary tools to \nimplement and enhance their cybersecurity posture. the tools are available through microsoft \nlicensing program depending on the organization needs and deployment model. \n \n• week 13 \nweek 13 was a very relaxed week and we just completed the remaining task from battle \nroom 11. the tasks helped us to learn to create a list of scheduled tasks for the kali linux.  \nused schtasks.exe which basically enables an administrator to create, delete, query, change, \nrun and end scheduled tasks on a local or remote computer. running schtasks.exe without \narguments displays the status and next run time for each registered task. learned to use \npowershell to find out the incorrect file hash and to verify the contents of the file. got \nknowledge that some hash algorithms including md5 and sha1 are no longer considered \nsecure against attacks. basically, the goal of using a secure hash algorithm against a file is to \nmaintain the same hash value even if the file moves from location to location. a changed \nhash values represents change to the contents of the file by accident, malicious attempt or \nunauthorized usage. we used the get-filehash cmdlet tool to get the hashes of all the files in \nthe directory to compare the 2 files. during solving the task, we were able to identify unusual \noutbound network activity on the kali linux client as well as identify any processes that may \nbe potentially malicious and terminate them. we were also able to extract all the list of \nprocesses being run by the administrator by using tasklist command line tool which can be \nused to list running process locally or on networked devices. verified signature of all the \nsystem files using powershell and the get-authenticodesignature commandlet. the signature \ntools built in powershell, like get-authenticodesignature and set-authenticodesignature \nallows to get the certificate objective associated with a signed binary.  \nwas able to get knowledge on indicators of compromise (ioc) that are positively identified \nas malicious activity in the network and the system. typical iocs are virus signature and ip \naddresses, md5 hashes of malware files or urls or domain names of botnet command and \ncontrol servers. iocs are useful to a security expert and a system administrator as they may \nindicate system intrusions. for this week jeff discussed about the project methodologies and \nthe acronyms related to project management. i gained knowledge on project management, \nagile alliance, and pmbok guide. for the weekly assessment task, i gave presentation on \n28 \n \nprince2 methodology which is a project-based method for effective project management \nand provides a structured framework for managing projects. it consists of a set of principles, \nthemes, and processes that guide project management activities from initiation to closure. \nthe methodology is flexible and scalable, making it suitable for projects of various sizes and \ncomplexities.  \n \n \n10. ",
    "page": null,
    "goal": "Reflective journal Entities",
    "children": [
        {
            "id": "1.1",
            "name": "week 1",
            "nodeType": "title",
            "text": "week 1",
            "page": null,
            "goal": "week 1",
            "children": []
        },
        {
            "id": "1.2",
            "name": "week 1 in",
            "nodeType": "paragraph",
            "text": "week 1 \nin week 1, i learned about the technological changes that occur and how they impact \ninvestigations in the modern age.  learned about the 7 phases that need to be followed in the \n17 \n \nincident response life cycle to manage incidents from detection to resolution. the 7-phase \nthe model can be used and adapted to fit almost any incident. also gained knowledge about \nhow some steps can be followed to help reduce incidents. the actions and steps will be valuable \nnot just during investigations but also as \"remediation solutions\" in a final report of the \nincident response. i learned how threat landscape can be a dynamically changing ecosystem \nand the primary factors influencing these changes are an increase in the complexity of it \nproducts, \nvarious \nexternal \nfactors \nlike \nthe \nfinancial \ncrisis, \nnew vulnerabilities, the sophistication of available tools and attacks, resources, available skills, \nnetworks facilitating knowledge transfer, and growth in illegal profits in cyberspace. got a \nthe clear understanding of the difference between cyber adversaries and cyber defenders and \nthe role of the nation states and non-nation states in cyberattacks. learned economic \ncrimes and identity theft and how these crime types can generate significant profit for \noffenders and causes considerable financial loss for the victims. gained knowledge on how \nintellectual property theft and piracy occur, and how people give free passes of their \npersonal information over the internet media has opened the doors for cyber stalking and \nonline harassment and given rise to identity theft. learned about the legal challenges in \ndigital investigations due to changing privacy and data protection laws across borders. got \nknowledge of different amendments under the electronics communication privacy act got \nknowledge of different amendments under the electronics communication privacy act. got \nto know about the challenges faced during cybercrime investigations and international \nenforcement. learned about different technical websites for cyber security. during the \nreporting period, knowing more about occurrences and how to address them captivated my \ninterest and alleviated any stress or fatigue, making it one of the most delightful experiences. \nit was also enlightening to discover more about the c-cirm objectives and requirements. \n \n•",
            "page": null,
            "goal": "week 1 \nin week 1, i learned about the technological changes that occur and how they impact \ninvestigations in the modern age.  learned about the 7 phases that need to be followed in the \n17 \n \nincident response life cycle to manage incidents from detection to resolution. the 7-phase \nthe model can be used and adapted to fit almost any incident. also gained knowledge about \nhow some steps can be followed to help reduce incidents. the actions and steps will be valuable \nnot just during investigations but also as \"remediation solutions\" in a final report of the \nincident response. i learned how threat landscape can be a dynamically changing ecosystem \nand the primary factors influencing these changes are an increase in the complexity of it \nproducts, \nvarious \nexternal \nfactors \nlike \nthe \nfinancial \ncrisis, \nnew vulnerabilities, the sophistication of available tools and attacks, resources, available skills, \nnetworks facilitating knowledge transfer, and growth in illegal profits in cyberspace. got a \nthe clear understanding of the difference between cyber adversaries and cyber defenders and \nthe role of the nation states and non-nation states in cyberattacks. learned economic \ncrimes and identity theft and how these crime types can generate significant profit for \noffenders and causes considerable financial loss for the victims. gained knowledge on how \nintellectual property theft and piracy occur, and how people give free passes of their \npersonal information over the internet media has opened the doors for cyber stalking and \nonline harassment and given rise to identity theft. learned about the legal challenges in \ndigital investigations due to changing privacy and data protection laws across borders. got \nknowledge of different amendments under the electronics communication privacy act got \nknowledge of different amendments under the electronics communication privacy act. got \nto know about the challenges faced during cybercrime investigations and international \nenforcement. learned about different technical websites for cyber security. during the \nreporting period, knowing more about occurrences and how to address them captivated my \ninterest and alleviated any stress or fatigue, making it one of the most delightful experiences. \nit was also enlightening to discover more about the c-cirm objectives and requirements. \n \n•",
            "children": []
        },
        {
            "id": "1.3",
            "name": "week 2",
            "nodeType": "title",
            "text": "week 2",
            "page": null,
            "goal": "week 2",
            "children": []
        },
        {
            "id": "1.4",
            "name": "week 2 in",
            "nodeType": "paragraph",
            "text": "week 2 \n \nin week 2, i was able to get knowledge of the incident response plan. i was able to learn the \nimportance of an incident response plan and know that without an incident response plan, a \ncompany may fail to identify an attack in the first place, or, if it is, it may fail to take the \nrequired precautions to limit damage, get rid of the attackers, and recover safely. i was able to \nlearn the capability of building an effective incident response plan. learned how external and \ninternal communication procedures take place and how teams should explicitly define primary \ndeliverables, including a realistic target completion time and recurrence intervals. we were \nalso taught that teams should plan for everything and should prepare a field forensic kit ready \nin case of an emergency which can be used as a set of shared resources in addition to daily use \nat their assigned workstations. got to know about the admissibility of forensic evidence and \nusing write blockers for all interface types so that the pieces of evidence remain safe and \ncannot be tampered with. i learned how a strong cybersecurity program must have an incident \nresponse plan in place for handling incidents. its goal is to create and test precise \ncountermeasures that an organization might and probably ought to implement to lessen the \neffects of a breach from both internal and external threats. while not all attacks can be stopped, \nan organization's incident response strategy should place a strong emphasis on \nreadiness, adaptability, and anticipation. writing an incident response plan that considers how \nan occurrence that would affect the entire security posture is impossible without knowledge \n18 \n \nabout an organization's continuity strategy, failovers, or most crucial systems. got an in-depth \nknowledge of collecting initial facts which comes under the category of investigative \nchecklists which are not all-inclusive but are typically common and useful for incident \nresponse investigations. it is essential to retain case notes and keep them professional from \nthe initial detection and notification to the final report and disclosure documents. also, it is \ncrucial to understand that events will not always be included in chronological sequence when \ncreating an assault chronology. (when discovered, not when they happened) vulnerability \nassessment and management were explained. there will always be several difficulties due to \nthe volume of data used for vulnerability assessment and management at any given time but \nstill, it’s necessary to prioritize vulnerability and risk. before doing vulnerability assessments, \neach asset must be profiled to provide a risk sensitivity score and a workflow should be defined \nfor processing risks. was able to learn and understand how to examine a vulnerability \nassessment for information that influences the likelihood and seriousness of the potential \ndanger. learned that - intelligence services, vendor announcements, published research, \nscanning, and other methods can all be used to find sources of vulnerability information and \nthe risk ratings can be adjusted accordingly and detailed analysis can be conducted. an efficient \nworkflow that is comprised of efficient processes and procedures for addressing vulnerabilities \nmust be created after vulnerabilities have been classified by severity level. \n \n•",
            "page": null,
            "goal": "week 2 \n \nin week 2, i was able to get knowledge of the incident response plan. i was able to learn the \nimportance of an incident response plan and know that without an incident response plan, a \ncompany may fail to identify an attack in the first place, or, if it is, it may fail to take the \nrequired precautions to limit damage, get rid of the attackers, and recover safely. i was able to \nlearn the capability of building an effective incident response plan. learned how external and \ninternal communication procedures take place and how teams should explicitly define primary \ndeliverables, including a realistic target completion time and recurrence intervals. we were \nalso taught that teams should plan for everything and should prepare a field forensic kit ready \nin case of an emergency which can be used as a set of shared resources in addition to daily use \nat their assigned workstations. got to know about the admissibility of forensic evidence and \nusing write blockers for all interface types so that the pieces of evidence remain safe and \ncannot be tampered with. i learned how a strong cybersecurity program must have an incident \nresponse plan in place for handling incidents. its goal is to create and test precise \ncountermeasures that an organization might and probably ought to implement to lessen the \neffects of a breach from both internal and external threats. while not all attacks can be stopped, \nan organization's incident response strategy should place a strong emphasis on \nreadiness, adaptability, and anticipation. writing an incident response plan that considers how \nan occurrence that would affect the entire security posture is impossible without knowledge \n18 \n \nabout an organization's continuity strategy, failovers, or most crucial systems. got an in-depth \nknowledge of collecting initial facts which comes under the category of investigative \nchecklists which are not all-inclusive but are typically common and useful for incident \nresponse investigations. it is essential to retain case notes and keep them professional from \nthe initial detection and notification to the final report and disclosure documents. also, it is \ncrucial to understand that events will not always be included in chronological sequence when \ncreating an assault chronology. (when discovered, not when they happened) vulnerability \nassessment and management were explained. there will always be several difficulties due to \nthe volume of data used for vulnerability assessment and management at any given time but \nstill, it’s necessary to prioritize vulnerability and risk. before doing vulnerability assessments, \neach asset must be profiled to provide a risk sensitivity score and a workflow should be defined \nfor processing risks. was able to learn and understand how to examine a vulnerability \nassessment for information that influences the likelihood and seriousness of the potential \ndanger. learned that - intelligence services, vendor announcements, published research, \nscanning, and other methods can all be used to find sources of vulnerability information and \nthe risk ratings can be adjusted accordingly and detailed analysis can be conducted. an efficient \nworkflow that is comprised of efficient processes and procedures for addressing vulnerabilities \nmust be created after vulnerabilities have been classified by severity level. \n \n•",
            "children": []
        },
        {
            "id": "1.5",
            "name": "week 3",
            "nodeType": "title",
            "text": "week 3",
            "page": null,
            "goal": "week 3",
            "children": []
        },
        {
            "id": "1.6",
            "name": "week 3 this",
            "nodeType": "paragraph",
            "text": "week 3 \n \nthis week was a very important long week and we learned a lot of essential and important \nskills during this duration of the internship.  we got knowledge of baselining networks helps \nadministrators identify and define what is normal for enterprise networks and identify patterns \nthat indicate signs of trouble. learned about network sensor deployment and how the \nplacement of the network sensor is possibly the most important factor in setting up an effective \nmonitoring system and how incident response teams can utilize statistical monitoring to help \nidentify malicious communications and unauthorized port scans. got a clear understanding of \nheader and full packet logging which serves the purpose of collecting transferred system data \nand closely monitoring the network activity. i learned indicators of compromise (iocs) which \nserve as forensic evidence of potential intrusions on a host system or network that can be used \nto enable information security (infosec) professionals and system administrators to detect \nintrusion attempts or other malicious activities. got knowledge of the potential signs of \ncompromise in a network and the need for network monitoring and taking notes of the \npotentially actionable information while generating a timeline of events taking place in a \nnetwork. the iterative lead-gathering process was taught stating that leads have detectable \ncharacteristics which can be turned into indicators that can be used to generate reliable \ninformation. also learned the ioc creation process which can be defined as the process of \ndocumenting the characteristics and the artifacts of an incident in a structured manner. got an \nunderstanding of event-based alert monitoring, snort, and snort rules which helps in identifying \nthe difference between normal internet activities and malicious activities in real-time. \n \ninvestigative principles and the lead and evidence development process were explained this \nweek. every case type and investigation has its own considerations whether the goal is \nlitigation or developing a stronger security posture and for that understanding, the elements of \nproof are a must. i was able to acquire knowledge of locard’s exchange principle which states \nthat “when a person or object comes into contact with another person or object, a cross-transfer \nof materials occurs.” according to locard's concept in forensic science, both items taken from \nand brought to the crime scene by the offender can be utilized as forensic evidence. all the \n19 \n \nnecessary steps, tasks, and actions that should be performed at the crime scene were taught \nalong with the incident scene management process. how we should approach and secure the \nincident scene by determining scene boundaries, a detailed search of the scene should be \nconducted by evaluating physical and digital evidence possibilities. photos and videos should \nbe captured. all the evidence should be collected, recorded, and preserved for further \ninvestigation. a final survey should be conducted before releasing the scene. learned that \nincident scene management and evidence management should be incorporated into the \ninvestigation toolkit. the toolkit should have search warrant consent and authorization forms \nalong with evidence collection containers, evidence labels, identifiers, seals, notes, maps, \ndrawings, digital cameras, memory, and batteries. gained an understanding of the evidence \ndynamics by the behavior of the evidence. the chain of custody form should be filled out after \nthe investigation is completed which includes the date and time of the evidence acquisition, \nprocesses, and procedures performed during the investigation, the person handling the \nevidence, original location of the collected evidence, method of evidence collection with the \nmethod of examination and analysis, and the reason for collecting the evidence. chain of \ncustody is basically the documentation of acquisition, control, analysis, and disposition of \nphysical and digital evidence. interviews with detailed documentation should also be \nperformed by creating a comprehensive question list that can act as the main tool in the \ninvestigation process. in domain 9, i was able to learn the collection and analysis of threat \nintelligence with an understanding of the maneuver warfare mindset. learned about the ooda \nloop concept which can help observe, orient, decide, act, and evaluate the threat. got \nknowledge to deal with the unexpected in addition to refining our decision-making and \ncomprehending our strengths and weaknesses. the centre of gravity was explained stating \nevery organization has a core area of strength and capability that is both present and being used \nand flexibility and adaptability can be attained by understanding an organization’s capabilities \nand training to address multiple address scenarios. the threat intelligence cycle was introduced \nto take raw information and produce a product that can be used for action. \n \n•",
            "page": null,
            "goal": "week 3 \n \nthis week was a very important long week and we learned a lot of essential and important \nskills during this duration of the internship.  we got knowledge of baselining networks helps \nadministrators identify and define what is normal for enterprise networks and identify patterns \nthat indicate signs of trouble. learned about network sensor deployment and how the \nplacement of the network sensor is possibly the most important factor in setting up an effective \nmonitoring system and how incident response teams can utilize statistical monitoring to help \nidentify malicious communications and unauthorized port scans. got a clear understanding of \nheader and full packet logging which serves the purpose of collecting transferred system data \nand closely monitoring the network activity. i learned indicators of compromise (iocs) which \nserve as forensic evidence of potential intrusions on a host system or network that can be used \nto enable information security (infosec) professionals and system administrators to detect \nintrusion attempts or other malicious activities. got knowledge of the potential signs of \ncompromise in a network and the need for network monitoring and taking notes of the \npotentially actionable information while generating a timeline of events taking place in a \nnetwork. the iterative lead-gathering process was taught stating that leads have detectable \ncharacteristics which can be turned into indicators that can be used to generate reliable \ninformation. also learned the ioc creation process which can be defined as the process of \ndocumenting the characteristics and the artifacts of an incident in a structured manner. got an \nunderstanding of event-based alert monitoring, snort, and snort rules which helps in identifying \nthe difference between normal internet activities and malicious activities in real-time. \n \ninvestigative principles and the lead and evidence development process were explained this \nweek. every case type and investigation has its own considerations whether the goal is \nlitigation or developing a stronger security posture and for that understanding, the elements of \nproof are a must. i was able to acquire knowledge of locard’s exchange principle which states \nthat “when a person or object comes into contact with another person or object, a cross-transfer \nof materials occurs.” according to locard's concept in forensic science, both items taken from \nand brought to the crime scene by the offender can be utilized as forensic evidence. all the \n19 \n \nnecessary steps, tasks, and actions that should be performed at the crime scene were taught \nalong with the incident scene management process. how we should approach and secure the \nincident scene by determining scene boundaries, a detailed search of the scene should be \nconducted by evaluating physical and digital evidence possibilities. photos and videos should \nbe captured. all the evidence should be collected, recorded, and preserved for further \ninvestigation. a final survey should be conducted before releasing the scene. learned that \nincident scene management and evidence management should be incorporated into the \ninvestigation toolkit. the toolkit should have search warrant consent and authorization forms \nalong with evidence collection containers, evidence labels, identifiers, seals, notes, maps, \ndrawings, digital cameras, memory, and batteries. gained an understanding of the evidence \ndynamics by the behavior of the evidence. the chain of custody form should be filled out after \nthe investigation is completed which includes the date and time of the evidence acquisition, \nprocesses, and procedures performed during the investigation, the person handling the \nevidence, original location of the collected evidence, method of evidence collection with the \nmethod of examination and analysis, and the reason for collecting the evidence. chain of \ncustody is basically the documentation of acquisition, control, analysis, and disposition of \nphysical and digital evidence. interviews with detailed documentation should also be \nperformed by creating a comprehensive question list that can act as the main tool in the \ninvestigation process. in domain 9, i was able to learn the collection and analysis of threat \nintelligence with an understanding of the maneuver warfare mindset. learned about the ooda \nloop concept which can help observe, orient, decide, act, and evaluate the threat. got \nknowledge to deal with the unexpected in addition to refining our decision-making and \ncomprehending our strengths and weaknesses. the centre of gravity was explained stating \nevery organization has a core area of strength and capability that is both present and being used \nand flexibility and adaptability can be attained by understanding an organization’s capabilities \nand training to address multiple address scenarios. the threat intelligence cycle was introduced \nto take raw information and produce a product that can be used for action. \n \n•",
            "children": []
        },
        {
            "id": "1.7",
            "name": "week 4",
            "nodeType": "title",
            "text": "week 4",
            "page": null,
            "goal": "week 4",
            "children": []
        },
        {
            "id": "1.8",
            "name": "week 4 week",
            "nodeType": "paragraph",
            "text": "week 4 \n \nweek 4 provided me with a thorough understanding of network and digital forensics. i learned \nabout the general method of completing the analysis. the need of setting realistic and \ndemanding goals, as well as the need to investigate potential locations and data sources and \nunderstand how to use them. it explained how to access the encountered data and how to handle \nthe evidence. learning to explain the investigation's approach, which may generate clues, and \nto explore the systems for artifacts of data alteration or theft, which may expose the attacker's \ntactics. i was able to learn the technique required for malware identification, as well as analysis \nmethods and specialist forensic tools, as well as their limitations. it was discovered that sorting \nand filtering data is a critical step and that the incident response team should do conversion \nand normalization when conducting data analysis. the notion of file carving was taught, which \nis the process of reassembling computer files from pieces in the absence of filesystem metadata. \n \nfirst responder responsibilities were covered during this week of the internship. the first \nperson to identify the event scene should document the current situation of the incident \nlocation. the scene area should be secured with something obvious stating that the scene is \noff-limits, and the area should be physically monitored. we discovered the order of volatility, \nwhich relates to the order in which evidence should be gathered. evidence should be gathered \nbeginning with the most volatile and progressing to the least volatile. learned about the file \nsystem generated by the operating system, which is where the fingerprints of system users can \n20 \n \nbe located. understood the typical windows directory structure and ntfs (new technology \nfile system), the file system used by the windows nt operating system (os) for storing and \nretrieving files on hard disc drives (hdds) and solid-state devices (ssds). i now know where \nto look for endpoint data, the windows registry, deleted files, and the windows recycle bin. i \ndiscovered the distinction between free and slack space. \n \ni discovered network-based data collection systems, which collect data from the network rather \nthan from individual host devices. the devices work on the basis of a \"wiretapping concept,\" \nin which information is obtained from the network traffic stream as it travels down the network \nsegment.  grasped how network data collection enables real-time network monitoring, \nnetwork performance evaluation, network charging, and traffic testing and filtering; it aids in \nnetwork intrusion detection, identifies network vulnerabilities, measures performance, \nevaluates service levels, and even detects unusual activity in the early stages and undesirable \ntraffic control. manage to acquire the critical aspects to consider when evaluating whether a \nlive response (lr) is acceptable while investigating an incident, problems encountered while \nexecuting a live response, and tools that should be selected and reviewed while performing a \nlive response. learned about the factors that must be addressed when collecting data, as well \nas the need to constantly assess what data must be collected throughout an incident. best \npractices for live collecting were also explained, as was the need of applying them effectively. \nlearned about the steps that must be taken if something goes wrong during the live response. \n \n•",
            "page": null,
            "goal": "week 4 \n \nweek 4 provided me with a thorough understanding of network and digital forensics. i learned \nabout the general method of completing the analysis. the need of setting realistic and \ndemanding goals, as well as the need to investigate potential locations and data sources and \nunderstand how to use them. it explained how to access the encountered data and how to handle \nthe evidence. learning to explain the investigation's approach, which may generate clues, and \nto explore the systems for artifacts of data alteration or theft, which may expose the attacker's \ntactics. i was able to learn the technique required for malware identification, as well as analysis \nmethods and specialist forensic tools, as well as their limitations. it was discovered that sorting \nand filtering data is a critical step and that the incident response team should do conversion \nand normalization when conducting data analysis. the notion of file carving was taught, which \nis the process of reassembling computer files from pieces in the absence of filesystem metadata. \n \nfirst responder responsibilities were covered during this week of the internship. the first \nperson to identify the event scene should document the current situation of the incident \nlocation. the scene area should be secured with something obvious stating that the scene is \noff-limits, and the area should be physically monitored. we discovered the order of volatility, \nwhich relates to the order in which evidence should be gathered. evidence should be gathered \nbeginning with the most volatile and progressing to the least volatile. learned about the file \nsystem generated by the operating system, which is where the fingerprints of system users can \n20 \n \nbe located. understood the typical windows directory structure and ntfs (new technology \nfile system), the file system used by the windows nt operating system (os) for storing and \nretrieving files on hard disc drives (hdds) and solid-state devices (ssds). i now know where \nto look for endpoint data, the windows registry, deleted files, and the windows recycle bin. i \ndiscovered the distinction between free and slack space. \n \ni discovered network-based data collection systems, which collect data from the network rather \nthan from individual host devices. the devices work on the basis of a \"wiretapping concept,\" \nin which information is obtained from the network traffic stream as it travels down the network \nsegment.  grasped how network data collection enables real-time network monitoring, \nnetwork performance evaluation, network charging, and traffic testing and filtering; it aids in \nnetwork intrusion detection, identifies network vulnerabilities, measures performance, \nevaluates service levels, and even detects unusual activity in the early stages and undesirable \ntraffic control. manage to acquire the critical aspects to consider when evaluating whether a \nlive response (lr) is acceptable while investigating an incident, problems encountered while \nexecuting a live response, and tools that should be selected and reviewed while performing a \nlive response. learned about the factors that must be addressed when collecting data, as well \nas the need to constantly assess what data must be collected throughout an incident. best \npractices for live collecting were also explained, as was the need of applying them effectively. \nlearned about the steps that must be taken if something goes wrong during the live response. \n \n•",
            "children": []
        },
        {
            "id": "1.9",
            "name": "week 5",
            "nodeType": "title",
            "text": "week 5",
            "page": null,
            "goal": "week 5",
            "children": []
        },
        {
            "id": "1.10",
            "name": "week 5 this",
            "nodeType": "paragraph",
            "text": "week 5 \nthis week we mainly focused on malware triage, i was able to learn about triage, which is \nbasically the examination of a security incident to determine whether or not there is a security \nevent, its priority, and the need for escalation. the process requires adequate planning and \nstrategy, adequate expertise and experience, a thorough understanding of the environment, and \nappropriate tools based on the incident response team's goals. it is a dangerous task that \nshould only be performed by qualified specialists in the proper setting and only when \nabsolutely essential. i learned about malware handling protocols and how malware analysis \ndata may be used to build indicators of compromise (iocs), which can subsequently be used \nto scan a larger population of systems. processes and standards should encompass all elements \nof dealing with malware, from pre-analysis to final storage or disposition even after the case \nhas been closed. learned that any information that will help in the triage process should be \nsupplied, and analysts should question investigators when information is insufficient. malware \ndistribution should be done correctly; when giving malware to third parties, ensure that it is \ndone on your terms, after much thought and planning. acquired a clear understanding that \nteams should avoid visiting harmful websites for a variety of reasons, some of which can have \nan impact on both the victim organization and others. i discovered that malware analysis is \ndivided into two broad categories: static and dynamic analysis, and i studied the distinction \nbetween the two. static analysis searches for indications of malicious intent in files without \nexecuting the application. whereas dynamic analysis uses a closed system (also called a \nsandbox) to start the malicious application in a protected environment and just monitor what it \ndoes. \nthis week gave me a thorough grasp of the incident containment and remediation process,  and \nthe steps required to contain and mitigate the repercussions of a security issue, as well as \nidentify and remedy the incident's fundamental cause. this method is critical in digital \nforensics since it aids in limiting the damage caused by a security breach and preventing such \n21 \n \nincidents in the future. i learned about the steps involved in the containment and remediation \nprocedure and the initial steps that need to be followed in determining the nature of the \nincidence and assessing its magnitude and implications. this includes gathering information \nabout the incident, such as the type of assault, the technology and information used, and the \npotential damage. i discovered that the issue must also be managed in order to prevent more \nharm, which involves isolating impacted systems, disconnecting communication, and taking \nother actions to slow the assault's development. learned how to determine the root cause of an \nissue by studying log files, network activity, and other evidence to pinpoint the source of the \nincident. we learned to execute remediation after determining the root cause of the issue, which \nincludes correcting vulnerabilities, modifying security rules, and taking additional \nprecautionary measures to avoid such incidents in the future. \n \n•",
            "page": null,
            "goal": "week 5 \nthis week we mainly focused on malware triage, i was able to learn about triage, which is \nbasically the examination of a security incident to determine whether or not there is a security \nevent, its priority, and the need for escalation. the process requires adequate planning and \nstrategy, adequate expertise and experience, a thorough understanding of the environment, and \nappropriate tools based on the incident response team's goals. it is a dangerous task that \nshould only be performed by qualified specialists in the proper setting and only when \nabsolutely essential. i learned about malware handling protocols and how malware analysis \ndata may be used to build indicators of compromise (iocs), which can subsequently be used \nto scan a larger population of systems. processes and standards should encompass all elements \nof dealing with malware, from pre-analysis to final storage or disposition even after the case \nhas been closed. learned that any information that will help in the triage process should be \nsupplied, and analysts should question investigators when information is insufficient. malware \ndistribution should be done correctly; when giving malware to third parties, ensure that it is \ndone on your terms, after much thought and planning. acquired a clear understanding that \nteams should avoid visiting harmful websites for a variety of reasons, some of which can have \nan impact on both the victim organization and others. i discovered that malware analysis is \ndivided into two broad categories: static and dynamic analysis, and i studied the distinction \nbetween the two. static analysis searches for indications of malicious intent in files without \nexecuting the application. whereas dynamic analysis uses a closed system (also called a \nsandbox) to start the malicious application in a protected environment and just monitor what it \ndoes. \nthis week gave me a thorough grasp of the incident containment and remediation process,  and \nthe steps required to contain and mitigate the repercussions of a security issue, as well as \nidentify and remedy the incident's fundamental cause. this method is critical in digital \nforensics since it aids in limiting the damage caused by a security breach and preventing such \n21 \n \nincidents in the future. i learned about the steps involved in the containment and remediation \nprocedure and the initial steps that need to be followed in determining the nature of the \nincidence and assessing its magnitude and implications. this includes gathering information \nabout the incident, such as the type of assault, the technology and information used, and the \npotential damage. i discovered that the issue must also be managed in order to prevent more \nharm, which involves isolating impacted systems, disconnecting communication, and taking \nother actions to slow the assault's development. learned how to determine the root cause of an \nissue by studying log files, network activity, and other evidence to pinpoint the source of the \nincident. we learned to execute remediation after determining the root cause of the issue, which \nincludes correcting vulnerabilities, modifying security rules, and taking additional \nprecautionary measures to avoid such incidents in the future. \n \n•",
            "children": []
        },
        {
            "id": "1.11",
            "name": "week 6",
            "nodeType": "title",
            "text": "week 6",
            "page": null,
            "goal": "week 6",
            "children": []
        },
        {
            "id": "1.12",
            "name": "week 6 week",
            "nodeType": "paragraph",
            "text": "week 6 \nweek 6 was the last week of the training session for the ccirm program. during this week, i \nwas able to gain in-depth understanding of incident reporting and lessons learned as a result of \nthe containment, eradication, and recovery from a serious security issue. it requires looking \ninto the situation and discovering how it came to be in the first place. in other words, this step \nis about understanding how and why something happened and documenting each step of the \nprocess throughout the investigation. i've learned that event documentation must be done \ncorrectly and must be exact, detailed, and readable. it must handle all of the issues, meet all of \nthe minimum standards, and be done promptly. i discovered that making a report is always \nsuggested when responding to an incident or analyzing evidence, regardless of how big or small \nthe event is, and the report should fulfill all legal and policy conditions. the report information \ncan be used in a variety of ways. - provides input and status updates - passes on specific and \nunique knowledge, forces action-related ideas, assists the investigator in discovering \nconnections and finding flaws - and may be utilized for training. i figured out how to create a \nlessons-learned document for any incident response operation requiring considerable \nparticipation, planning, and execution. a lessons-learned report will greatly reduce the amount \nof time and effort required to respond to another enterprise-wide issue. \n i learned about developing playbooks and reaction scenarios, which are a low-cost and \neffective approach to practicing incident response skills and identifying potential issues with \nincident response systems. we learned about the concept of a playbook, which is essentially a \ndocumented set of instructions describing how to respond to a specific type of security risk. \ngot knowledge on playbooks that are created ahead of time and are designed to be executed in \nan organized and consistent manner. they frequently include detailed instructions for \ndiscovering, containing, and resolving the issue, as well as engaging with stakeholders and \nreporting the occurrence. i learned about response scenarios, which are hypothetical situations \nused to evaluate the success of playbooks and response plans. these scenarios are meant to \nreveal gaps in the response plan and improve overall response effectiveness by simulating real-\nworld security issues. playbooks should be examined weekly, with required updates and \nadjustments made, and annual testing planned to identify any present gaps. \nthis week i was also assigned to read for the presentation task: \n1.  forensic challenge (fc) \n22 \n \n2. introduction to cyber security forensics (icsf) \n3. battle room 9 forensics, lab guide & exercises (br9f)  \njeff asked us to read weeks 1 to 4 and follow the instructions from the forensics challenges \nthat include an introduction to cyber security forensics and provide a powerpoint presentation \nthat reports to wade six things that we have learned from two of any four weeks (three from \neach). the presentation was done in week 6 with wade.  \n \n•",
            "page": null,
            "goal": "week 6 \nweek 6 was the last week of the training session for the ccirm program. during this week, i \nwas able to gain in-depth understanding of incident reporting and lessons learned as a result of \nthe containment, eradication, and recovery from a serious security issue. it requires looking \ninto the situation and discovering how it came to be in the first place. in other words, this step \nis about understanding how and why something happened and documenting each step of the \nprocess throughout the investigation. i've learned that event documentation must be done \ncorrectly and must be exact, detailed, and readable. it must handle all of the issues, meet all of \nthe minimum standards, and be done promptly. i discovered that making a report is always \nsuggested when responding to an incident or analyzing evidence, regardless of how big or small \nthe event is, and the report should fulfill all legal and policy conditions. the report information \ncan be used in a variety of ways. - provides input and status updates - passes on specific and \nunique knowledge, forces action-related ideas, assists the investigator in discovering \nconnections and finding flaws - and may be utilized for training. i figured out how to create a \nlessons-learned document for any incident response operation requiring considerable \nparticipation, planning, and execution. a lessons-learned report will greatly reduce the amount \nof time and effort required to respond to another enterprise-wide issue. \n i learned about developing playbooks and reaction scenarios, which are a low-cost and \neffective approach to practicing incident response skills and identifying potential issues with \nincident response systems. we learned about the concept of a playbook, which is essentially a \ndocumented set of instructions describing how to respond to a specific type of security risk. \ngot knowledge on playbooks that are created ahead of time and are designed to be executed in \nan organized and consistent manner. they frequently include detailed instructions for \ndiscovering, containing, and resolving the issue, as well as engaging with stakeholders and \nreporting the occurrence. i learned about response scenarios, which are hypothetical situations \nused to evaluate the success of playbooks and response plans. these scenarios are meant to \nreveal gaps in the response plan and improve overall response effectiveness by simulating real-\nworld security issues. playbooks should be examined weekly, with required updates and \nadjustments made, and annual testing planned to identify any present gaps. \nthis week i was also assigned to read for the presentation task: \n1.  forensic challenge (fc) \n22 \n \n2. introduction to cyber security forensics (icsf) \n3. battle room 9 forensics, lab guide & exercises (br9f)  \njeff asked us to read weeks 1 to 4 and follow the instructions from the forensics challenges \nthat include an introduction to cyber security forensics and provide a powerpoint presentation \nthat reports to wade six things that we have learned from two of any four weeks (three from \neach). the presentation was done in week 6 with wade.  \n \n•",
            "children": []
        },
        {
            "id": "1.13",
            "name": "week 7",
            "nodeType": "title",
            "text": "week 7",
            "page": null,
            "goal": "week 7",
            "children": []
        },
        {
            "id": "1.14",
            "name": "week 7 in",
            "nodeType": "paragraph",
            "text": "week 7 \nin week 7 we only had the session on monday in which jeff explained and assigned the task \nthat needs to be done during the mid-semester break. in the session, we discussed the project \nmethodologies and project methodologies can be varied depending on the type of project we \nare assigned to. with a consistent framework in place, decision-makers may establish, \nsupport, and implement the best management practices in their organizational environment. \ngot knowledge of how a good business approach raises the likelihood of success, prevents \ntime and effort waste, removes superfluous actions, and assures uniform reporting and \nanalysis. we briefly discussed the scrum methodology which is the most popular one at the \nmoment. talked about the principles of scrum and how it is an agile framework that is \nadaptable, rapid, flexible, and effective at delivering value to the customer throughout the \nproject's development. scrum masters are project managers who are well-versed in scrum \nmethodology and such methodologies are now used by the vast majority of businesses. we \nwere assigned to do detailed research on different methodologies that are used in today’s \norganizations. jeff also asked us to go through the cyber forensics document and assigned \nthe tasks for battle room 9. we learned about the autopsy process which is required for the \ntask in battle room 9. jeff walked us through all the steps required for performing the tasks \nfor the battle room because the document needs to be followed to answer the questions asked \nin the forensics challenge. we learned to export ophcrack and registry explorer to the \nvirtual desktop. \n \n•",
            "page": null,
            "goal": "week 7 \nin week 7 we only had the session on monday in which jeff explained and assigned the task \nthat needs to be done during the mid-semester break. in the session, we discussed the project \nmethodologies and project methodologies can be varied depending on the type of project we \nare assigned to. with a consistent framework in place, decision-makers may establish, \nsupport, and implement the best management practices in their organizational environment. \ngot knowledge of how a good business approach raises the likelihood of success, prevents \ntime and effort waste, removes superfluous actions, and assures uniform reporting and \nanalysis. we briefly discussed the scrum methodology which is the most popular one at the \nmoment. talked about the principles of scrum and how it is an agile framework that is \nadaptable, rapid, flexible, and effective at delivering value to the customer throughout the \nproject's development. scrum masters are project managers who are well-versed in scrum \nmethodology and such methodologies are now used by the vast majority of businesses. we \nwere assigned to do detailed research on different methodologies that are used in today’s \norganizations. jeff also asked us to go through the cyber forensics document and assigned \nthe tasks for battle room 9. we learned about the autopsy process which is required for the \ntask in battle room 9. jeff walked us through all the steps required for performing the tasks \nfor the battle room because the document needs to be followed to answer the questions asked \nin the forensics challenge. we learned to export ophcrack and registry explorer to the \nvirtual desktop. \n \n•",
            "children": []
        },
        {
            "id": "1.15",
            "name": "week 8",
            "nodeType": "title",
            "text": "week 8",
            "page": null,
            "goal": "week 8",
            "children": []
        },
        {
            "id": "1.16",
            "name": "week 8 in",
            "nodeType": "paragraph",
            "text": "week 8 \nin week 8, we worked as a team and completed the project ares tasks within a specific time \nframe. we also had weekly tasks scheduled which we needed to report back to our company \nstakeholder wade irvin. so basically, in week 8, we started performing hands-on tasks for \nthe things which have learned during weeks 1-7. for battle room 9 we were provided a \nwindows 10 virtual computer that can be accessed via an ssh or vnc interface supplied in \nthe scenario. in this scenario, we employed forensic tools to do research and offer evidence to \nsupport an intrusion case. i was able to demonstrate the ability to conduct data recovery, disc \nimage analysis, and forensics analysis. we were also introduced to the battle room 9 \nforensic challenge guide which had step-wise guidance on how to install the particular \nforensic tools and perform tasks for that particular project area. jeff also asked us to go \nthrough forensic challenge and perform the tasks as instructed from week 1 to 4. the tool \nwe used this week was autopsy which is a digital forensics platform that also serves as a \ngraphical interface to the sleuth kit and other digital forensics tools. it is used to investigate \nincidents on a variety of computer systems by law enforcement, military, and corporate \nforensic examiners. it is also used to recover data from peripheral devices such as usb flash \ndrives, external hard drives, and digital cameras. while completing the objectives in project \nares battle room 09, we were introduced to numerous critical forensic abilities while using \nthe autopsy software. the software was used for performing timeline analysis, hash filtering, \nkeyword searches, artifact discovery, data carving, and multimedia analysis. for password-\n23 \n \ncracking tasks ophcrack tool was used which is a free open-source (gpl) program that \nbreaks windows log-in passwords using ntlm hashes and rainbow tables. a rainbow table \n(also known as a dictionary) is a precomputed table used to cache the result of cryptographic \nhash functions, typically used to crack password hashes. the program supports importing \nhashes from a variety of formats, including dumping directly from windows sam files. \nophcrack can crack most passwords in a matter of minutes on most pcs and the software is \ncompatible with windows, linux/unix, and mac os x. along with these tools we also used \nwindows registry for finding the solutions for several tasks. windows registry editor \n(regedit), commonly known as registry explorer, is a graphical application in the windows \noperating system (os) that enables authorized users to view and modify the windows \nregistry. this tool is used in network and digital forensics to allow investigators to access \nsecret systems and user files in order to obtain the important evidence that would not be \nvisible through the file explorer. this utility can retrieve user and system data, modifications \nto files and logs, timestamps, dates, passwords, and the location of deleted files. at the end of \nthe week, i gave a presentation to the stakeholder wade and the team explaining the process \nof the incident response life cycle and critical dependency on logs. \n \n•",
            "page": null,
            "goal": "week 8 \nin week 8, we worked as a team and completed the project ares tasks within a specific time \nframe. we also had weekly tasks scheduled which we needed to report back to our company \nstakeholder wade irvin. so basically, in week 8, we started performing hands-on tasks for \nthe things which have learned during weeks 1-7. for battle room 9 we were provided a \nwindows 10 virtual computer that can be accessed via an ssh or vnc interface supplied in \nthe scenario. in this scenario, we employed forensic tools to do research and offer evidence to \nsupport an intrusion case. i was able to demonstrate the ability to conduct data recovery, disc \nimage analysis, and forensics analysis. we were also introduced to the battle room 9 \nforensic challenge guide which had step-wise guidance on how to install the particular \nforensic tools and perform tasks for that particular project area. jeff also asked us to go \nthrough forensic challenge and perform the tasks as instructed from week 1 to 4. the tool \nwe used this week was autopsy which is a digital forensics platform that also serves as a \ngraphical interface to the sleuth kit and other digital forensics tools. it is used to investigate \nincidents on a variety of computer systems by law enforcement, military, and corporate \nforensic examiners. it is also used to recover data from peripheral devices such as usb flash \ndrives, external hard drives, and digital cameras. while completing the objectives in project \nares battle room 09, we were introduced to numerous critical forensic abilities while using \nthe autopsy software. the software was used for performing timeline analysis, hash filtering, \nkeyword searches, artifact discovery, data carving, and multimedia analysis. for password-\n23 \n \ncracking tasks ophcrack tool was used which is a free open-source (gpl) program that \nbreaks windows log-in passwords using ntlm hashes and rainbow tables. a rainbow table \n(also known as a dictionary) is a precomputed table used to cache the result of cryptographic \nhash functions, typically used to crack password hashes. the program supports importing \nhashes from a variety of formats, including dumping directly from windows sam files. \nophcrack can crack most passwords in a matter of minutes on most pcs and the software is \ncompatible with windows, linux/unix, and mac os x. along with these tools we also used \nwindows registry for finding the solutions for several tasks. windows registry editor \n(regedit), commonly known as registry explorer, is a graphical application in the windows \noperating system (os) that enables authorized users to view and modify the windows \nregistry. this tool is used in network and digital forensics to allow investigators to access \nsecret systems and user files in order to obtain the important evidence that would not be \nvisible through the file explorer. this utility can retrieve user and system data, modifications \nto files and logs, timestamps, dates, passwords, and the location of deleted files. at the end of \nthe week, i gave a presentation to the stakeholder wade and the team explaining the process \nof the incident response life cycle and critical dependency on logs. \n \n•",
            "children": []
        },
        {
            "id": "1.17",
            "name": "week 9",
            "nodeType": "title",
            "text": "week 9",
            "page": null,
            "goal": "week 9",
            "children": []
        },
        {
            "id": "1.18",
            "name": "week 9 for",
            "nodeType": "paragraph",
            "text": "week 9 \nfor week 9, we continued doing the project areas tasks for battle room 9. we learned about \ndifferent hash functions and used md5 hash verification in the tasks to check the integrity of \na file via an alphanumeric string. also, for the tasks, we used registry explorer which \nenables us to load and browse local registry hives. it basically consists of information about \nthe windows system setup, data for a secure random number generator (rng), and the list of \ncurrently mounted systems containing a file system. we also loaded the local registry hives \nfrom a forensic image file. we used the dcode utility tool for one of the tasks to calculate \nthe date/time values from the various timestamps that may be found inside the image files. \nfrom the tasks, i learned about the unique alphanumeric identifier called the security id \n(sid). the relative id (rid) is the last part of sid which uniquely identifies a security \nprincipal relative to the local or domain security authority that issued the sid. after \nexporting the ophcrack to the desktop in week 8 we utilized the tool this week for finding out \nthe password for the systems. it uses the rainbow table to attempt to crack the attempts of \nwindows accounts when given the appropriate registry hives. i was able to learn about ntfs \nfile system which is a proprietary journaling file system developed by microsoft which is \nused for storing, organizing, and finding files on hard disk efficiently.  we were asked to go \nthrough all the domains and perform the quiz for that particular domain. i was also asked to \npresent a power-point presentation to the stakeholder wade and the team explaining static \nand live digital forensics: methods, practices, and tools. i was able to learn about static \nanalysis which is a traditional approach in which the system is analyzed forensically after \ntaking the memory dump and shutting down the system. it is focused on examining a \nduplicate copy of the disc to extract memory contents, such as deleted files, history of web \nbrowsing, file fragments, network connections, open files, user login history, and so on to \ncreate a timeline that gives a view of partial or total statistics about the activities performed \non the victim’s system before shutting it down. the whole process involves analysing the \ncode, metadata, structure, and control of digital artifacts to identify malicious or suspicious \nbehavior. in static analysis, several software and hardware technologies such as fundl and \nregcon are utilised for memory dumping and sorting of evidentiary data for analysis and \n24 \n \ndisplay. forensic data is collected via various external devices such as usbs, external hard \ndrives, cds and dvds, and then brought into the forensic lab for investigators to undertake \nvarious operations/steps to forensically analyse evidentiary material. some of the tools that \nwe used in project ares while doing the static analysis were encase, ftk (forensics toolkit), \nhash keeper, caine, registry recon and safeback. while in live forensics the \ncompromised system remains operational, information is gathered, analyzed, and reports are \ngenerated in live digital forensics. the tools used for live digital forensic analysis can provide \nvery clear pictures of knowledge such as memory dumps, running processes, open network \nconnections, and unencrypted versions of encrypted files, while such memory contents \ncannot be acquired properly in static analysis. the tools used for live digital forensic analysis \ncan provide very clear pictures of knowledge such as memory dumps, running processes, \nopen network connections and unencrypted versions of encrypted files, while such memory \ncontents cannot be acquired properly in static analysis. some of the tools for live digital \nforensic are the sleuth kit, os forensics, evidence eliminator, net-sleuth, and wireshark. \n \n•",
            "page": null,
            "goal": "week 9 \nfor week 9, we continued doing the project areas tasks for battle room 9. we learned about \ndifferent hash functions and used md5 hash verification in the tasks to check the integrity of \na file via an alphanumeric string. also, for the tasks, we used registry explorer which \nenables us to load and browse local registry hives. it basically consists of information about \nthe windows system setup, data for a secure random number generator (rng), and the list of \ncurrently mounted systems containing a file system. we also loaded the local registry hives \nfrom a forensic image file. we used the dcode utility tool for one of the tasks to calculate \nthe date/time values from the various timestamps that may be found inside the image files. \nfrom the tasks, i learned about the unique alphanumeric identifier called the security id \n(sid). the relative id (rid) is the last part of sid which uniquely identifies a security \nprincipal relative to the local or domain security authority that issued the sid. after \nexporting the ophcrack to the desktop in week 8 we utilized the tool this week for finding out \nthe password for the systems. it uses the rainbow table to attempt to crack the attempts of \nwindows accounts when given the appropriate registry hives. i was able to learn about ntfs \nfile system which is a proprietary journaling file system developed by microsoft which is \nused for storing, organizing, and finding files on hard disk efficiently.  we were asked to go \nthrough all the domains and perform the quiz for that particular domain. i was also asked to \npresent a power-point presentation to the stakeholder wade and the team explaining static \nand live digital forensics: methods, practices, and tools. i was able to learn about static \nanalysis which is a traditional approach in which the system is analyzed forensically after \ntaking the memory dump and shutting down the system. it is focused on examining a \nduplicate copy of the disc to extract memory contents, such as deleted files, history of web \nbrowsing, file fragments, network connections, open files, user login history, and so on to \ncreate a timeline that gives a view of partial or total statistics about the activities performed \non the victim’s system before shutting it down. the whole process involves analysing the \ncode, metadata, structure, and control of digital artifacts to identify malicious or suspicious \nbehavior. in static analysis, several software and hardware technologies such as fundl and \nregcon are utilised for memory dumping and sorting of evidentiary data for analysis and \n24 \n \ndisplay. forensic data is collected via various external devices such as usbs, external hard \ndrives, cds and dvds, and then brought into the forensic lab for investigators to undertake \nvarious operations/steps to forensically analyse evidentiary material. some of the tools that \nwe used in project ares while doing the static analysis were encase, ftk (forensics toolkit), \nhash keeper, caine, registry recon and safeback. while in live forensics the \ncompromised system remains operational, information is gathered, analyzed, and reports are \ngenerated in live digital forensics. the tools used for live digital forensic analysis can provide \nvery clear pictures of knowledge such as memory dumps, running processes, open network \nconnections, and unencrypted versions of encrypted files, while such memory contents \ncannot be acquired properly in static analysis. the tools used for live digital forensic analysis \ncan provide very clear pictures of knowledge such as memory dumps, running processes, \nopen network connections and unencrypted versions of encrypted files, while such memory \ncontents cannot be acquired properly in static analysis. some of the tools for live digital \nforensic are the sleuth kit, os forensics, evidence eliminator, net-sleuth, and wireshark. \n \n•",
            "children": []
        },
        {
            "id": "1.19",
            "name": "week 10",
            "nodeType": "title",
            "text": "week 10",
            "page": null,
            "goal": "week 10",
            "children": []
        },
        {
            "id": "1.20",
            "name": "week 10 for",
            "nodeType": "paragraph",
            "text": "week 10 \nfor week 10 we started working on dual task for 2 battle rooms. this week we started task \nwith battle room 11 as well which is basically for system security analyst where we give \ntrials on reconnaissance, domain administration, policy, log collection and system analysis. \nwe were able to learn about usb historian which is a utility by 4discovery that parses the \nusb connection history of a machine based on specific registry hives. we used plugins and \ningest modules that provided us some automated processing capabilities for the some of the \ntasks. the modules can especially be useful for parsing complex file types that are not easily \nreadable. we were able to understand the magic file number which is a file true format and \ncan often be identified based on the file signature. this type of signature basically refers to \nthe first byte of hex in the file that is unique to a specific file type. we loaded and browsed \nregistry hives exported from a forensic image file. keyword list was also created and \nsearched for via ingest modules for password cracking tasks. we did trial and error process \nfor cracking the password but then we switched to keyword searching process by loading \nregistry hives to identify potential clues in the unlocated or slack space.  \nwe also talked about procedures and framework used by governments in today’s era. jeff \ngave a brief description on the actions that can be performed in case of data breach. overall \nthe main focus was to emphasize on the need for a comprehensive understanding of \ncybersecurity beyond financial aspects, the importance of education and communication with \nstakeholders, and the responsibilities of individuals at all levels within an organization. \ni was also asked to give presentation on essential eight which was developed by the \naustralian cyber security centre (acsc) and they prioritised these mitigation strategies, in \nthe form of the strategies to mitigate cyber security incidents, to help organisations protect \nthemselves against various cyber threats. the most effective of these mitigation strategies are \nthe essential eight.  \n25 \n \nthe essential eight (e8) is basically a cybersecurity framework developed by the australian \nsignals directorate (asd) to provide guidance on mitigating targeted cyber intrusions. it \nconsists of eight essential mitigation strategies that organizations can implement to enhance \ntheir cybersecurity posture. these strategies are based on analysis of real-world incidents and \nare designed to be effective against a range of adversaries. \nthe mitigation strategies that constitute the essential eight are:      \n• application control \n• patch applications \n• configure microsoft office macro settings \n• user application hardening \n• restrict administrative privileges \n• patch operating systems \n• multi-factor authentication \n• regular backups. \nimplementing essential eight can help mitigate target cyber intrusions, and helps \norganization enhance their overall security posture and reduce the risk of successful cyber- \nattacks. cybersecurity breaches can result in financial losses, reputational damage, and legal \nimplications for organizations. by adopting the essential eight framework, organizations can \nproactively address common security vulnerabilities and reduce the likelihood and impact of \ncyber incidents. it helps in mitigating the potential risks associated with cyber threats. \n \n•",
            "page": null,
            "goal": "week 10 \nfor week 10 we started working on dual task for 2 battle rooms. this week we started task \nwith battle room 11 as well which is basically for system security analyst where we give \ntrials on reconnaissance, domain administration, policy, log collection and system analysis. \nwe were able to learn about usb historian which is a utility by 4discovery that parses the \nusb connection history of a machine based on specific registry hives. we used plugins and \ningest modules that provided us some automated processing capabilities for the some of the \ntasks. the modules can especially be useful for parsing complex file types that are not easily \nreadable. we were able to understand the magic file number which is a file true format and \ncan often be identified based on the file signature. this type of signature basically refers to \nthe first byte of hex in the file that is unique to a specific file type. we loaded and browsed \nregistry hives exported from a forensic image file. keyword list was also created and \nsearched for via ingest modules for password cracking tasks. we did trial and error process \nfor cracking the password but then we switched to keyword searching process by loading \nregistry hives to identify potential clues in the unlocated or slack space.  \nwe also talked about procedures and framework used by governments in today’s era. jeff \ngave a brief description on the actions that can be performed in case of data breach. overall \nthe main focus was to emphasize on the need for a comprehensive understanding of \ncybersecurity beyond financial aspects, the importance of education and communication with \nstakeholders, and the responsibilities of individuals at all levels within an organization. \ni was also asked to give presentation on essential eight which was developed by the \naustralian cyber security centre (acsc) and they prioritised these mitigation strategies, in \nthe form of the strategies to mitigate cyber security incidents, to help organisations protect \nthemselves against various cyber threats. the most effective of these mitigation strategies are \nthe essential eight.  \n25 \n \nthe essential eight (e8) is basically a cybersecurity framework developed by the australian \nsignals directorate (asd) to provide guidance on mitigating targeted cyber intrusions. it \nconsists of eight essential mitigation strategies that organizations can implement to enhance \ntheir cybersecurity posture. these strategies are based on analysis of real-world incidents and \nare designed to be effective against a range of adversaries. \nthe mitigation strategies that constitute the essential eight are:      \n• application control \n• patch applications \n• configure microsoft office macro settings \n• user application hardening \n• restrict administrative privileges \n• patch operating systems \n• multi-factor authentication \n• regular backups. \nimplementing essential eight can help mitigate target cyber intrusions, and helps \norganization enhance their overall security posture and reduce the risk of successful cyber- \nattacks. cybersecurity breaches can result in financial losses, reputational damage, and legal \nimplications for organizations. by adopting the essential eight framework, organizations can \nproactively address common security vulnerabilities and reduce the likelihood and impact of \ncyber incidents. it helps in mitigating the potential risks associated with cyber threats. \n \n•",
            "children": []
        },
        {
            "id": "1.21",
            "name": "week 11",
            "nodeType": "title",
            "text": "week 11",
            "page": null,
            "goal": "week 11",
            "children": []
        },
        {
            "id": "1.22",
            "name": "week 11 along",
            "nodeType": "paragraph",
            "text": "week 11 \nalong with battle room 9 we started doing the task for battle room 11 which was basically \nbased on kali linux. i was able to learn all the tools required for performing the tasks in the \nkali linux. i used the apt program which is a common command line tool and package \nmanager among debian based linux system. apt is built as a high-level command line \nfunction, and apt-get or apt-cache were utilized for specialized tasks. i also got familiarized \nwith the systemctl which is a system management which is used in ubuntu based operating \nsystem to determine the status of the services which is being used. the tool can be helpful in \nmany ways like to start, stop, query, enable, disable and generally check and change a service \nstatus. i also learned the handy commands that aid in the management of systemd. for \ninstance, journalctl is a utility for manipulating data in the journal or systemd log. i also got \nequipped with windows operating tool like icacls which is a very handy tool for managing \ndirectory access control lists (dacls). it can grant and deny access, backup control lists and \nis robust enough to even manage inherited vs explicit grants and denials. another advantage \nof icacls is that the tool can be used in the scripts. it might be handy for management of a \nwindows file server. icacls can be used for new servers to distribute access to shared spaces \nbased on established policy and procedures. in this week jeff also asked me to give \npresentation on explaining what kali linux is and why organizations and engineers use it. \nkali linux is known for its extensive collection of pre-installed software and tools tailored to \nmeet the needs of penetration testers and cybersecurity professionals.  some key features and \ncapabilities of kali linux are: penetration testing tools, forensics and incident response, \n26 \n \nsecurity assessments, and customizability. performed hashing of the memory images and \nchecked if the image has been corrupted or tampered with in transit by measuring the size of \nthe file before and after the transport. were able to obtain a list of all the active network \nconnections without doing the names lookup with the help of netstat command. this \ncommand runs more quickly than the version that resolves name to the address and used in \nthe course of local troubleshooting. for example: like identifying an open port or identifying \nwhich ip and/or application is listening to a particular port. in addition to printing network \nconnections, netstat can print routing tables, interface statistics, masquerade connections and \nmulticast membership. also learned the difference between windows and linux netstat \ncommands which have very subtle differences even though the binary name is the same. was \nable to find the domain name and ip address of the windows client system with the help of \nnslookup program which is a network administration command line tool for many computers \noperating system for querying the domain name system (dns) to obtain the domain name \nor ip address mapping or for any other specific dns record. basically, the name ‘nslookup’ \nmeans ‘name server lookup’. nslookup does not use the operating system local domain name \nsystem resolver library to perform its queries, and thus may behave differently from dig. a \nforward lookup is a lookup where the name is specified for resolving the ip whereas a reverse \nlookup is where you specify the ip and it returns the name.  both entries require the \nconfiguration though. we also tested the connectivity between 2 network devices for one of \nthe tasks with the help of ping command. it works by sending a series of internet control \nmessage protocol (icmp) messages to the target host and waiting for an icmp echo message \nfrom and to the host and device and that’s how we were able to an icmp request from the \nkali client to the target machine. other than that, we used hping command for transferring \nfiles under supported protocols, handles fragmentation, as well as arbitrary packet sizes. it is \nsometimes referred to as swiss army knife for packets in which we can create custom crafted \npackets using command line switches.  \n•",
            "page": null,
            "goal": "week 11 \nalong with battle room 9 we started doing the task for battle room 11 which was basically \nbased on kali linux. i was able to learn all the tools required for performing the tasks in the \nkali linux. i used the apt program which is a common command line tool and package \nmanager among debian based linux system. apt is built as a high-level command line \nfunction, and apt-get or apt-cache were utilized for specialized tasks. i also got familiarized \nwith the systemctl which is a system management which is used in ubuntu based operating \nsystem to determine the status of the services which is being used. the tool can be helpful in \nmany ways like to start, stop, query, enable, disable and generally check and change a service \nstatus. i also learned the handy commands that aid in the management of systemd. for \ninstance, journalctl is a utility for manipulating data in the journal or systemd log. i also got \nequipped with windows operating tool like icacls which is a very handy tool for managing \ndirectory access control lists (dacls). it can grant and deny access, backup control lists and \nis robust enough to even manage inherited vs explicit grants and denials. another advantage \nof icacls is that the tool can be used in the scripts. it might be handy for management of a \nwindows file server. icacls can be used for new servers to distribute access to shared spaces \nbased on established policy and procedures. in this week jeff also asked me to give \npresentation on explaining what kali linux is and why organizations and engineers use it. \nkali linux is known for its extensive collection of pre-installed software and tools tailored to \nmeet the needs of penetration testers and cybersecurity professionals.  some key features and \ncapabilities of kali linux are: penetration testing tools, forensics and incident response, \n26 \n \nsecurity assessments, and customizability. performed hashing of the memory images and \nchecked if the image has been corrupted or tampered with in transit by measuring the size of \nthe file before and after the transport. were able to obtain a list of all the active network \nconnections without doing the names lookup with the help of netstat command. this \ncommand runs more quickly than the version that resolves name to the address and used in \nthe course of local troubleshooting. for example: like identifying an open port or identifying \nwhich ip and/or application is listening to a particular port. in addition to printing network \nconnections, netstat can print routing tables, interface statistics, masquerade connections and \nmulticast membership. also learned the difference between windows and linux netstat \ncommands which have very subtle differences even though the binary name is the same. was \nable to find the domain name and ip address of the windows client system with the help of \nnslookup program which is a network administration command line tool for many computers \noperating system for querying the domain name system (dns) to obtain the domain name \nor ip address mapping or for any other specific dns record. basically, the name ‘nslookup’ \nmeans ‘name server lookup’. nslookup does not use the operating system local domain name \nsystem resolver library to perform its queries, and thus may behave differently from dig. a \nforward lookup is a lookup where the name is specified for resolving the ip whereas a reverse \nlookup is where you specify the ip and it returns the name.  both entries require the \nconfiguration though. we also tested the connectivity between 2 network devices for one of \nthe tasks with the help of ping command. it works by sending a series of internet control \nmessage protocol (icmp) messages to the target host and waiting for an icmp echo message \nfrom and to the host and device and that’s how we were able to an icmp request from the \nkali client to the target machine. other than that, we used hping command for transferring \nfiles under supported protocols, handles fragmentation, as well as arbitrary packet sizes. it is \nsometimes referred to as swiss army knife for packets in which we can create custom crafted \npackets using command line switches.  \n•",
            "children": []
        },
        {
            "id": "1.23",
            "name": "week 12",
            "nodeType": "title",
            "text": "week 12",
            "page": null,
            "goal": "week 12",
            "children": []
        },
        {
            "id": "1.24",
            "name": "week 12 in",
            "nodeType": "paragraph",
            "text": "week 12 \nin week 12 we just focused on completing battle room 11 tasks and doing the weekly \npresentation with the stakeholder wade. we continued with the tasks and used different kali \nlinux tool. learned how to pipe the output of the system output command with the help of \nsysteminfo and findstr. tools like systeminfo are handy in themselves, but piping large \nquantities of data to findstr can help pare that data down quickly. findstr has many \nparameters that can be specified but the only one required is the string itself. also, while \nusing the command for the tasks, we realized that findstr is case sensitive while searching \nstrings in files or piped data. findstr can also be used by itself to look for data in files as well. \nalso resolved the naming issues by able to find the host name for the windows system for \nthe tasks for the battle room. learned to identify a network path with traceroute, which is a \ntroubleshooting application for measuring distance by time between routing points between \nthe client and the target. it can be used to determine where there may be network difficulties. \nthe program is typically implemented by sending udp messages with increasing time to live \nmessages with high network ports and waiting for the icmp responses indicating whether the \ntime to live has expired, which give the traceroute the ip of that hop, or whether a host or port \nis unreachable, indicating that the traceroute is complete. while the behaviour of traceroute \namong different implantation appears to be the same, there may be some differences in the \n27 \n \nusage, including the same name of the program. in this week, i gave a presentation explaining \nmicrosoft sentinel and shadow hunter and the microsoft licensing required for these tools. \ngot knowledge on these cloud native solutions that is designed to help organizations collect, \nanalyze, and respond to security threats and incidents. it combines security event monitoring \nand analytics, threat intelligence, and automated response capabilities to provide a \ncomprehensive security monitoring and management platform. it provides security \ninformation and event management (siem) and security orchestration, automation and \nresponse (soar). got knowledge on key features which includes collecting data at cloud \nscale, detecting previously undetected threats, investigating threats with artificial intelligence \nand responding to incidents rapidly with built in orchestration and automation of common \ntasks. was also able to get knowledge on shadow hunter which is a tool used for detecting \nand mitigating the risks associated with the unauthorized network access through shadow it. \nboth microsoft sentinel and shadow hunter provide organizations with the necessary tools to \nimplement and enhance their cybersecurity posture. the tools are available through microsoft \nlicensing program depending on the organization needs and deployment model. \n \n•",
            "page": null,
            "goal": "week 12 \nin week 12 we just focused on completing battle room 11 tasks and doing the weekly \npresentation with the stakeholder wade. we continued with the tasks and used different kali \nlinux tool. learned how to pipe the output of the system output command with the help of \nsysteminfo and findstr. tools like systeminfo are handy in themselves, but piping large \nquantities of data to findstr can help pare that data down quickly. findstr has many \nparameters that can be specified but the only one required is the string itself. also, while \nusing the command for the tasks, we realized that findstr is case sensitive while searching \nstrings in files or piped data. findstr can also be used by itself to look for data in files as well. \nalso resolved the naming issues by able to find the host name for the windows system for \nthe tasks for the battle room. learned to identify a network path with traceroute, which is a \ntroubleshooting application for measuring distance by time between routing points between \nthe client and the target. it can be used to determine where there may be network difficulties. \nthe program is typically implemented by sending udp messages with increasing time to live \nmessages with high network ports and waiting for the icmp responses indicating whether the \ntime to live has expired, which give the traceroute the ip of that hop, or whether a host or port \nis unreachable, indicating that the traceroute is complete. while the behaviour of traceroute \namong different implantation appears to be the same, there may be some differences in the \n27 \n \nusage, including the same name of the program. in this week, i gave a presentation explaining \nmicrosoft sentinel and shadow hunter and the microsoft licensing required for these tools. \ngot knowledge on these cloud native solutions that is designed to help organizations collect, \nanalyze, and respond to security threats and incidents. it combines security event monitoring \nand analytics, threat intelligence, and automated response capabilities to provide a \ncomprehensive security monitoring and management platform. it provides security \ninformation and event management (siem) and security orchestration, automation and \nresponse (soar). got knowledge on key features which includes collecting data at cloud \nscale, detecting previously undetected threats, investigating threats with artificial intelligence \nand responding to incidents rapidly with built in orchestration and automation of common \ntasks. was also able to get knowledge on shadow hunter which is a tool used for detecting \nand mitigating the risks associated with the unauthorized network access through shadow it. \nboth microsoft sentinel and shadow hunter provide organizations with the necessary tools to \nimplement and enhance their cybersecurity posture. the tools are available through microsoft \nlicensing program depending on the organization needs and deployment model. \n \n•",
            "children": []
        },
        {
            "id": "1.25",
            "name": "week 13",
            "nodeType": "title",
            "text": "week 13",
            "page": null,
            "goal": "week 13",
            "children": []
        },
        {
            "id": "1.26",
            "name": "week 13 week",
            "nodeType": "paragraph",
            "text": "week 13 \nweek 13 was a very relaxed week and we just completed the remaining task from battle \nroom 11. the tasks helped us to learn to create a list of scheduled tasks for the kali linux.  \nused schtasks.exe which basically enables an administrator to create, delete, query, change, \nrun and end scheduled tasks on a local or remote computer. running schtasks.exe without \narguments displays the status and next run time for each registered task. learned to use \npowershell to find out the incorrect file hash and to verify the contents of the file. got \nknowledge that some hash algorithms including md5 and sha1 are no longer considered \nsecure against attacks. basically, the goal of using a secure hash algorithm against a file is to \nmaintain the same hash value even if the file moves from location to location. a changed \nhash values represents change to the contents of the file by accident, malicious attempt or \nunauthorized usage. we used the get-filehash cmdlet tool to get the hashes of all the files in \nthe directory to compare the 2 files. during solving the task, we were able to identify unusual \noutbound network activity on the kali linux client as well as identify any processes that may \nbe potentially malicious and terminate them. we were also able to extract all the list of \nprocesses being run by the administrator by using tasklist command line tool which can be \nused to list running process locally or on networked devices. verified signature of all the \nsystem files using powershell and the get-authenticodesignature commandlet. the signature \ntools built in powershell, like get-authenticodesignature and set-authenticodesignature \nallows to get the certificate objective associated with a signed binary.  \nwas able to get knowledge on indicators of compromise (ioc) that are positively identified \nas malicious activity in the network and the system. typical iocs are virus signature and ip \naddresses, md5 hashes of malware files or urls or domain names of botnet command and \ncontrol servers. iocs are useful to a security expert and a system administrator as they may \nindicate system intrusions. for this week jeff discussed about the project methodologies and \nthe acronyms related to project management. i gained knowledge on project management, \nagile alliance, and pmbok guide. for the weekly assessment task, i gave presentation on \n28 \n \nprince2 methodology which is a project-based method for effective project management \nand provides a structured framework for managing projects. it consists of a set of principles, \nthemes, and processes that guide project management activities from initiation to closure. \nthe methodology is flexible and scalable, making it suitable for projects of various sizes and \ncomplexities.  \n \n \n10.",
            "page": null,
            "goal": "week 13 \nweek 13 was a very relaxed week and we just completed the remaining task from battle \nroom 11. the tasks helped us to learn to create a list of scheduled tasks for the kali linux.  \nused schtasks.exe which basically enables an administrator to create, delete, query, change, \nrun and end scheduled tasks on a local or remote computer. running schtasks.exe without \narguments displays the status and next run time for each registered task. learned to use \npowershell to find out the incorrect file hash and to verify the contents of the file. got \nknowledge that some hash algorithms including md5 and sha1 are no longer considered \nsecure against attacks. basically, the goal of using a secure hash algorithm against a file is to \nmaintain the same hash value even if the file moves from location to location. a changed \nhash values represents change to the contents of the file by accident, malicious attempt or \nunauthorized usage. we used the get-filehash cmdlet tool to get the hashes of all the files in \nthe directory to compare the 2 files. during solving the task, we were able to identify unusual \noutbound network activity on the kali linux client as well as identify any processes that may \nbe potentially malicious and terminate them. we were also able to extract all the list of \nprocesses being run by the administrator by using tasklist command line tool which can be \nused to list running process locally or on networked devices. verified signature of all the \nsystem files using powershell and the get-authenticodesignature commandlet. the signature \ntools built in powershell, like get-authenticodesignature and set-authenticodesignature \nallows to get the certificate objective associated with a signed binary.  \nwas able to get knowledge on indicators of compromise (ioc) that are positively identified \nas malicious activity in the network and the system. typical iocs are virus signature and ip \naddresses, md5 hashes of malware files or urls or domain names of botnet command and \ncontrol servers. iocs are useful to a security expert and a system administrator as they may \nindicate system intrusions. for this week jeff discussed about the project methodologies and \nthe acronyms related to project management. i gained knowledge on project management, \nagile alliance, and pmbok guide. for the weekly assessment task, i gave presentation on \n28 \n \nprince2 methodology which is a project-based method for effective project management \nand provides a structured framework for managing projects. it consists of a set of principles, \nthemes, and processes that guide project management activities from initiation to closure. \nthe methodology is flexible and scalable, making it suitable for projects of various sizes and \ncomplexities.  \n \n \n10.",
            "children": []
        }
    ]
}