{
    "id": "1",
    "name": "Reflective journal Entities",
    "nodeType": "section",
    "text": "week 1: data science kick off meetings \n \non the first day of my internship, i was greeted with enthusiasm and warmth by the itic team, \nwhich immediately set a positive tone for my experience. during the initial team meeting, i \nhad the opportunity to connect with key members of the data science team, such as stephen \nelbourn, the ceo/cto of iti group, and masoud safilian, a phd candidate at macquarie \nuniversity. engaging with these accomplished professionals was truly inspiring, motivating me \nto strive for excellence in the data science field throughout my internship. additionally, i had \nthe pleasure of meeting fellow interns from macquarie university. we took time to introduce \nourselves and learn more about one another, fostering strong team relationships that will \nundoubtedly contribute to our collective success in the future. \n \non the second day of the first week, we attended a meeting focused on the company's \npolicies, organisational structure, and ownership structure. this session provided valuable \ninsight into the roles and responsibilities of various team members, as well as the appropriate \ncontacts for addressing any questions or concerns that may arise during my internship. \nalthough this information is not directly related to my primary internship objectives, i found \nit beneficial to gain a comprehensive understanding of the company's operational framework. \nthis knowledge will undoubtedly enhance my overall experience and enable me to navigate \nthe organisation more effectively. \n \non the third and final day of the first week, the excitement truly began. we dedicated over \ntwo hours to discussing the project we would be working on throughout the internship. \nadditionally, i was presented with a gantt chart that outlined the project's timeline and the \nvarious tasks to be completed each week, from start to finish. our discussion involved several \nkey aspects of the project, such as the overview, the technical skills and tools required for \nsuccessful execution, and the roles and responsibilities of each team member. we also \naddressed potential challenges and promising solutions to ensure a smooth project \nimplementation. \n \n \nweek 2: preparing project management plan and research \n \nduring the first meeting of the second week, which took place online, we discussed the tasks \nto be completed throughout the week. stephen assigned four tasks during the meeting. the \nfirst task involved preparing a project management plan, while the second required \nresearching existing commercial tools and writing a report outlining their functionalities, \nfeatures, methods, and technologies.  the third task focused on proposing a new paper with \navailable code related to using the gan method for essay dataset generation. finally, the \nfourth task entailed researching and proposing six papers that utilise machine learning for \nessay scoring across three different categories: regression, neural network, and \nclassification. three of these papers should have available code.     \n \nin addition to assigning tasks to the interns, stephen also discussed the solution concept of \nthe project. this concept provided a basic understanding of the system's functionality and its \npresentation on the front-end user interface. the process involves training the system to \ncreate a contextualised rubric, followed by several stages such as feature engineering, \ninstructor feedback loops, evaluation, and ultimately displaying the results on the platform. \nstephen also covered the key solution components and the proposed framework, which \nexplained the foundation upon which the solution was developed. furthermore, he presented \na prototype of the student portal ui design, providing a visual representation of the system's \nappearance and helping us understand the concepts and processes for both the student and \nteacher portals.     \n \nstephen's plan involved allowing the interns to select their preferred tasks, conduct research \non those tasks, and then present their findings to the company during the final meeting of the \nweek. we agreed upon a schedule that includes three working days per week. on the first day, \ntasks are assigned to team members; on the second day, any questions that arise during the \nresearch process can be addressed by seeking guidance from experts, namely stephen and \nmasoud. on the third and final day of the week, i will deliver a presentation on what i have \nlearned and researched, based on the tasks assigned to me or those i chose to undertake. \n \nweek 3: diving deeper into nlp and automatic exam marking project \n \nduring the first half of the initial meeting in week 3, stephen provided more information about \nthe general tasks to be completed this week. there were seven tasks in total: \n \n1. updating the project management plan: stephen aimed to develop a comprehensive \nplan for task documentation and project execution, which includes selecting tools for \ndocument management, updating the project gantt chart throughout the process, \nanalysing risks, and managing stakeholder meetings and feedback. \n2. utilising chatgpt to generate data sources in the early stages. \n3. analysing suitable commercial tools from week 2 and designing wireframes for these \ntools. this task involves researching their functionalities, understanding the datasets \nand technologies used, and examining the methodologies and approaches applied to \nthese tools. \n \n4. investigating existing gan methods for generating essay datasets. \n5. exploring existing machine learning approaches for essay scoring. \n6. researching the bert method for essay scoring. \n7. analysing machine learning models, approaches, and data sources from kaggle. \n \nby covering these tasks, the team can further develop their understanding of various tools \nand techniques relevant to the project. \n \ntasks 4, 5, and 6 were particularly important and became my primary focus. these tasks \nsignificantly contributed to enhancing my knowledge of natural language processing (nlp) \nskills, as my background was more gravitate towards the computer vision domain within \nmachine learning. throughout the third week, i devoted most of my time to researching gan \nmethods, examining existing gan techniques, and identifying their similarities and \ndifferences. based on my understanding at that time, i determined the most suitable gan \napproach for the project, considering the desired data generation method, project approach, \nand scope. \n \na critical activity during this week's meetings was masoud's presentation on various aspects \nof the essay scoring system. he introduced us to using gan for essay dataset generation and \nmultiple machine learning methods for essay scoring systems. masoud's presentation \nsignificantly improved my understanding of the subject, as he addressed several of my \nquestions and provided insight into important aspects, such as: \n \n the datasets available for research on automated essay grading. \n the features extracted for essay assessment. \n the evaluation metrics for measuring algorithm accuracy. \n the machine learning techniques used for aes and their implementation. \n the challenges and limitations in current research. \n \ni gained valuable knowledge during this meeting, which i greatly appreciate. masoud \nexplained various vectorisation techniques in nlp, including tokenisation, vocabulary creation, \nand vector creation. he also introduced different types of vectorisations, such as bag of \nwords, tf-idf, word2vec, and glove. furthermore, masoud presented useful evaluation \nmetrics for the project that are suitable for the automatic essay scoring system model, \nincluding quadratic weighted kappa (qwk), mean absolute error (mae), and pearson \ncorrelation coefficient (pcc). i realised that these methods are crucial and may play a key role \nin my future nlp projects, as they are widely used and reliable evaluation methods for nlp. \n \nweek 4: wireframes for academic integrity ui and important paper research \n \nthere were two tasks assigned by stephen for this week for the interns, which are: \n1. build wireframes for academic integrity ui \n2. research the paper “neural network automated scoring for reading comprehension \nvia in-context bert tuning.” \n \ni was given the option to choose one of these two tasks for the week, with the expectation to \nreport back to the company over the weekend. base on the useful approaches and \nmethodologies introduced last week, it was an opportune time to dive into a specific state-of-\nthe-art paper for a deeper understanding of the latest nlp methods. the paper \"neural \nnetwork automated scoring for reading comprehension via in-context bert tuning\" proved \nto be highly informative and essential approach. it effectively explains how to implement the \nbert method for automatic essay scoring systems. the paper presents a fine-tuned bert \nmodel for scoring reading comprehension tasks, with the in-context bert tuning approach \nenhancing the model's contextual understanding. the study's promising results outperform \nother methods and contribute to the development of automated essay scoring systems. \n \nas an intern and learner, i understand that i will face numerous challenges during my \ninternship, and reading and trying to understand this paper is one of them. while studying at \nmacquarie university, i realised that reading scientific papers is difficult and time-consuming, \nas it requires revisiting prior knowledge from the past. however, this process helps me develop \nand improve my skills. to succeed, i must overcome this challenge. in my view, this task is \ncrucial and demanded the most mental and physical effort, as i spent the majority of my week \nresearching and trying to comprehend the paper's content. after thoroughly reading the \npaper, i gained confidence in explaining and sharing the knowledge i acquired. although i \ncannot claim to fully understand the entire paper, i have become a better version of myself \ncompared to last week, as i now grasp the process and implementation of a state-of-the-art \napproach. i am now prepared to apply the bert tuning process in my future natural language \nprocessing work to see if it can enhance my performance. \n \nweek 5: research recurrent neural network, lstm and transformer architecture \n \nthis week began with exciting news from stephen: the company set up accounts for the \nmacquarie university data science project team to access itic's learning management \nsystem (lms). this platform consolidates course calendars, meeting links, recordings, \nmaterials, and weekly tasks in one place, making it intuitive and user-friendly.  i can easily \naccess the \"data science project team s1 2023\" course by clicking the \"my courses\" tab in the \ntop menu bar, where i'll find course materials and meeting recordings. meeting schedules and \nlinks to join sessions are available under the \"dashboard\" tab. this feature is incredibly helpful \nbecause, previously, communication was only via email, making it challenging to stay \norganised. after expressing my concerns to stephen, i'm pleased he listened and provided the \nteam with a more efficient way to track our work. \n \nthis week, we shifted our focus to the technical aspects of the project, as the first half \nprimarily involved heavy research on various approaches and methodologies. to build a solid \nfoundation in natural language processing, we'll need to study additional concepts. masoud \nwill play a significant role in explaining these technical aspects. \n \nmasoud began by introducing recurrent neural networks (rnns) and discussing the proper \nway to perform word embeddings in a model. he also highlighted the disadvantages of rnns, \nsuch as their slow training time, susceptibility to vanishing/exploding gradient issues, and lack \nof true bidirectionality. consequently, masoud introduced a better approach: long short-term \nmemory (lstm) models, explaining why they outperform rnns. he provided an easily \nunderstandable overview of lstm's architecture. \n \nhowever, masoud also pointed out the drawbacks of lstm models, explaining that they have \nbecome outdated and that newer, more effective approaches are needed. this led to the \ndiscussion of transformer architecture, a state-of-the-art method in the nlp field. recognising \nthe importance of understanding transformers, masoud delved into the details of the model, \nits workings, and provided insightful examples of its flow. he also explained why transformers \noutperform older methods and how they overcome the disadvantages of previous models. \n \n \nweek 6: deeper into transformer, gpt and bert architecture for nlp \n \nthis week's tasks are primarily focused on gaining a deep understanding of the transformer, \ngpt, and bert architectures. masoud explained the two main components of the transformer \nmodel to the team: the encoder and the decoder. the encoder interprets and analyses the \ninput text, while the decoder produces the resulting text based on the output from the \nencoder. the self-attention mechanisms within both the encoder and decoder allow the \nmodel to concentrate on different portions of the text and comprehend the context, making \nthe transformer architecture highly effective for a variety of nlp tasks. \n \nmasoud continued to discuss the pre-training phase, which consists of two steps: pre-training \nfor language understanding and fine-tuning to customise the pre-trained model for a specific \nnlp task. he also highlighted the advantages of pre-training: the opportunity to use a vast \namount of unsupervised data for effective language understanding and the reduction of \nexecution time during fine-tuning, as the model already understands the language and only \nneeds to learn a few parameters for the specific task. \n \nnext, we delved into the bert architecture. to comprehend the difference between \nunidirectional and bidirectional models, masoud explained these concepts to us. he then \nprovided a better understanding of multi-head attention, residual connections, normalisation, \nand the feed-forward stage in the transformer workflow. \n \nin my opinion, i've realised that understanding just one concept in nlp doesn't guarantee \nsuccess in all situations using that technique. it is essential to explore and compare various \nmethods, as some may be more suitable than others in certain cases. to truly grasp how these \nmethods work and how to fine-tune their intricate details, it is important to gain a deep \nunderstanding of their architectures and the differences between them. \n \nthis week's tasks made me question whether i am truly qualified for this job and, more \ngenerally, for the it, machine learning, and data science fields. i know that there is much \nmore to learn, and many challenges lie ahead, especially since ai is likely to outperform those \nwho don't excel in this competitive environment. therefore, i dedicated a significant amount \nof time this week to reviewing my previous work and knowledge. i sought to explore and \nresearch new concepts by watching numerous youtube videos explaining different ml \ntheories. \n \nfrom what i have observed during my internship, i am not the type of person to nit-pick or \nfocus on minor issues. instead, i understand that challenges will arise in any company and \nmust be addressed by competent candidates or employees. this understanding serves as my \nprimary source of motivation to continually strive for improvement every day. \n \n \n \n \n \n \nweek 7: holiday break \n \non the 7th week, we have an early easter break due to a conflict with stephen's work and flight \nschedule, as he is currently outside of sydney. we will resume activities on the 17th of april \nwhen stephen returns. \n \nalthough there is more free time available this week, i am not going to waste this valuable \nopportunity. i know there is still much to work on regarding the internship, completing my \nmid-term report, and continuing my self-improvement by studying and researching new \nknowledge. \n \nat the beginning of the week, i took a few days to unwind and relax, as i felt i had worked \ndiligently during the first half of the project. although our tasks have primarily focused on the \nresearch phase, and we haven't done much practical coding work, i believe the work we have \naccomplished is crucial from a project management perspective and should be included in any \nproject lifecycle. i am aware that many projects in various companies fail because they don't \nexecute the initial steps properly. nonetheless, i am pleased with our progress, as i have \ngained more knowledge than i initially anticipated. \n \nthe natural language processing aspect of data science has provided me with more \nopportunities to understand myself and shift my perspective when approaching other data \nscience concepts. in the past, i mainly focused on computer vision work, but this field is \nentirely new to me. i have discovered that i genuinely enjoy nlp, and it has become an exciting \narea that continually motivates me to push my limits and explore new things to learn. \n \nweek 8: bbc dataset and learn hyper parameter \n \nin the 8th week, our focus was on examining and working with the bbc news classification \ndataset sourced from kaggle. this dataset comprises 2,225 articles from the bbc, each \nlabelled under one of five categories: business, entertainment, politics, sport, or tech. the \ndataset is divided into 1,490 records for training and 735 for testing. \n \nadditionally, masoud provided us with a comprehensive overview of the fundamental process \ninvolved in constructing a model. the following steps were covered: \n \nutilising colab gpu for training: we learned how to utilise the computational power of \ncolab's gpu for training our models, which can significantly enhance processing speed. \n \ninstalling the hugging face library: hugging face is a well-known library in the nlp field, and \nwe learned how to install it, enabling us to employ its powerful tools and models. \n \ntokenisation & input formatting: we explored the topic of tokenisation, with a specific \nemphasis on the bert tokeniser. we learned about the necessary formatting for inputs, \nincluding special tokens, sentence length, and attention mask. furthermore, we acquired \nknowledge on tokenising the dataset and dividing it into training and validation sets. \n \ntraining our classification model: to adapt the pre-trained bert model for our classification \ntask, we delved into the bertforsequenceclassification interface provided by the hugging \nface library. this interface permits the addition of a linear layer on top of bert for \nclassification purposes. by training this modified model on our dataset, our objective was to \nmake it well-suited for our specific task. \n \nperformance on test set: we discussed the significance of evaluating our model's \nperformance on a separate test set. this step aids in assessing the model's effectiveness in \naccurately classifying unseen data. \n \nfine-tuning techniques: masoud introduced us to several techniques for optimising our \nmodel's performance. these techniques encompassed early stopping, batch sise selection, \nadam optimisation, choosing the appropriate activation function, and determining the \nnumber of epochs to train the model for. \n \nthroughout the week, we made progress in comprehending these concepts and applying \nthem to the bbc news classification dataset. by implementing masoud's outlined steps, we \ngained insights into training a bert-based model for text classification and evaluated its \nperformance on the test set. \n \nweek 9: model training and evaluating and param tuning \n \n \nin the ninth week of our project, we learned about important topics that are necessary to \nmake sure our models are reliable and accurate. we talked about random number generation \nand how it affects reproducibility, dropout and early stopping to prevent overfitting, and \ndifferent types of biases that can happen during data collection, preprocessing, feature \nengineering, data selection, model training, and model evaluation. let's summarise what we \nlearned: \n \nrandom number generation: we discussed the importance of setting the random seed when \nworking with random numbers. setting the seed allows us to reproduce the same results. \nhowever, in larger projects or when other parts of the code reset the global random seed, it \ncan cause problems. in such cases, we need to use the same random numbers for specific \nparts of the code, like tests or functions. \n \ndropout: dropout is a technique used in neural networks to prevent overfitting. it randomly \nignores certain nodes or layers during training. this helps make the network less flexible and \nreduces the risk of overfitting. a good dropout ratio value is usually between 0.5 and 0.6. \n \nearly stopping: early stopping is a method used to prevent overfitting in machine learning and \ndeep learning models. we found that after a certain number of training iterations, the model \nmay start overfitting the training data. continuing training beyond this point leads to a larger \ngap between training and validation scores and worse performance. early stopping allows us \nto stop training at the right time to prevent overfitting. \n \nbiases in data collection: we explored different biases that can happen during data \ncollection. bias in data collection occurs when information is gathered incorrectly based on \nprejudiced or biased assumptions. to avoid bias, it's important to involve experts who \nunderstand the subject matter and can accurately capture key features and their \ncharacteristics. they can ensure that the data collection process includes relevant variables \nand avoids skewed representation. \n \nbias in model training and evaluation: we discussed biases that can occur during model \ntraining and evaluation. it's important to choose the right model based on the type of data, \nthe problem we're solving, and the desired outcome. evaluating the model's performance \nusing test data is recommended because evaluating it based on the training data may be \nbiased. understanding the specific performance indicators that are important for our use case, \nlike sensitivity or accuracy, is crucial for accurate model evaluation. \n \nin addition to the topics covered in week 9, we also focused on data analysis and model \nbuilding using the bbc dataset. here are the additional processes i performed during this \nweek: \n \ndata analysis: i conducted a thorough data analysis on the bbc dataset to gain insights and \nunderstand its characteristics. then i employed various data visualisation methods such as pie \ncharts and bar charts to visualise the distribution of articles across different categories like \nbusiness, entertainment, politics, sport, and tech. these visualisations helped us understand \nthe dataset's composition and identify any class imbalances or patterns. \n \ndata cleaning: to ensure the quality of our dataset, we performed data cleaning processes. \nthis involved removing any null or missing values from the dataset. null values can adversely \naffect the model's performance, so it is crucial to handle them appropriately. by eliminating \nthese instances, we ensured that the dataset was complete and ready for further analysis. \n \noutlier removal: outliers can significantly impact the performance of machine learning \nmodels. outliers can skew the data distribution and lead to biased model predictions. by \ncarefully examining the dataset and applying suitable statistical methods, i successfully \nidentified and eliminated any outliers present. \n \ntokenisation: tokenisation is a fundamental step in natural language processing tasks. in the \ncontext of the bbc dataset, i used a tokeniser to break down the textual content of the articles \ninto individual tokens or words. this process is essential for further analysis and model \ntraining, as it enables us to convert text data into numerical representations that can be \nunderstood by machine learning algorithms. \n \nmodel building: i constructed a model to classify the articles in the bbc dataset into their \nrespective categories. for this task, i utilised the bertclassifier, which leverages the power of \nthe bert-base-cased bert model. the bert model is a pre-trained language model that captures \ncontextual information from text data effectively. by fine-tuning this model using our dataset, \nwe aimed to create a classification model specifically tailored to our task. \n \noptimiser and activation function: to optimise the performance of our model, i employed \nthe adam optimiser. adam is a popular optimisation algorithm that combines the benefits of \nthe adagrad and rmsprop algorithms. it helps in efficient gradient-based optimisation. \nadditionally, i used relu (rectified linear unit) activation function as the final layer in our \nmodel. relu is a widely used activation function that introduces non-linearity to the model \nand helps in capturing complex patterns in the data. \n \nmodel evaluation: to assess the performance of our model, i employed various evaluation \nmetrics, including training loss, training accuracy, validation loss, and validation accuracy. \nthese metrics provided insights into how well the model was learning during the training \nprocess and how it generalised to unseen data. by monitoring these metrics, we could make \ninformed decisions regarding model improvements, hyperparameter tuning, and potential \noverfitting. \n \nweek 10: model building, training and evaluating on asap dataset \n \nduring week 10, i encountered some challenges while working on the bert model, as it was \na new field for me. as a result, i fell behind schedule. however, i remained determined and \ndedicated to improving my understanding. i spent the entire weekend studying, revising, and \ntraining the model. fortunately, i made significant progress at the beginning of the week by \nachieving a test accuracy of 0.996 after training the model for 5 epochs. i also experimented \nwith different numbers of epochs, such as 10 and 20. although the results were still good, \nthey were not as impressive as the 5-epoch training. furthermore, the model began to show \nsigns of overfitting around 7-8 epochs. consequently, i decided that training the model for 5 \nepochs was a suitable threshold for this bbc news dataset. i saved this successful result for \nfurther analysis. \n \nmoving on to the lessons of week 10, masoud and i determined that i had gained a solid \nfoundation and understanding of the bert model architecture. therefore, we decided to \nimplement the same model on a different dataset called the asap dataset. this dataset, \nobtained from the hewlett foundation: automated essay scoring dataset on kaggle, is used \nto develop an automated scoring algorithm for student-written essays. the dataset includes \nascii formatted text for each essay, accompanied by one or more human scores and, in some \ncases, a final resolved human score. the training data contains varying numbers of essays for \neach prompt, ranging from the lowest amount of 1,190 essays randomly selected from a total \nof 1,982. \n \nin week 10, my goal was to perform data analysis on the new asap dataset. however, i \nencountered some challenging issues. upon analysing the dataset and examining its \ndistribution, i discovered that it was highly skewed and imbalanced. despite performing some \ndata preprocessing, i was unable to resolve the imbalance issue due to its severity. \nnevertheless, with permission from steve and masoud, i was allowed to continue attempting \nto implement the current model on this imbalanced dataset to observe the resulting \noutcomes. \n \n \n \n \nweek 11: asap dataset training/evaluating and tuning \n \nduring week 11, i began the process of adapting the current model to fit the new asap dataset \nand commenced training. however, i encountered several new challenges during the training \nphase. unexpected errors surfaced, making it difficult to identify their root causes. it took me \nseveral days of intensive troubleshooting to finally pinpoint the error hidden deep within the \nlayers of the model. i discovered that this error was specific to the new dataset and its \nincompatibility with certain aspects of the model. this experience served as a valuable lesson, \nemphasising the need to tailor the approach for different datasets to ensure compatibility. \n \nto address the issue, i experimented with various debugging methods. i meticulously analysed \nthe error logs, inspected the model architecture, and reviewed the dataset structure to gain \ninsights into the source of the problem. this iterative debugging process involved making \nadjustments to the model's layers, fine-tuning parameters, and implementing dataset-specific \nmodifications. it required a combination of patience, perseverance, and problem-solving \nskills. \n \nanother significant challenge i encountered was the extensive training time required by the \nasap dataset. unlike the bbc news dataset, which trained relatively quickly, the asap dataset \ndemanded considerably more time to converge. the training process became exceptionally \ntime-consuming, especially when faced with the uncertainty of predicting the outcome and \nwaiting for the model to train and evaluate. this extended timeframe posed a challenge to my \nworkflow and necessitated effective time management strategies. \n \nfurthermore, if the initial results did not meet expectations, i had to retrain the model and \nperform hyperparameter tuning. this involved adjusting various parameters such as dropout \nvalues, learning rates, and the number of epochs. as time was limited, i made every effort to \nmaximise the remaining time i had to explore different hyperparameter configurations. this \niterative process involved training the model, evaluating the results, adjusting parameters, \nand repeating the cycle until achieving the desired performance. \n \nthroughout this process, i recognised the importance of experimentation and the iterative \nnature of model development. it was essential to carefully balance the time-consuming \ntraining process with the need for timely results. although challenging, this experience \nallowed me to deepen my understanding of model adaptation, troubleshooting, and \nhyperparameter tuning in the context of a new dataset. \n \nweek 12: corpus dataset + api deployment \n \nas we enter the final week of our internship project, i am pleased with the progress we have \nmade thus far. we have successfully accomplished 90% of the important project milestones, \ndespite encountering some challenging issues along the way. this is a natural part of the \nproject life cycle, and i am proud of our ability to overcome obstacles and achieve the \nmajority of our initial objectives. \n \nin this last week, we decided to explore training the model on another dataset called the \nfirst certificate in english (fce) corpus. this dataset, known as the cambridge learner \ncorpus first certificate in english (clc fce), comprises short texts written by learners of \nenglish as an additional language in response to exam prompts. these texts assess the \nlearners' mastery of the upper-intermediate proficiency level and have been manually error-\nannotated using a taxonomy of 77 error types. the complete fce dataset contains 323,192 \nsentences. however, we focused on a publicly released subset called fce-public, which \nconsists of 33,673 sentences divided into test and training sets of 2,720 and 30,953 \nsentences, respectively. \n \nworking with this dataset provided me with the opportunity to perform data analysis on a \nlarge folder containing various subfolders, each containing numerous xml files. this step \nwas both challenging and exciting, as i had limited experience working with xml files in the \npast. to navigate through the folder structure and extract the necessary information, i \nutilised the xml.etree.elementtree module, which offers a simple and efficient api for \nparsing and creating xml data. i developed a function to iterate through the dataset, read \nthe xml data, and parse the xml elements. while i faced some challenges due to the \nvariations in xml file structures and the need for careful extraction of relevant elements, i \nmanaged to obtain satisfactory results and retrieve most of the crucial information required \nfor the model. \n \nunfortunately, time constraints became a limiting factor as the project approached its end. \nwith limited time remaining, i was unable to dive deeper into this direction and explore \nfurther possibilities. nevertheless, i am eager to continue exploring this area in the future \nand enhance my understanding and skills for personal self-development purposes. \n \nin addition to our work, we had a delightful lunch meeting together, where we discussed \nfuture opportunities and received valuable support from steve and masoud. the kindness \nand support from every person in the company made this journey unforgettable, and i am \ntruly grateful for their assistance throughout the journey. \n \n ",
    "page": null,
    "goal": "Reflective journal Entities",
    "children": [
        {
            "id": "1.1",
            "name": "week 1",
            "nodeType": "title",
            "text": "week 1",
            "page": null,
            "goal": "week 1",
            "children": []
        },
        {
            "id": "1.2",
            "name": "week 1: data",
            "nodeType": "paragraph",
            "text": "week 1: data science kick off meetings \n \non the first day of my internship, i was greeted with enthusiasm and warmth by the itic team, \nwhich immediately set a positive tone for my experience. during the initial team meeting, i \nhad the opportunity to connect with key members of the data science team, such as stephen \nelbourn, the ceo/cto of iti group, and masoud safilian, a phd candidate at macquarie \nuniversity. engaging with these accomplished professionals was truly inspiring, motivating me \nto strive for excellence in the data science field throughout my internship. additionally, i had \nthe pleasure of meeting fellow interns from macquarie university. we took time to introduce \nourselves and learn more about one another, fostering strong team relationships that will \nundoubtedly contribute to our collective success in the future. \n \non the second day of the first week, we attended a meeting focused on the company's \npolicies, organisational structure, and ownership structure. this session provided valuable \ninsight into the roles and responsibilities of various team members, as well as the appropriate \ncontacts for addressing any questions or concerns that may arise during my internship. \nalthough this information is not directly related to my primary internship objectives, i found \nit beneficial to gain a comprehensive understanding of the company's operational framework. \nthis knowledge will undoubtedly enhance my overall experience and enable me to navigate \nthe organisation more effectively. \n \non the third and final day of the first week, the excitement truly began. we dedicated over \ntwo hours to discussing the project we would be working on throughout the internship. \nadditionally, i was presented with a gantt chart that outlined the project's timeline and the \nvarious tasks to be completed each week, from start to finish. our discussion involved several \nkey aspects of the project, such as the overview, the technical skills and tools required for \nsuccessful execution, and the roles and responsibilities of each team member. we also \naddressed potential challenges and promising solutions to ensure a smooth project \nimplementation.",
            "page": null,
            "goal": "week 1: data science kick off meetings \n \non the first day of my internship, i was greeted with enthusiasm and warmth by the itic team, \nwhich immediately set a positive tone for my experience. during the initial team meeting, i \nhad the opportunity to connect with key members of the data science team, such as stephen \nelbourn, the ceo/cto of iti group, and masoud safilian, a phd candidate at macquarie \nuniversity. engaging with these accomplished professionals was truly inspiring, motivating me \nto strive for excellence in the data science field throughout my internship. additionally, i had \nthe pleasure of meeting fellow interns from macquarie university. we took time to introduce \nourselves and learn more about one another, fostering strong team relationships that will \nundoubtedly contribute to our collective success in the future. \n \non the second day of the first week, we attended a meeting focused on the company's \npolicies, organisational structure, and ownership structure. this session provided valuable \ninsight into the roles and responsibilities of various team members, as well as the appropriate \ncontacts for addressing any questions or concerns that may arise during my internship. \nalthough this information is not directly related to my primary internship objectives, i found \nit beneficial to gain a comprehensive understanding of the company's operational framework. \nthis knowledge will undoubtedly enhance my overall experience and enable me to navigate \nthe organisation more effectively. \n \non the third and final day of the first week, the excitement truly began. we dedicated over \ntwo hours to discussing the project we would be working on throughout the internship. \nadditionally, i was presented with a gantt chart that outlined the project's timeline and the \nvarious tasks to be completed each week, from start to finish. our discussion involved several \nkey aspects of the project, such as the overview, the technical skills and tools required for \nsuccessful execution, and the roles and responsibilities of each team member. we also \naddressed potential challenges and promising solutions to ensure a smooth project \nimplementation.",
            "children": []
        },
        {
            "id": "1.3",
            "name": "week 2",
            "nodeType": "title",
            "text": "week 2",
            "page": null,
            "goal": "week 2",
            "children": []
        },
        {
            "id": "1.4",
            "name": "week 2: preparing",
            "nodeType": "paragraph",
            "text": "week 2: preparing project management plan and research \n \nduring the first meeting of the second week, which took place online, we discussed the tasks \nto be completed throughout the week. stephen assigned four tasks during the meeting. the \nfirst task involved preparing a project management plan, while the second required \nresearching existing commercial tools and writing a report outlining their functionalities, \nfeatures, methods, and technologies.  the third task focused on proposing a new paper with \navailable code related to using the gan method for essay dataset generation. finally, the \nfourth task entailed researching and proposing six papers that utilise machine learning for \nessay scoring across three different categories: regression, neural network, and \nclassification. three of these papers should have available code.     \n \nin addition to assigning tasks to the interns, stephen also discussed the solution concept of \nthe project. this concept provided a basic understanding of the system's functionality and its \npresentation on the front-end user interface. the process involves training the system to \ncreate a contextualised rubric, followed by several stages such as feature engineering, \ninstructor feedback loops, evaluation, and ultimately displaying the results on the platform. \nstephen also covered the key solution components and the proposed framework, which \nexplained the foundation upon which the solution was developed. furthermore, he presented \na prototype of the student portal ui design, providing a visual representation of the system's \nappearance and helping us understand the concepts and processes for both the student and \nteacher portals.     \n \nstephen's plan involved allowing the interns to select their preferred tasks, conduct research \non those tasks, and then present their findings to the company during the final meeting of the \nweek. we agreed upon a schedule that includes three working days per week. on the first day, \ntasks are assigned to team members; on the second day, any questions that arise during the \nresearch process can be addressed by seeking guidance from experts, namely stephen and \nmasoud. on the third and final day of the week, i will deliver a presentation on what i have \nlearned and researched, based on the tasks assigned to me or those i chose to undertake.",
            "page": null,
            "goal": "week 2: preparing project management plan and research \n \nduring the first meeting of the second week, which took place online, we discussed the tasks \nto be completed throughout the week. stephen assigned four tasks during the meeting. the \nfirst task involved preparing a project management plan, while the second required \nresearching existing commercial tools and writing a report outlining their functionalities, \nfeatures, methods, and technologies.  the third task focused on proposing a new paper with \navailable code related to using the gan method for essay dataset generation. finally, the \nfourth task entailed researching and proposing six papers that utilise machine learning for \nessay scoring across three different categories: regression, neural network, and \nclassification. three of these papers should have available code.     \n \nin addition to assigning tasks to the interns, stephen also discussed the solution concept of \nthe project. this concept provided a basic understanding of the system's functionality and its \npresentation on the front-end user interface. the process involves training the system to \ncreate a contextualised rubric, followed by several stages such as feature engineering, \ninstructor feedback loops, evaluation, and ultimately displaying the results on the platform. \nstephen also covered the key solution components and the proposed framework, which \nexplained the foundation upon which the solution was developed. furthermore, he presented \na prototype of the student portal ui design, providing a visual representation of the system's \nappearance and helping us understand the concepts and processes for both the student and \nteacher portals.     \n \nstephen's plan involved allowing the interns to select their preferred tasks, conduct research \non those tasks, and then present their findings to the company during the final meeting of the \nweek. we agreed upon a schedule that includes three working days per week. on the first day, \ntasks are assigned to team members; on the second day, any questions that arise during the \nresearch process can be addressed by seeking guidance from experts, namely stephen and \nmasoud. on the third and final day of the week, i will deliver a presentation on what i have \nlearned and researched, based on the tasks assigned to me or those i chose to undertake.",
            "children": []
        },
        {
            "id": "1.5",
            "name": "week 3",
            "nodeType": "title",
            "text": "week 3",
            "page": null,
            "goal": "week 3",
            "children": []
        },
        {
            "id": "1.6",
            "name": "week 3: diving",
            "nodeType": "paragraph",
            "text": "week 3: diving deeper into nlp and automatic exam marking project \n \nduring the first half of the initial meeting in week 3, stephen provided more information about \nthe general tasks to be completed this week. there were seven tasks in total: \n \n1. updating the project management plan: stephen aimed to develop a comprehensive \nplan for task documentation and project execution, which includes selecting tools for \ndocument management, updating the project gantt chart throughout the process, \nanalysing risks, and managing stakeholder meetings and feedback. \n2. utilising chatgpt to generate data sources in the early stages. \n3. analysing suitable commercial tools from week 2 and designing wireframes for these \ntools. this task involves researching their functionalities, understanding the datasets \nand technologies used, and examining the methodologies and approaches applied to \nthese tools. \n \n4. investigating existing gan methods for generating essay datasets. \n5. exploring existing machine learning approaches for essay scoring. \n6. researching the bert method for essay scoring. \n7. analysing machine learning models, approaches, and data sources from kaggle. \n \nby covering these tasks, the team can further develop their understanding of various tools \nand techniques relevant to the project. \n \ntasks 4, 5, and 6 were particularly important and became my primary focus. these tasks \nsignificantly contributed to enhancing my knowledge of natural language processing (nlp) \nskills, as my background was more gravitate towards the computer vision domain within \nmachine learning. throughout the third week, i devoted most of my time to researching gan \nmethods, examining existing gan techniques, and identifying their similarities and \ndifferences. based on my understanding at that time, i determined the most suitable gan \napproach for the project, considering the desired data generation method, project approach, \nand scope. \n \na critical activity during this week's meetings was masoud's presentation on various aspects \nof the essay scoring system. he introduced us to using gan for essay dataset generation and \nmultiple machine learning methods for essay scoring systems. masoud's presentation \nsignificantly improved my understanding of the subject, as he addressed several of my \nquestions and provided insight into important aspects, such as: \n \n the datasets available for research on automated essay grading. \n the features extracted for essay assessment. \n the evaluation metrics for measuring algorithm accuracy. \n the machine learning techniques used for aes and their implementation. \n the challenges and limitations in current research. \n \ni gained valuable knowledge during this meeting, which i greatly appreciate. masoud \nexplained various vectorisation techniques in nlp, including tokenisation, vocabulary creation, \nand vector creation. he also introduced different types of vectorisations, such as bag of \nwords, tf-idf, word2vec, and glove. furthermore, masoud presented useful evaluation \nmetrics for the project that are suitable for the automatic essay scoring system model, \nincluding quadratic weighted kappa (qwk), mean absolute error (mae), and pearson \ncorrelation coefficient (pcc). i realised that these methods are crucial and may play a key role \nin my future nlp projects, as they are widely used and reliable evaluation methods for nlp.",
            "page": null,
            "goal": "week 3: diving deeper into nlp and automatic exam marking project \n \nduring the first half of the initial meeting in week 3, stephen provided more information about \nthe general tasks to be completed this week. there were seven tasks in total: \n \n1. updating the project management plan: stephen aimed to develop a comprehensive \nplan for task documentation and project execution, which includes selecting tools for \ndocument management, updating the project gantt chart throughout the process, \nanalysing risks, and managing stakeholder meetings and feedback. \n2. utilising chatgpt to generate data sources in the early stages. \n3. analysing suitable commercial tools from week 2 and designing wireframes for these \ntools. this task involves researching their functionalities, understanding the datasets \nand technologies used, and examining the methodologies and approaches applied to \nthese tools. \n \n4. investigating existing gan methods for generating essay datasets. \n5. exploring existing machine learning approaches for essay scoring. \n6. researching the bert method for essay scoring. \n7. analysing machine learning models, approaches, and data sources from kaggle. \n \nby covering these tasks, the team can further develop their understanding of various tools \nand techniques relevant to the project. \n \ntasks 4, 5, and 6 were particularly important and became my primary focus. these tasks \nsignificantly contributed to enhancing my knowledge of natural language processing (nlp) \nskills, as my background was more gravitate towards the computer vision domain within \nmachine learning. throughout the third week, i devoted most of my time to researching gan \nmethods, examining existing gan techniques, and identifying their similarities and \ndifferences. based on my understanding at that time, i determined the most suitable gan \napproach for the project, considering the desired data generation method, project approach, \nand scope. \n \na critical activity during this week's meetings was masoud's presentation on various aspects \nof the essay scoring system. he introduced us to using gan for essay dataset generation and \nmultiple machine learning methods for essay scoring systems. masoud's presentation \nsignificantly improved my understanding of the subject, as he addressed several of my \nquestions and provided insight into important aspects, such as: \n \n the datasets available for research on automated essay grading. \n the features extracted for essay assessment. \n the evaluation metrics for measuring algorithm accuracy. \n the machine learning techniques used for aes and their implementation. \n the challenges and limitations in current research. \n \ni gained valuable knowledge during this meeting, which i greatly appreciate. masoud \nexplained various vectorisation techniques in nlp, including tokenisation, vocabulary creation, \nand vector creation. he also introduced different types of vectorisations, such as bag of \nwords, tf-idf, word2vec, and glove. furthermore, masoud presented useful evaluation \nmetrics for the project that are suitable for the automatic essay scoring system model, \nincluding quadratic weighted kappa (qwk), mean absolute error (mae), and pearson \ncorrelation coefficient (pcc). i realised that these methods are crucial and may play a key role \nin my future nlp projects, as they are widely used and reliable evaluation methods for nlp.",
            "children": []
        },
        {
            "id": "1.7",
            "name": "week 4",
            "nodeType": "title",
            "text": "week 4",
            "page": null,
            "goal": "week 4",
            "children": []
        },
        {
            "id": "1.8",
            "name": "week 4: wireframes",
            "nodeType": "paragraph",
            "text": "week 4: wireframes for academic integrity ui and important paper research \n \nthere were two tasks assigned by stephen for this week for the interns, which are: \n1. build wireframes for academic integrity ui \n2. research the paper “neural network automated scoring for reading comprehension \nvia in-context bert tuning.” \n \ni was given the option to choose one of these two tasks for the week, with the expectation to \nreport back to the company over the weekend. base on the useful approaches and \nmethodologies introduced last week, it was an opportune time to dive into a specific state-of-\nthe-art paper for a deeper understanding of the latest nlp methods. the paper \"neural \nnetwork automated scoring for reading comprehension via in-context bert tuning\" proved \nto be highly informative and essential approach. it effectively explains how to implement the \nbert method for automatic essay scoring systems. the paper presents a fine-tuned bert \nmodel for scoring reading comprehension tasks, with the in-context bert tuning approach \nenhancing the model's contextual understanding. the study's promising results outperform \nother methods and contribute to the development of automated essay scoring systems. \n \nas an intern and learner, i understand that i will face numerous challenges during my \ninternship, and reading and trying to understand this paper is one of them. while studying at \nmacquarie university, i realised that reading scientific papers is difficult and time-consuming, \nas it requires revisiting prior knowledge from the past. however, this process helps me develop \nand improve my skills. to succeed, i must overcome this challenge. in my view, this task is \ncrucial and demanded the most mental and physical effort, as i spent the majority of my week \nresearching and trying to comprehend the paper's content. after thoroughly reading the \npaper, i gained confidence in explaining and sharing the knowledge i acquired. although i \ncannot claim to fully understand the entire paper, i have become a better version of myself \ncompared to last week, as i now grasp the process and implementation of a state-of-the-art \napproach. i am now prepared to apply the bert tuning process in my future natural language \nprocessing work to see if it can enhance my performance.",
            "page": null,
            "goal": "week 4: wireframes for academic integrity ui and important paper research \n \nthere were two tasks assigned by stephen for this week for the interns, which are: \n1. build wireframes for academic integrity ui \n2. research the paper “neural network automated scoring for reading comprehension \nvia in-context bert tuning.” \n \ni was given the option to choose one of these two tasks for the week, with the expectation to \nreport back to the company over the weekend. base on the useful approaches and \nmethodologies introduced last week, it was an opportune time to dive into a specific state-of-\nthe-art paper for a deeper understanding of the latest nlp methods. the paper \"neural \nnetwork automated scoring for reading comprehension via in-context bert tuning\" proved \nto be highly informative and essential approach. it effectively explains how to implement the \nbert method for automatic essay scoring systems. the paper presents a fine-tuned bert \nmodel for scoring reading comprehension tasks, with the in-context bert tuning approach \nenhancing the model's contextual understanding. the study's promising results outperform \nother methods and contribute to the development of automated essay scoring systems. \n \nas an intern and learner, i understand that i will face numerous challenges during my \ninternship, and reading and trying to understand this paper is one of them. while studying at \nmacquarie university, i realised that reading scientific papers is difficult and time-consuming, \nas it requires revisiting prior knowledge from the past. however, this process helps me develop \nand improve my skills. to succeed, i must overcome this challenge. in my view, this task is \ncrucial and demanded the most mental and physical effort, as i spent the majority of my week \nresearching and trying to comprehend the paper's content. after thoroughly reading the \npaper, i gained confidence in explaining and sharing the knowledge i acquired. although i \ncannot claim to fully understand the entire paper, i have become a better version of myself \ncompared to last week, as i now grasp the process and implementation of a state-of-the-art \napproach. i am now prepared to apply the bert tuning process in my future natural language \nprocessing work to see if it can enhance my performance.",
            "children": []
        },
        {
            "id": "1.9",
            "name": "week 5",
            "nodeType": "title",
            "text": "week 5",
            "page": null,
            "goal": "week 5",
            "children": []
        },
        {
            "id": "1.10",
            "name": "week 5: research",
            "nodeType": "paragraph",
            "text": "week 5: research recurrent neural network, lstm and transformer architecture \n \nthis week began with exciting news from stephen: the company set up accounts for the \nmacquarie university data science project team to access itic's learning management \nsystem (lms). this platform consolidates course calendars, meeting links, recordings, \nmaterials, and weekly tasks in one place, making it intuitive and user-friendly.  i can easily \naccess the \"data science project team s1 2023\" course by clicking the \"my courses\" tab in the \ntop menu bar, where i'll find course materials and meeting recordings. meeting schedules and \nlinks to join sessions are available under the \"dashboard\" tab. this feature is incredibly helpful \nbecause, previously, communication was only via email, making it challenging to stay \norganised. after expressing my concerns to stephen, i'm pleased he listened and provided the \nteam with a more efficient way to track our work. \n \nthis week, we shifted our focus to the technical aspects of the project, as the first half \nprimarily involved heavy research on various approaches and methodologies. to build a solid \nfoundation in natural language processing, we'll need to study additional concepts. masoud \nwill play a significant role in explaining these technical aspects. \n \nmasoud began by introducing recurrent neural networks (rnns) and discussing the proper \nway to perform word embeddings in a model. he also highlighted the disadvantages of rnns, \nsuch as their slow training time, susceptibility to vanishing/exploding gradient issues, and lack \nof true bidirectionality. consequently, masoud introduced a better approach: long short-term \nmemory (lstm) models, explaining why they outperform rnns. he provided an easily \nunderstandable overview of lstm's architecture. \n \nhowever, masoud also pointed out the drawbacks of lstm models, explaining that they have \nbecome outdated and that newer, more effective approaches are needed. this led to the \ndiscussion of transformer architecture, a state-of-the-art method in the nlp field. recognising \nthe importance of understanding transformers, masoud delved into the details of the model, \nits workings, and provided insightful examples of its flow. he also explained why transformers \noutperform older methods and how they overcome the disadvantages of previous models.",
            "page": null,
            "goal": "week 5: research recurrent neural network, lstm and transformer architecture \n \nthis week began with exciting news from stephen: the company set up accounts for the \nmacquarie university data science project team to access itic's learning management \nsystem (lms). this platform consolidates course calendars, meeting links, recordings, \nmaterials, and weekly tasks in one place, making it intuitive and user-friendly.  i can easily \naccess the \"data science project team s1 2023\" course by clicking the \"my courses\" tab in the \ntop menu bar, where i'll find course materials and meeting recordings. meeting schedules and \nlinks to join sessions are available under the \"dashboard\" tab. this feature is incredibly helpful \nbecause, previously, communication was only via email, making it challenging to stay \norganised. after expressing my concerns to stephen, i'm pleased he listened and provided the \nteam with a more efficient way to track our work. \n \nthis week, we shifted our focus to the technical aspects of the project, as the first half \nprimarily involved heavy research on various approaches and methodologies. to build a solid \nfoundation in natural language processing, we'll need to study additional concepts. masoud \nwill play a significant role in explaining these technical aspects. \n \nmasoud began by introducing recurrent neural networks (rnns) and discussing the proper \nway to perform word embeddings in a model. he also highlighted the disadvantages of rnns, \nsuch as their slow training time, susceptibility to vanishing/exploding gradient issues, and lack \nof true bidirectionality. consequently, masoud introduced a better approach: long short-term \nmemory (lstm) models, explaining why they outperform rnns. he provided an easily \nunderstandable overview of lstm's architecture. \n \nhowever, masoud also pointed out the drawbacks of lstm models, explaining that they have \nbecome outdated and that newer, more effective approaches are needed. this led to the \ndiscussion of transformer architecture, a state-of-the-art method in the nlp field. recognising \nthe importance of understanding transformers, masoud delved into the details of the model, \nits workings, and provided insightful examples of its flow. he also explained why transformers \noutperform older methods and how they overcome the disadvantages of previous models.",
            "children": []
        },
        {
            "id": "1.11",
            "name": "week 6",
            "nodeType": "title",
            "text": "week 6",
            "page": null,
            "goal": "week 6",
            "children": []
        },
        {
            "id": "1.12",
            "name": "week 6: deeper",
            "nodeType": "paragraph",
            "text": "week 6: deeper into transformer, gpt and bert architecture for nlp \n \nthis week's tasks are primarily focused on gaining a deep understanding of the transformer, \ngpt, and bert architectures. masoud explained the two main components of the transformer \nmodel to the team: the encoder and the decoder. the encoder interprets and analyses the \ninput text, while the decoder produces the resulting text based on the output from the \nencoder. the self-attention mechanisms within both the encoder and decoder allow the \nmodel to concentrate on different portions of the text and comprehend the context, making \nthe transformer architecture highly effective for a variety of nlp tasks. \n \nmasoud continued to discuss the pre-training phase, which consists of two steps: pre-training \nfor language understanding and fine-tuning to customise the pre-trained model for a specific \nnlp task. he also highlighted the advantages of pre-training: the opportunity to use a vast \namount of unsupervised data for effective language understanding and the reduction of \nexecution time during fine-tuning, as the model already understands the language and only \nneeds to learn a few parameters for the specific task. \n \nnext, we delved into the bert architecture. to comprehend the difference between \nunidirectional and bidirectional models, masoud explained these concepts to us. he then \nprovided a better understanding of multi-head attention, residual connections, normalisation, \nand the feed-forward stage in the transformer workflow. \n \nin my opinion, i've realised that understanding just one concept in nlp doesn't guarantee \nsuccess in all situations using that technique. it is essential to explore and compare various \nmethods, as some may be more suitable than others in certain cases. to truly grasp how these \nmethods work and how to fine-tune their intricate details, it is important to gain a deep \nunderstanding of their architectures and the differences between them. \n \nthis week's tasks made me question whether i am truly qualified for this job and, more \ngenerally, for the it, machine learning, and data science fields. i know that there is much \nmore to learn, and many challenges lie ahead, especially since ai is likely to outperform those \nwho don't excel in this competitive environment. therefore, i dedicated a significant amount \nof time this week to reviewing my previous work and knowledge. i sought to explore and \nresearch new concepts by watching numerous youtube videos explaining different ml \ntheories. \n \nfrom what i have observed during my internship, i am not the type of person to nit-pick or \nfocus on minor issues. instead, i understand that challenges will arise in any company and \nmust be addressed by competent candidates or employees. this understanding serves as my \nprimary source of motivation to continually strive for improvement every day.",
            "page": null,
            "goal": "week 6: deeper into transformer, gpt and bert architecture for nlp \n \nthis week's tasks are primarily focused on gaining a deep understanding of the transformer, \ngpt, and bert architectures. masoud explained the two main components of the transformer \nmodel to the team: the encoder and the decoder. the encoder interprets and analyses the \ninput text, while the decoder produces the resulting text based on the output from the \nencoder. the self-attention mechanisms within both the encoder and decoder allow the \nmodel to concentrate on different portions of the text and comprehend the context, making \nthe transformer architecture highly effective for a variety of nlp tasks. \n \nmasoud continued to discuss the pre-training phase, which consists of two steps: pre-training \nfor language understanding and fine-tuning to customise the pre-trained model for a specific \nnlp task. he also highlighted the advantages of pre-training: the opportunity to use a vast \namount of unsupervised data for effective language understanding and the reduction of \nexecution time during fine-tuning, as the model already understands the language and only \nneeds to learn a few parameters for the specific task. \n \nnext, we delved into the bert architecture. to comprehend the difference between \nunidirectional and bidirectional models, masoud explained these concepts to us. he then \nprovided a better understanding of multi-head attention, residual connections, normalisation, \nand the feed-forward stage in the transformer workflow. \n \nin my opinion, i've realised that understanding just one concept in nlp doesn't guarantee \nsuccess in all situations using that technique. it is essential to explore and compare various \nmethods, as some may be more suitable than others in certain cases. to truly grasp how these \nmethods work and how to fine-tune their intricate details, it is important to gain a deep \nunderstanding of their architectures and the differences between them. \n \nthis week's tasks made me question whether i am truly qualified for this job and, more \ngenerally, for the it, machine learning, and data science fields. i know that there is much \nmore to learn, and many challenges lie ahead, especially since ai is likely to outperform those \nwho don't excel in this competitive environment. therefore, i dedicated a significant amount \nof time this week to reviewing my previous work and knowledge. i sought to explore and \nresearch new concepts by watching numerous youtube videos explaining different ml \ntheories. \n \nfrom what i have observed during my internship, i am not the type of person to nit-pick or \nfocus on minor issues. instead, i understand that challenges will arise in any company and \nmust be addressed by competent candidates or employees. this understanding serves as my \nprimary source of motivation to continually strive for improvement every day.",
            "children": []
        },
        {
            "id": "1.13",
            "name": "week 7",
            "nodeType": "title",
            "text": "week 7",
            "page": null,
            "goal": "week 7",
            "children": []
        },
        {
            "id": "1.14",
            "name": "week 7: holiday",
            "nodeType": "paragraph",
            "text": "week 7: holiday break \n \non the 7th week, we have an early easter break due to a conflict with stephen's work and flight \nschedule, as he is currently outside of sydney. we will resume activities on the 17th of april \nwhen stephen returns. \n \nalthough there is more free time available this week, i am not going to waste this valuable \nopportunity. i know there is still much to work on regarding the internship, completing my \nmid-term report, and continuing my self-improvement by studying and researching new \nknowledge. \n \nat the beginning of the week, i took a few days to unwind and relax, as i felt i had worked \ndiligently during the first half of the project. although our tasks have primarily focused on the \nresearch phase, and we haven't done much practical coding work, i believe the work we have \naccomplished is crucial from a project management perspective and should be included in any \nproject lifecycle. i am aware that many projects in various companies fail because they don't \nexecute the initial steps properly. nonetheless, i am pleased with our progress, as i have \ngained more knowledge than i initially anticipated. \n \nthe natural language processing aspect of data science has provided me with more \nopportunities to understand myself and shift my perspective when approaching other data \nscience concepts. in the past, i mainly focused on computer vision work, but this field is \nentirely new to me. i have discovered that i genuinely enjoy nlp, and it has become an exciting \narea that continually motivates me to push my limits and explore new things to learn.",
            "page": null,
            "goal": "week 7: holiday break \n \non the 7th week, we have an early easter break due to a conflict with stephen's work and flight \nschedule, as he is currently outside of sydney. we will resume activities on the 17th of april \nwhen stephen returns. \n \nalthough there is more free time available this week, i am not going to waste this valuable \nopportunity. i know there is still much to work on regarding the internship, completing my \nmid-term report, and continuing my self-improvement by studying and researching new \nknowledge. \n \nat the beginning of the week, i took a few days to unwind and relax, as i felt i had worked \ndiligently during the first half of the project. although our tasks have primarily focused on the \nresearch phase, and we haven't done much practical coding work, i believe the work we have \naccomplished is crucial from a project management perspective and should be included in any \nproject lifecycle. i am aware that many projects in various companies fail because they don't \nexecute the initial steps properly. nonetheless, i am pleased with our progress, as i have \ngained more knowledge than i initially anticipated. \n \nthe natural language processing aspect of data science has provided me with more \nopportunities to understand myself and shift my perspective when approaching other data \nscience concepts. in the past, i mainly focused on computer vision work, but this field is \nentirely new to me. i have discovered that i genuinely enjoy nlp, and it has become an exciting \narea that continually motivates me to push my limits and explore new things to learn.",
            "children": []
        },
        {
            "id": "1.15",
            "name": "week 8",
            "nodeType": "title",
            "text": "week 8",
            "page": null,
            "goal": "week 8",
            "children": []
        },
        {
            "id": "1.16",
            "name": "week 8: bbc",
            "nodeType": "paragraph",
            "text": "week 8: bbc dataset and learn hyper parameter \n \nin the 8th week, our focus was on examining and working with the bbc news classification \ndataset sourced from kaggle. this dataset comprises 2,225 articles from the bbc, each \nlabelled under one of five categories: business, entertainment, politics, sport, or tech. the \ndataset is divided into 1,490 records for training and 735 for testing. \n \nadditionally, masoud provided us with a comprehensive overview of the fundamental process \ninvolved in constructing a model. the following steps were covered: \n \nutilising colab gpu for training: we learned how to utilise the computational power of \ncolab's gpu for training our models, which can significantly enhance processing speed. \n \ninstalling the hugging face library: hugging face is a well-known library in the nlp field, and \nwe learned how to install it, enabling us to employ its powerful tools and models. \n \ntokenisation & input formatting: we explored the topic of tokenisation, with a specific \nemphasis on the bert tokeniser. we learned about the necessary formatting for inputs, \nincluding special tokens, sentence length, and attention mask. furthermore, we acquired \nknowledge on tokenising the dataset and dividing it into training and validation sets. \n \ntraining our classification model: to adapt the pre-trained bert model for our classification \ntask, we delved into the bertforsequenceclassification interface provided by the hugging \nface library. this interface permits the addition of a linear layer on top of bert for \nclassification purposes. by training this modified model on our dataset, our objective was to \nmake it well-suited for our specific task. \n \nperformance on test set: we discussed the significance of evaluating our model's \nperformance on a separate test set. this step aids in assessing the model's effectiveness in \naccurately classifying unseen data. \n \nfine-tuning techniques: masoud introduced us to several techniques for optimising our \nmodel's performance. these techniques encompassed early stopping, batch sise selection, \nadam optimisation, choosing the appropriate activation function, and determining the \nnumber of epochs to train the model for. \n \nthroughout the week, we made progress in comprehending these concepts and applying \nthem to the bbc news classification dataset. by implementing masoud's outlined steps, we \ngained insights into training a bert-based model for text classification and evaluated its \nperformance on the test set.",
            "page": null,
            "goal": "week 8: bbc dataset and learn hyper parameter \n \nin the 8th week, our focus was on examining and working with the bbc news classification \ndataset sourced from kaggle. this dataset comprises 2,225 articles from the bbc, each \nlabelled under one of five categories: business, entertainment, politics, sport, or tech. the \ndataset is divided into 1,490 records for training and 735 for testing. \n \nadditionally, masoud provided us with a comprehensive overview of the fundamental process \ninvolved in constructing a model. the following steps were covered: \n \nutilising colab gpu for training: we learned how to utilise the computational power of \ncolab's gpu for training our models, which can significantly enhance processing speed. \n \ninstalling the hugging face library: hugging face is a well-known library in the nlp field, and \nwe learned how to install it, enabling us to employ its powerful tools and models. \n \ntokenisation & input formatting: we explored the topic of tokenisation, with a specific \nemphasis on the bert tokeniser. we learned about the necessary formatting for inputs, \nincluding special tokens, sentence length, and attention mask. furthermore, we acquired \nknowledge on tokenising the dataset and dividing it into training and validation sets. \n \ntraining our classification model: to adapt the pre-trained bert model for our classification \ntask, we delved into the bertforsequenceclassification interface provided by the hugging \nface library. this interface permits the addition of a linear layer on top of bert for \nclassification purposes. by training this modified model on our dataset, our objective was to \nmake it well-suited for our specific task. \n \nperformance on test set: we discussed the significance of evaluating our model's \nperformance on a separate test set. this step aids in assessing the model's effectiveness in \naccurately classifying unseen data. \n \nfine-tuning techniques: masoud introduced us to several techniques for optimising our \nmodel's performance. these techniques encompassed early stopping, batch sise selection, \nadam optimisation, choosing the appropriate activation function, and determining the \nnumber of epochs to train the model for. \n \nthroughout the week, we made progress in comprehending these concepts and applying \nthem to the bbc news classification dataset. by implementing masoud's outlined steps, we \ngained insights into training a bert-based model for text classification and evaluated its \nperformance on the test set.",
            "children": []
        },
        {
            "id": "1.17",
            "name": "week 9",
            "nodeType": "title",
            "text": "week 9",
            "page": null,
            "goal": "week 9",
            "children": []
        },
        {
            "id": "1.18",
            "name": "week 9: model",
            "nodeType": "paragraph",
            "text": "week 9: model training and evaluating and param tuning \n \n \nin the ninth week of our project, we learned about important topics that are necessary to \nmake sure our models are reliable and accurate. we talked about random number generation \nand how it affects reproducibility, dropout and early stopping to prevent overfitting, and \ndifferent types of biases that can happen during data collection, preprocessing, feature \nengineering, data selection, model training, and model evaluation. let's summarise what we \nlearned: \n \nrandom number generation: we discussed the importance of setting the random seed when \nworking with random numbers. setting the seed allows us to reproduce the same results. \nhowever, in larger projects or when other parts of the code reset the global random seed, it \ncan cause problems. in such cases, we need to use the same random numbers for specific \nparts of the code, like tests or functions. \n \ndropout: dropout is a technique used in neural networks to prevent overfitting. it randomly \nignores certain nodes or layers during training. this helps make the network less flexible and \nreduces the risk of overfitting. a good dropout ratio value is usually between 0.5 and 0.6. \n \nearly stopping: early stopping is a method used to prevent overfitting in machine learning and \ndeep learning models. we found that after a certain number of training iterations, the model \nmay start overfitting the training data. continuing training beyond this point leads to a larger \ngap between training and validation scores and worse performance. early stopping allows us \nto stop training at the right time to prevent overfitting. \n \nbiases in data collection: we explored different biases that can happen during data \ncollection. bias in data collection occurs when information is gathered incorrectly based on \nprejudiced or biased assumptions. to avoid bias, it's important to involve experts who \nunderstand the subject matter and can accurately capture key features and their \ncharacteristics. they can ensure that the data collection process includes relevant variables \nand avoids skewed representation. \n \nbias in model training and evaluation: we discussed biases that can occur during model \ntraining and evaluation. it's important to choose the right model based on the type of data, \nthe problem we're solving, and the desired outcome. evaluating the model's performance \nusing test data is recommended because evaluating it based on the training data may be \nbiased. understanding the specific performance indicators that are important for our use case, \nlike sensitivity or accuracy, is crucial for accurate model evaluation. \n \nin addition to the topics covered in week 9, we also focused on data analysis and model \nbuilding using the bbc dataset. here are the additional processes i performed during this \nweek: \n \ndata analysis: i conducted a thorough data analysis on the bbc dataset to gain insights and \nunderstand its characteristics. then i employed various data visualisation methods such as pie \ncharts and bar charts to visualise the distribution of articles across different categories like \nbusiness, entertainment, politics, sport, and tech. these visualisations helped us understand \nthe dataset's composition and identify any class imbalances or patterns. \n \ndata cleaning: to ensure the quality of our dataset, we performed data cleaning processes. \nthis involved removing any null or missing values from the dataset. null values can adversely \naffect the model's performance, so it is crucial to handle them appropriately. by eliminating \nthese instances, we ensured that the dataset was complete and ready for further analysis. \n \noutlier removal: outliers can significantly impact the performance of machine learning \nmodels. outliers can skew the data distribution and lead to biased model predictions. by \ncarefully examining the dataset and applying suitable statistical methods, i successfully \nidentified and eliminated any outliers present. \n \ntokenisation: tokenisation is a fundamental step in natural language processing tasks. in the \ncontext of the bbc dataset, i used a tokeniser to break down the textual content of the articles \ninto individual tokens or words. this process is essential for further analysis and model \ntraining, as it enables us to convert text data into numerical representations that can be \nunderstood by machine learning algorithms. \n \nmodel building: i constructed a model to classify the articles in the bbc dataset into their \nrespective categories. for this task, i utilised the bertclassifier, which leverages the power of \nthe bert-base-cased bert model. the bert model is a pre-trained language model that captures \ncontextual information from text data effectively. by fine-tuning this model using our dataset, \nwe aimed to create a classification model specifically tailored to our task. \n \noptimiser and activation function: to optimise the performance of our model, i employed \nthe adam optimiser. adam is a popular optimisation algorithm that combines the benefits of \nthe adagrad and rmsprop algorithms. it helps in efficient gradient-based optimisation. \nadditionally, i used relu (rectified linear unit) activation function as the final layer in our \nmodel. relu is a widely used activation function that introduces non-linearity to the model \nand helps in capturing complex patterns in the data. \n \nmodel evaluation: to assess the performance of our model, i employed various evaluation \nmetrics, including training loss, training accuracy, validation loss, and validation accuracy. \nthese metrics provided insights into how well the model was learning during the training \nprocess and how it generalised to unseen data. by monitoring these metrics, we could make \ninformed decisions regarding model improvements, hyperparameter tuning, and potential \noverfitting.",
            "page": null,
            "goal": "week 9: model training and evaluating and param tuning \n \n \nin the ninth week of our project, we learned about important topics that are necessary to \nmake sure our models are reliable and accurate. we talked about random number generation \nand how it affects reproducibility, dropout and early stopping to prevent overfitting, and \ndifferent types of biases that can happen during data collection, preprocessing, feature \nengineering, data selection, model training, and model evaluation. let's summarise what we \nlearned: \n \nrandom number generation: we discussed the importance of setting the random seed when \nworking with random numbers. setting the seed allows us to reproduce the same results. \nhowever, in larger projects or when other parts of the code reset the global random seed, it \ncan cause problems. in such cases, we need to use the same random numbers for specific \nparts of the code, like tests or functions. \n \ndropout: dropout is a technique used in neural networks to prevent overfitting. it randomly \nignores certain nodes or layers during training. this helps make the network less flexible and \nreduces the risk of overfitting. a good dropout ratio value is usually between 0.5 and 0.6. \n \nearly stopping: early stopping is a method used to prevent overfitting in machine learning and \ndeep learning models. we found that after a certain number of training iterations, the model \nmay start overfitting the training data. continuing training beyond this point leads to a larger \ngap between training and validation scores and worse performance. early stopping allows us \nto stop training at the right time to prevent overfitting. \n \nbiases in data collection: we explored different biases that can happen during data \ncollection. bias in data collection occurs when information is gathered incorrectly based on \nprejudiced or biased assumptions. to avoid bias, it's important to involve experts who \nunderstand the subject matter and can accurately capture key features and their \ncharacteristics. they can ensure that the data collection process includes relevant variables \nand avoids skewed representation. \n \nbias in model training and evaluation: we discussed biases that can occur during model \ntraining and evaluation. it's important to choose the right model based on the type of data, \nthe problem we're solving, and the desired outcome. evaluating the model's performance \nusing test data is recommended because evaluating it based on the training data may be \nbiased. understanding the specific performance indicators that are important for our use case, \nlike sensitivity or accuracy, is crucial for accurate model evaluation. \n \nin addition to the topics covered in week 9, we also focused on data analysis and model \nbuilding using the bbc dataset. here are the additional processes i performed during this \nweek: \n \ndata analysis: i conducted a thorough data analysis on the bbc dataset to gain insights and \nunderstand its characteristics. then i employed various data visualisation methods such as pie \ncharts and bar charts to visualise the distribution of articles across different categories like \nbusiness, entertainment, politics, sport, and tech. these visualisations helped us understand \nthe dataset's composition and identify any class imbalances or patterns. \n \ndata cleaning: to ensure the quality of our dataset, we performed data cleaning processes. \nthis involved removing any null or missing values from the dataset. null values can adversely \naffect the model's performance, so it is crucial to handle them appropriately. by eliminating \nthese instances, we ensured that the dataset was complete and ready for further analysis. \n \noutlier removal: outliers can significantly impact the performance of machine learning \nmodels. outliers can skew the data distribution and lead to biased model predictions. by \ncarefully examining the dataset and applying suitable statistical methods, i successfully \nidentified and eliminated any outliers present. \n \ntokenisation: tokenisation is a fundamental step in natural language processing tasks. in the \ncontext of the bbc dataset, i used a tokeniser to break down the textual content of the articles \ninto individual tokens or words. this process is essential for further analysis and model \ntraining, as it enables us to convert text data into numerical representations that can be \nunderstood by machine learning algorithms. \n \nmodel building: i constructed a model to classify the articles in the bbc dataset into their \nrespective categories. for this task, i utilised the bertclassifier, which leverages the power of \nthe bert-base-cased bert model. the bert model is a pre-trained language model that captures \ncontextual information from text data effectively. by fine-tuning this model using our dataset, \nwe aimed to create a classification model specifically tailored to our task. \n \noptimiser and activation function: to optimise the performance of our model, i employed \nthe adam optimiser. adam is a popular optimisation algorithm that combines the benefits of \nthe adagrad and rmsprop algorithms. it helps in efficient gradient-based optimisation. \nadditionally, i used relu (rectified linear unit) activation function as the final layer in our \nmodel. relu is a widely used activation function that introduces non-linearity to the model \nand helps in capturing complex patterns in the data. \n \nmodel evaluation: to assess the performance of our model, i employed various evaluation \nmetrics, including training loss, training accuracy, validation loss, and validation accuracy. \nthese metrics provided insights into how well the model was learning during the training \nprocess and how it generalised to unseen data. by monitoring these metrics, we could make \ninformed decisions regarding model improvements, hyperparameter tuning, and potential \noverfitting.",
            "children": []
        },
        {
            "id": "1.19",
            "name": "week 10",
            "nodeType": "title",
            "text": "week 10",
            "page": null,
            "goal": "week 10",
            "children": []
        },
        {
            "id": "1.20",
            "name": "week 10: model",
            "nodeType": "paragraph",
            "text": "week 10: model building, training and evaluating on asap dataset \n \nduring week 10, i encountered some challenges while working on the bert model, as it was \na new field for me. as a result, i fell behind schedule. however, i remained determined and \ndedicated to improving my understanding. i spent the entire weekend studying, revising, and \ntraining the model. fortunately, i made significant progress at the beginning of the week by \nachieving a test accuracy of 0.996 after training the model for 5 epochs. i also experimented \nwith different numbers of epochs, such as 10 and 20. although the results were still good, \nthey were not as impressive as the 5-epoch training. furthermore, the model began to show \nsigns of overfitting around 7-8 epochs. consequently, i decided that training the model for 5 \nepochs was a suitable threshold for this bbc news dataset. i saved this successful result for \nfurther analysis. \n \nmoving on to the lessons of week 10, masoud and i determined that i had gained a solid \nfoundation and understanding of the bert model architecture. therefore, we decided to \nimplement the same model on a different dataset called the asap dataset. this dataset, \nobtained from the hewlett foundation: automated essay scoring dataset on kaggle, is used \nto develop an automated scoring algorithm for student-written essays. the dataset includes \nascii formatted text for each essay, accompanied by one or more human scores and, in some \ncases, a final resolved human score. the training data contains varying numbers of essays for \neach prompt, ranging from the lowest amount of 1,190 essays randomly selected from a total \nof 1,982. \n \nin week 10, my goal was to perform data analysis on the new asap dataset. however, i \nencountered some challenging issues. upon analysing the dataset and examining its \ndistribution, i discovered that it was highly skewed and imbalanced. despite performing some \ndata preprocessing, i was unable to resolve the imbalance issue due to its severity. \nnevertheless, with permission from steve and masoud, i was allowed to continue attempting \nto implement the current model on this imbalanced dataset to observe the resulting \noutcomes.",
            "page": null,
            "goal": "week 10: model building, training and evaluating on asap dataset \n \nduring week 10, i encountered some challenges while working on the bert model, as it was \na new field for me. as a result, i fell behind schedule. however, i remained determined and \ndedicated to improving my understanding. i spent the entire weekend studying, revising, and \ntraining the model. fortunately, i made significant progress at the beginning of the week by \nachieving a test accuracy of 0.996 after training the model for 5 epochs. i also experimented \nwith different numbers of epochs, such as 10 and 20. although the results were still good, \nthey were not as impressive as the 5-epoch training. furthermore, the model began to show \nsigns of overfitting around 7-8 epochs. consequently, i decided that training the model for 5 \nepochs was a suitable threshold for this bbc news dataset. i saved this successful result for \nfurther analysis. \n \nmoving on to the lessons of week 10, masoud and i determined that i had gained a solid \nfoundation and understanding of the bert model architecture. therefore, we decided to \nimplement the same model on a different dataset called the asap dataset. this dataset, \nobtained from the hewlett foundation: automated essay scoring dataset on kaggle, is used \nto develop an automated scoring algorithm for student-written essays. the dataset includes \nascii formatted text for each essay, accompanied by one or more human scores and, in some \ncases, a final resolved human score. the training data contains varying numbers of essays for \neach prompt, ranging from the lowest amount of 1,190 essays randomly selected from a total \nof 1,982. \n \nin week 10, my goal was to perform data analysis on the new asap dataset. however, i \nencountered some challenging issues. upon analysing the dataset and examining its \ndistribution, i discovered that it was highly skewed and imbalanced. despite performing some \ndata preprocessing, i was unable to resolve the imbalance issue due to its severity. \nnevertheless, with permission from steve and masoud, i was allowed to continue attempting \nto implement the current model on this imbalanced dataset to observe the resulting \noutcomes.",
            "children": []
        },
        {
            "id": "1.21",
            "name": "week 11",
            "nodeType": "title",
            "text": "week 11",
            "page": null,
            "goal": "week 11",
            "children": []
        },
        {
            "id": "1.22",
            "name": "week 11: asap",
            "nodeType": "paragraph",
            "text": "week 11: asap dataset training/evaluating and tuning \n \nduring week 11, i began the process of adapting the current model to fit the new asap dataset \nand commenced training. however, i encountered several new challenges during the training \nphase. unexpected errors surfaced, making it difficult to identify their root causes. it took me \nseveral days of intensive troubleshooting to finally pinpoint the error hidden deep within the \nlayers of the model. i discovered that this error was specific to the new dataset and its \nincompatibility with certain aspects of the model. this experience served as a valuable lesson, \nemphasising the need to tailor the approach for different datasets to ensure compatibility. \n \nto address the issue, i experimented with various debugging methods. i meticulously analysed \nthe error logs, inspected the model architecture, and reviewed the dataset structure to gain \ninsights into the source of the problem. this iterative debugging process involved making \nadjustments to the model's layers, fine-tuning parameters, and implementing dataset-specific \nmodifications. it required a combination of patience, perseverance, and problem-solving \nskills. \n \nanother significant challenge i encountered was the extensive training time required by the \nasap dataset. unlike the bbc news dataset, which trained relatively quickly, the asap dataset \ndemanded considerably more time to converge. the training process became exceptionally \ntime-consuming, especially when faced with the uncertainty of predicting the outcome and \nwaiting for the model to train and evaluate. this extended timeframe posed a challenge to my \nworkflow and necessitated effective time management strategies. \n \nfurthermore, if the initial results did not meet expectations, i had to retrain the model and \nperform hyperparameter tuning. this involved adjusting various parameters such as dropout \nvalues, learning rates, and the number of epochs. as time was limited, i made every effort to \nmaximise the remaining time i had to explore different hyperparameter configurations. this \niterative process involved training the model, evaluating the results, adjusting parameters, \nand repeating the cycle until achieving the desired performance. \n \nthroughout this process, i recognised the importance of experimentation and the iterative \nnature of model development. it was essential to carefully balance the time-consuming \ntraining process with the need for timely results. although challenging, this experience \nallowed me to deepen my understanding of model adaptation, troubleshooting, and \nhyperparameter tuning in the context of a new dataset.",
            "page": null,
            "goal": "week 11: asap dataset training/evaluating and tuning \n \nduring week 11, i began the process of adapting the current model to fit the new asap dataset \nand commenced training. however, i encountered several new challenges during the training \nphase. unexpected errors surfaced, making it difficult to identify their root causes. it took me \nseveral days of intensive troubleshooting to finally pinpoint the error hidden deep within the \nlayers of the model. i discovered that this error was specific to the new dataset and its \nincompatibility with certain aspects of the model. this experience served as a valuable lesson, \nemphasising the need to tailor the approach for different datasets to ensure compatibility. \n \nto address the issue, i experimented with various debugging methods. i meticulously analysed \nthe error logs, inspected the model architecture, and reviewed the dataset structure to gain \ninsights into the source of the problem. this iterative debugging process involved making \nadjustments to the model's layers, fine-tuning parameters, and implementing dataset-specific \nmodifications. it required a combination of patience, perseverance, and problem-solving \nskills. \n \nanother significant challenge i encountered was the extensive training time required by the \nasap dataset. unlike the bbc news dataset, which trained relatively quickly, the asap dataset \ndemanded considerably more time to converge. the training process became exceptionally \ntime-consuming, especially when faced with the uncertainty of predicting the outcome and \nwaiting for the model to train and evaluate. this extended timeframe posed a challenge to my \nworkflow and necessitated effective time management strategies. \n \nfurthermore, if the initial results did not meet expectations, i had to retrain the model and \nperform hyperparameter tuning. this involved adjusting various parameters such as dropout \nvalues, learning rates, and the number of epochs. as time was limited, i made every effort to \nmaximise the remaining time i had to explore different hyperparameter configurations. this \niterative process involved training the model, evaluating the results, adjusting parameters, \nand repeating the cycle until achieving the desired performance. \n \nthroughout this process, i recognised the importance of experimentation and the iterative \nnature of model development. it was essential to carefully balance the time-consuming \ntraining process with the need for timely results. although challenging, this experience \nallowed me to deepen my understanding of model adaptation, troubleshooting, and \nhyperparameter tuning in the context of a new dataset.",
            "children": []
        },
        {
            "id": "1.23",
            "name": "week 12",
            "nodeType": "title",
            "text": "week 12",
            "page": null,
            "goal": "week 12",
            "children": []
        },
        {
            "id": "1.24",
            "name": "week 12: corpus",
            "nodeType": "paragraph",
            "text": "week 12: corpus dataset + api deployment \n \nas we enter the final week of our internship project, i am pleased with the progress we have \nmade thus far. we have successfully accomplished 90% of the important project milestones, \ndespite encountering some challenging issues along the way. this is a natural part of the \nproject life cycle, and i am proud of our ability to overcome obstacles and achieve the \nmajority of our initial objectives. \n \nin this last week, we decided to explore training the model on another dataset called the \nfirst certificate in english (fce) corpus. this dataset, known as the cambridge learner \ncorpus first certificate in english (clc fce), comprises short texts written by learners of \nenglish as an additional language in response to exam prompts. these texts assess the \nlearners' mastery of the upper-intermediate proficiency level and have been manually error-\nannotated using a taxonomy of 77 error types. the complete fce dataset contains 323,192 \nsentences. however, we focused on a publicly released subset called fce-public, which \nconsists of 33,673 sentences divided into test and training sets of 2,720 and 30,953 \nsentences, respectively. \n \nworking with this dataset provided me with the opportunity to perform data analysis on a \nlarge folder containing various subfolders, each containing numerous xml files. this step \nwas both challenging and exciting, as i had limited experience working with xml files in the \npast. to navigate through the folder structure and extract the necessary information, i \nutilised the xml.etree.elementtree module, which offers a simple and efficient api for \nparsing and creating xml data. i developed a function to iterate through the dataset, read \nthe xml data, and parse the xml elements. while i faced some challenges due to the \nvariations in xml file structures and the need for careful extraction of relevant elements, i \nmanaged to obtain satisfactory results and retrieve most of the crucial information required \nfor the model. \n \nunfortunately, time constraints became a limiting factor as the project approached its end. \nwith limited time remaining, i was unable to dive deeper into this direction and explore \nfurther possibilities. nevertheless, i am eager to continue exploring this area in the future \nand enhance my understanding and skills for personal self-development purposes. \n \nin addition to our work, we had a delightful lunch meeting together, where we discussed \nfuture opportunities and received valuable support from steve and masoud. the kindness \nand support from every person in the company made this journey unforgettable, and i am \ntruly grateful for their assistance throughout the journey. \n \n",
            "page": null,
            "goal": "week 12: corpus dataset + api deployment \n \nas we enter the final week of our internship project, i am pleased with the progress we have \nmade thus far. we have successfully accomplished 90% of the important project milestones, \ndespite encountering some challenging issues along the way. this is a natural part of the \nproject life cycle, and i am proud of our ability to overcome obstacles and achieve the \nmajority of our initial objectives. \n \nin this last week, we decided to explore training the model on another dataset called the \nfirst certificate in english (fce) corpus. this dataset, known as the cambridge learner \ncorpus first certificate in english (clc fce), comprises short texts written by learners of \nenglish as an additional language in response to exam prompts. these texts assess the \nlearners' mastery of the upper-intermediate proficiency level and have been manually error-\nannotated using a taxonomy of 77 error types. the complete fce dataset contains 323,192 \nsentences. however, we focused on a publicly released subset called fce-public, which \nconsists of 33,673 sentences divided into test and training sets of 2,720 and 30,953 \nsentences, respectively. \n \nworking with this dataset provided me with the opportunity to perform data analysis on a \nlarge folder containing various subfolders, each containing numerous xml files. this step \nwas both challenging and exciting, as i had limited experience working with xml files in the \npast. to navigate through the folder structure and extract the necessary information, i \nutilised the xml.etree.elementtree module, which offers a simple and efficient api for \nparsing and creating xml data. i developed a function to iterate through the dataset, read \nthe xml data, and parse the xml elements. while i faced some challenges due to the \nvariations in xml file structures and the need for careful extraction of relevant elements, i \nmanaged to obtain satisfactory results and retrieve most of the crucial information required \nfor the model. \n \nunfortunately, time constraints became a limiting factor as the project approached its end. \nwith limited time remaining, i was unable to dive deeper into this direction and explore \nfurther possibilities. nevertheless, i am eager to continue exploring this area in the future \nand enhance my understanding and skills for personal self-development purposes. \n \nin addition to our work, we had a delightful lunch meeting together, where we discussed \nfuture opportunities and received valuable support from steve and masoud. the kindness \nand support from every person in the company made this journey unforgettable, and i am \ntruly grateful for their assistance throughout the journey. \n \n",
            "children": []
        }
    ]
}