[
  {
    "children": [
      {
        "goal": "- **Research the viability of GANs to enhance the performance of the liveness ML model.**\n- **Use GANs to generate and enlarge the existing dataset.**\n- **Evaluate the generated data and assess the improved performance of the liveness model with the enlarged dataset.** \n",
        "id": "24.1",
        "name": "Week ",
        "nodeType": "Paragraph",
        "page": null,
        "text": "Week 1 \nIn the first week of my internship, I was formally onboarded. I was asked \nto read and confirm that I understood the security procedures of the \ncompany. I was provided access to their AWS system, specifically the \nSageMaker Jupyter Notebook instances where I would be able to develop \ncode, access the datasets and execute new notebook implementations.  \nI was part of the \u2018liveness\u2019 team, which consisted of Ms Le Nga Ho, Ms \nJennifer Box and myself. Our supervisor was Ms Matineh Poushide. Our team \nwas to have three standup meetings a week to make sure the work assigned \nto us was progressing at an acceptable pace, and to account for any delays \nthat were anticipated. The CEO, Mr Mike Simpson would also attend most, if \nnot all these standup meetings. \nThe current state of the project was that the liveness ML model was not \nperforming well in real-life deployment when tested on di\ufb00erent users, in \ndi\ufb00erent lighting conditions, and in di\ufb00erent scenarios. Moreover, the \nliveness model was not performing well in its primary task of binary \nclassification: judging if the image was of a live person or a presentation \nattack.  \nThe liveness team suspected that either the training data size was not \nsufficient for e\ufb00ective learning, or the training labels \u201cLive\u201d and \u201cFake\u201d were \nnot representative of real life scenarios such as when users are in difficult \nlighting conditions or when a photo is used as a presentation attack. \nTo solve this problem, the company had set about collecting their own \nimage dataset for training purposes. When I began my internship, the \ncompany had collected 700 live and 700 fake images manually.  \nThe specifics of the project were detailed to me \u2014 my task would be to \nresearch the state of the art in GANs and examine their viability in our \nInternship Report (Final)\nPage \n16 of \n40\nbusiness problem. Afterwards we were required to conduct an associated \nliterature review where a similar method was used to enhance the learning \nand performance of machine learning models when the dataset was \ninsufficient. \nAfter the research phase, we would begin the long work of reproducing \nthe work outlined in our chosen research paper(s) by generating the training \nweights of the GANs we had selected for our business problem and use them \non the company\u2019s dataset to enlarge the image data. \nIn the next step, we would validate the images that were generated by a \nchosen metric to judge whether it would be acceptable to use it for model \ntraining. Finally, using the enlarged dataset, we would test how well the \nmodel improves in its classification tasks compared to the baseline accuracy \nperformance. \nIt was emphasised to us that this was an experimental task, and all \nexperiments require documentation. Therefore, we were strongly \nencouraged to write a report about our research, experiments, and failures in \nthe company\u2019s Confluence page. \nBefore the week had ended, I was deep into research papers about the \nstate of the art GANs used today, and the more recent development in their \nimprovement. As per instructions, I would document my work in the \nConfluence page."
      }
    ],
    "goal": "\n- **Research the viability of GANs to enhance the performance of the liveness ML model.**\n- **Use GANs to generate and enlarge the existing dataset.**\n- **Evaluate the generated data and assess the improved performance of the liveness model with the enlarged dataset.** \n",
    "id": 24,
    "name": "Week 1",
    "nodeType": "Section",
    "page": 16,
    "text": "Week 1 \nIn the first week of my internship, I was formally onboarded. I was asked \nto read and confirm that I understood the security procedures of the \ncompany. I was provided access to their AWS system, specifically the \nSageMaker Jupyter Notebook instances where I would be able to develop \ncode, access the datasets and execute new notebook implementations.  \nI was part of the \u2018liveness\u2019 team, which consisted of Ms Le Nga Ho, Ms \nJennifer Box and myself. Our supervisor was Ms Matineh Poushide. Our team \nwas to have three standup meetings a week to make sure the work assigned \nto us was progressing at an acceptable pace, and to account for any delays \nthat were anticipated. The CEO, Mr Mike Simpson would also attend most, if \nnot all these standup meetings. \nThe current state of the project was that the liveness ML model was not \nperforming well in real-life deployment when tested on di\ufb00erent users, in \ndi\ufb00erent lighting conditions, and in di\ufb00erent scenarios. Moreover, the \nliveness model was not performing well in its primary task of binary \nclassification: judging if the image was of a live person or a presentation \nattack.  \nThe liveness team suspected that either the training data size was not \nsufficient for e\ufb00ective learning, or the training labels \u201cLive\u201d and \u201cFake\u201d were \nnot representative of real life scenarios such as when users are in difficult \nlighting conditions or when a photo is used as a presentation attack. \nTo solve this problem, the company had set about collecting their own \nimage dataset for training purposes. When I began my internship, the \ncompany had collected 700 live and 700 fake images manually.  \nThe specifics of the project were detailed to me \u2014 my task would be to \nresearch the state of the art in GANs and examine their viability in our \nInternship Report (Final)\nPage \n16 of \n40\nbusiness problem. Afterwards we were required to conduct an associated \nliterature review where a similar method was used to enhance the learning \nand performance of machine learning models when the dataset was \ninsufficient. \nAfter the research phase, we would begin the long work of reproducing \nthe work outlined in our chosen research paper(s) by generating the training \nweights of the GANs we had selected for our business problem and use them \non the company\u2019s dataset to enlarge the image data. \nIn the next step, we would validate the images that were generated by a \nchosen metric to judge whether it would be acceptable to use it for model \ntraining. Finally, using the enlarged dataset, we would test how well the \nmodel improves in its classification tasks compared to the baseline accuracy \nperformance. \nIt was emphasised to us that this was an experimental task, and all \nexperiments require documentation. Therefore, we were strongly \nencouraged to write a report about our research, experiments, and failures in \nthe company\u2019s Confluence page. \nBefore the week had ended, I was deep into research papers about the \nstate of the art GANs used today, and the more recent development in their \nimprovement. As per instructions, I would document my work in the \nConfluence page."
  },
  {
    "children": [
      {
        "goal": "- Conduct a literature review of state-of-the-art GANs, focusing on conditional GANs and those with publicly available code and weights.\n- Summarize the key concepts and advantages/shortcomings of each approach in a way that is accessible to non-technical audiences.\n- Collaborate with Ms. Ho to ensure progress and provide a high-level overview of the findings to the CEO. \n",
        "id": "25.1",
        "name": "Week ",
        "nodeType": "Paragraph",
        "page": null,
        "text": "Week 2 \nIn week 2, I continued my research on the state-of-the-art in GANs, this \ntime focusing on the GANs that were conditional, i.e changing the facial \ncharacteristics based on age, race and gender. I also focused on GANs that \nfocused on image quality and background. This week I was focused on \nwriting up my findings on Confluence and in doing so, I learned a lot about \nthe architecture of GANs and how they work.  \nInternship Report (Final)\nPage \n17 of \n40\nDuring one of our stand up meetings, we were asked to focus on those \nGANs that have publicly available code and if possible, their associated \nweights. This was requested so as to save time training and writing code, \nespecially if the instructions in the paper were vague or unclear. Since the \nliterature review might be read by people who did not want to get \nentrenched in the technical details of various GAN architecture, I avoided \ntechnical details and focused on the approach that each author took and \ntheir respective advantage and shortcomings. I also included other details I \nthought were relevant so that future work on this topic would not require a \nfurther literature review about the topic of GANs. \nI did this work alongside Ms Ho and the two of us kept in contact about \nthe progress of our work with regular stand up meetings where we made \nsure that the tasks due that week were on track.  \nBy the end of the week, we had compiled a literature review of the GANs \nthat we believed were the state-of-the-art and summarised their high level \nideas. The final standup of the week with the team involved a summary of our \nwork, and overview of the best performing GANs currently in literature.  \nI found these standup meetings helpful for a unique reason. Since the \nCEO was not well versed in the technical detail of GANs, I truly had to \nunderstand the concept and the di\ufb00erent GAN approaches in order to \nexplain the overall high level concept in a manner that was straightforward \nand not convoluted or loaded with unnecessary detail. Before every stand up \nmeeting, I would prepare what I wanted to say, and sometimes kept notes \nabout specific GAN architectures that I wanted to draw his attention to. \nFor his part, the CEO was very quick to catch onto the details and \nconcepts and was always steering us to the path we were originally set on, \nrather than getting lost in the weeds. \nInternship Report (Final)\nPage \n18 of"
      }
    ],
    "goal": "\n- Conduct a literature review of state-of-the-art GANs, focusing on conditional GANs and those with publicly available code and weights.\n- Summarize the key concepts and advantages/shortcomings of each approach in a way that is accessible to non-technical audiences.\n- Collaborate with Ms. Ho to ensure progress and provide a high-level overview of the findings to the CEO. \n",
    "id": 25,
    "name": "Week 2",
    "nodeType": "Section",
    "page": 17,
    "text": "Week 2 \nIn week 2, I continued my research on the state-of-the-art in GANs, this \ntime focusing on the GANs that were conditional, i.e changing the facial \ncharacteristics based on age, race and gender. I also focused on GANs that \nfocused on image quality and background. This week I was focused on \nwriting up my findings on Confluence and in doing so, I learned a lot about \nthe architecture of GANs and how they work.  \nInternship Report (Final)\nPage \n17 of \n40\nDuring one of our stand up meetings, we were asked to focus on those \nGANs that have publicly available code and if possible, their associated \nweights. This was requested so as to save time training and writing code, \nespecially if the instructions in the paper were vague or unclear. Since the \nliterature review might be read by people who did not want to get \nentrenched in the technical details of various GAN architecture, I avoided \ntechnical details and focused on the approach that each author took and \ntheir respective advantage and shortcomings. I also included other details I \nthought were relevant so that future work on this topic would not require a \nfurther literature review about the topic of GANs. \nI did this work alongside Ms Ho and the two of us kept in contact about \nthe progress of our work with regular stand up meetings where we made \nsure that the tasks due that week were on track.  \nBy the end of the week, we had compiled a literature review of the GANs \nthat we believed were the state-of-the-art and summarised their high level \nideas. The final standup of the week with the team involved a summary of our \nwork, and overview of the best performing GANs currently in literature.  \nI found these standup meetings helpful for a unique reason. Since the \nCEO was not well versed in the technical detail of GANs, I truly had to \nunderstand the concept and the di\ufb00erent GAN approaches in order to \nexplain the overall high level concept in a manner that was straightforward \nand not convoluted or loaded with unnecessary detail. Before every stand up \nmeeting, I would prepare what I wanted to say, and sometimes kept notes \nabout specific GAN architectures that I wanted to draw his attention to. \nFor his part, the CEO was very quick to catch onto the details and \nconcepts and was always steering us to the path we were originally set on, \nrather than getting lost in the weeds. \nInternship Report (Final)\nPage \n18 of"
  },
  {
    "children": [
      {
        "goal": "- Upgrade official codebase from Tensorflow 1.0 to 2.0.\n- Collect and prepare age-labelled facial datasets for further processing.\n- Identify and propose the Fr\u00e9chet inception Distance (FID) as a standardized metric for evaluating image quality. \n",
        "id": "26.1",
        "name": "Week ",
        "nodeType": "Paragraph",
        "page": null,
        "text": "Week 3 \nIn Week 3, I decided to focus on the GANs that conditionally age the \nfacial features of a given image, while Ms Le focused on the GANs that \nconditionally age the racial characteristics. My supervisor, Ms Poushide, \napproved my work for the week and I set about cloning the public code of \ntwo age-conditional GANs that I had documented and believed to be of good \nquality. One of these codebases were the official work of the authors; the \nother was an interpretation of the paper\u2019s instructions. \nRight from the start, there were a few problems I encountered. The \nunofficial codebase implementation of one paper had multiple instances of \nincorrect python syntax and linting. The official codebase of the other GAN \npaper was written in Tensorflow version 1, which is now depreciated. \nOrdinarily, running an old tensorflow version with pip and the python virtual \nenvironment, venv, would have sidestepped this issue, but the company\u2019s \nAWS SageMaker instance did not support any version of tensorflow below \n2.0.  \nI informed my supervisor of this issue, and told her that I would begin \nwork on upgrading the official codebase from tensorflow 1.0 to tensorflow \n2.0.  \nMy work in week 3 became glacial since my attention was refocused into \nsubmitting an assignment for another unit in my coursework. However, even \nduring this busy time, I was able to identify and collect various age-labelled \nfacial datasets and store them on the company\u2019s AWS instance for further \nprocessing. \nFor this, I also wrote a small script to download the compressed datasets \nfrom their respective repositories using the GNU wget package and unzipped \nthem for storage on the AWS instance. I also wrote a script to tag age labels \nto faces for one dataset that did not have explicit age labelling, rather the \nage information was stored as metadata on the images. The script extracted \nInternship Report (Final)\nPage \n19 of \n40\nthe required age information and generated a label for the respective face \nimage. \nTowards the end of the week, the standup meeting with the team \nrevealed that we did not have a standardised way to measure the quality of \nimages generated by the GAN. I realised this was an oversight on my part, \nbecause I did not document the various image quality metrics that each GAN \npaper used. I promised that I would add this information to the Confluence \npage as soon as possible. Over the weekend, Ms Ho and I reviewed the \nmetrics used by each paper and added the pertinent information. \nThis actually revealed a common metric across all papers, which is the \nFr\u00e9chet inception Distance (FID) that measures the di\ufb00erence between two \nimages. We decided that this is also the metric we should use to judge the \nimages that we generate, and presented this recommendation during the last \nstand up meeting of the week."
      }
    ],
    "goal": "\n- Upgrade official codebase from Tensorflow 1.0 to 2.0.\n- Collect and prepare age-labelled facial datasets for further processing.\n- Identify and propose the Fr\u00e9chet inception Distance (FID) as a standardized metric for evaluating image quality. \n",
    "id": 26,
    "name": "Week 3",
    "nodeType": "Section",
    "page": 19,
    "text": "Week 3 \nIn Week 3, I decided to focus on the GANs that conditionally age the \nfacial features of a given image, while Ms Le focused on the GANs that \nconditionally age the racial characteristics. My supervisor, Ms Poushide, \napproved my work for the week and I set about cloning the public code of \ntwo age-conditional GANs that I had documented and believed to be of good \nquality. One of these codebases were the official work of the authors; the \nother was an interpretation of the paper\u2019s instructions. \nRight from the start, there were a few problems I encountered. The \nunofficial codebase implementation of one paper had multiple instances of \nincorrect python syntax and linting. The official codebase of the other GAN \npaper was written in Tensorflow version 1, which is now depreciated. \nOrdinarily, running an old tensorflow version with pip and the python virtual \nenvironment, venv, would have sidestepped this issue, but the company\u2019s \nAWS SageMaker instance did not support any version of tensorflow below \n2.0.  \nI informed my supervisor of this issue, and told her that I would begin \nwork on upgrading the official codebase from tensorflow 1.0 to tensorflow \n2.0.  \nMy work in week 3 became glacial since my attention was refocused into \nsubmitting an assignment for another unit in my coursework. However, even \nduring this busy time, I was able to identify and collect various age-labelled \nfacial datasets and store them on the company\u2019s AWS instance for further \nprocessing. \nFor this, I also wrote a small script to download the compressed datasets \nfrom their respective repositories using the GNU wget package and unzipped \nthem for storage on the AWS instance. I also wrote a script to tag age labels \nto faces for one dataset that did not have explicit age labelling, rather the \nage information was stored as metadata on the images. The script extracted \nInternship Report (Final)\nPage \n19 of \n40\nthe required age information and generated a label for the respective face \nimage. \nTowards the end of the week, the standup meeting with the team \nrevealed that we did not have a standardised way to measure the quality of \nimages generated by the GAN. I realised this was an oversight on my part, \nbecause I did not document the various image quality metrics that each GAN \npaper used. I promised that I would add this information to the Confluence \npage as soon as possible. Over the weekend, Ms Ho and I reviewed the \nmetrics used by each paper and added the pertinent information. \nThis actually revealed a common metric across all papers, which is the \nFr\u00e9chet inception Distance (FID) that measures the di\ufb00erence between two \nimages. We decided that this is also the metric we should use to judge the \nimages that we generate, and presented this recommendation during the last \nstand up meeting of the week."
  }
]